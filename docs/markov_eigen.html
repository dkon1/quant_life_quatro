<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.280">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Quantifying Life - 16&nbsp; Dynamics of Markov models</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./summary.html" rel="next">
<link href="./markov_stat.html" rel="prev">
<link href="./Kondrashov_Comp.jpeg" rel="icon" type="image/jpeg">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<meta name="twitter:title" content="Quantifying Life - 16&nbsp; Dynamics of Markov models">
<meta name="twitter:description" content="We have learned several approaches for analyzing the behavior of Markov models. We know that many Markov models converge to a single stationary distribution over time.">
<meta name="twitter:card" content="summary">
</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Dynamics of Markov models</span></h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Quantifying Life</a> 
        <div class="sidebar-tools-main">
    <a href="https://github.com/dkon1/quant_life_quatro" title="Source Code" class="sidebar-tool px-1"><i class="bi bi-github"></i></a>
    <a href="" title="Download" id="sidebar-tool-dropdown-0" class="sidebar-tool dropdown-toggle px-1" data-bs-toggle="dropdown" aria-expanded="false"><i class="bi bi-download"></i></a>
    <ul class="dropdown-menu" aria-labelledby="sidebar-tool-dropdown-0">
        <li>
          <a class="dropdown-item sidebar-tools-main-item" href="./Quantifying-Life.pdf">
            <i class="bi bi-bi-file-pdf pe-1"></i>
          Download PDF
          </a>
        </li>
        <li>
          <a class="dropdown-item sidebar-tools-main-item" href="./Quantifying-Life.epub">
            <i class="bi bi-bi-journal pe-1"></i>
          Download ePub
          </a>
        </li>
    </ul>
</div>
    </div>
      </div>
      <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">Preface</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./counting.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Arithmetic and variables</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./functions.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Functions and their graphs</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./descriptive.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Describing data sets</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./probdist.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Random variables and distributions</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./linreg.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Linear regression</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./independence.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Independence</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./hypo_test.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Hypothesis testing</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./bayes.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Prior knowledge and Bayesian thinking</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./sampling.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Sampling distribution and estimation</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./lindiff.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Linear difference equations</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./graph_odes.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Graphical analysis of ordinary differential equations</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ode_sols.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Solutions of ordinary differential equations</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./markov_model.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Markov models with discrete states</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./markov_evol.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Probability distributions of Markov chains</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./markov_stat.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Stationary distributions of Markov chains</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./markov_eigen.html" class="sidebar-item-text sidebar-link active"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Dynamics of Markov models</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./summary.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Summary</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">References</a>
  </div>
</li>
    </ul>
    </div>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#phylogenetic-trees" id="toc-phylogenetic-trees" class="nav-link active" data-scroll-target="#phylogenetic-trees"><span class="toc-section-number">16.1</span>  Phylogenetic trees</a></li>
  <li><a href="#eigenvalues-and-eigenvectors" id="toc-eigenvalues-and-eigenvectors" class="nav-link" data-scroll-target="#eigenvalues-and-eigenvectors"><span class="toc-section-number">16.2</span>  Eigenvalues and eigenvectors</a>
  <ul class="collapse">
  <li><a href="#basic-linear-algebra" id="toc-basic-linear-algebra" class="nav-link" data-scroll-target="#basic-linear-algebra"><span class="toc-section-number">16.2.1</span>  basic linear algebra</a></li>
  <li><a href="#calculation-of-eigenvalues-on-paper" id="toc-calculation-of-eigenvalues-on-paper" class="nav-link" data-scroll-target="#calculation-of-eigenvalues-on-paper"><span class="toc-section-number">16.2.2</span>  calculation of eigenvalues on paper</a></li>
  <li><a href="#calculation-of-eigenvectors-on-paper" id="toc-calculation-of-eigenvectors-on-paper" class="nav-link" data-scroll-target="#calculation-of-eigenvectors-on-paper"><span class="toc-section-number">16.2.3</span>  calculation of eigenvectors on paper</a></li>
  <li><a href="#exercises" id="toc-exercises" class="nav-link" data-scroll-target="#exercises"><span class="toc-section-number">16.2.4</span>  Exercises</a></li>
  </ul></li>
  <li><a href="#eigenvectors-in-r" id="toc-eigenvectors-in-r" class="nav-link" data-scroll-target="#eigenvectors-in-r"><span class="toc-section-number">16.3</span>  Eigenvectors in R</a></li>
  <li><a href="#molecular-evolution" id="toc-molecular-evolution" class="nav-link" data-scroll-target="#molecular-evolution"><span class="toc-section-number">16.4</span>  Molecular evolution</a>
  <ul class="collapse">
  <li><a href="#time-since-divergence" id="toc-time-since-divergence" class="nav-link" data-scroll-target="#time-since-divergence"><span class="toc-section-number">16.4.1</span>  time since divergence</a></li>
  <li><a href="#phylogenetic-distance" id="toc-phylogenetic-distance" class="nav-link" data-scroll-target="#phylogenetic-distance"><span class="toc-section-number">16.4.2</span>  phylogenetic distance</a></li>
  <li><a href="#kimura-model" id="toc-kimura-model" class="nav-link" data-scroll-target="#kimura-model"><span class="toc-section-number">16.4.3</span>  Kimura model</a></li>
  <li><a href="#divergence-of-human-and-chimp-genomes" id="toc-divergence-of-human-and-chimp-genomes" class="nav-link" data-scroll-target="#divergence-of-human-and-chimp-genomes"><span class="toc-section-number">16.4.4</span>  divergence of human and chimp genomes</a></li>
  <li><a href="#discussion-questions" id="toc-discussion-questions" class="nav-link" data-scroll-target="#discussion-questions"><span class="toc-section-number">16.4.5</span>  Discussion questions</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title d-none d-lg-block"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Dynamics of Markov models</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<blockquote class="blockquote">
<p>No hour is ever eternity, but it has its right to weep.<br>
– Zora Neale Hurston, <em>Their Eyes Were Watching God</em></p>
</blockquote>
<p>We have learned several approaches for analyzing the behavior of Markov models. We know that many Markov models converge to a single stationary distribution over time. For many biological questions, however, the stationary distribution itself is not very interesting, but what matters is how fast the probability distribution converges. In this chapter we will encounter more advanced tools for analyzing matrices which will enable us to answer that question. This approach will be illustrated in application to determination of evolutionary distance based on sequence data. In this chapter you will learn to do the following:</p>
<ul>
<li>meaning of eigenvalues and eigenvectors</li>
<li>calculate eigenvalues and eigenvectors of 2 by 2 matrices</li>
<li>mixing times of Markov chains</li>
<li>calculate the phylogenetic distance between two DNA sequences</li>
</ul>
<section id="phylogenetic-trees" class="level2" data-number="16.1">
<h2 data-number="16.1" class="anchored" data-anchor-id="phylogenetic-trees"><span class="header-section-number">16.1</span> Phylogenetic trees</h2>
<p>Over many generations, genomes of living creatures accumulate random mutations, as we previously discussed in chapters 3 and 6. Each individual genome in a population has its own polymorphisms, and thus some are more advantageous for survival in a particular environment than others. The process of natural selection is stochastic, and the notion of “survival of the fittest” is not a guarantee that the best-adapted always out-compete the rest - sometimes a more fit individual has a bad day and can’t find any food or gets eaten by a predator. However, over time the allele that are advantageous have a better chance of survival and become more common in the population, while other alleles become rare or vanish . This process never stops, because the environmental conditions change, and new mutations arise, so at any point in time the individual genomes in a species have some variations, although the vast majority of their genomes are identical.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="ch13/Phylogenetic_tree.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Phylogenetic tree for life forms on Earth (by Eric Gaba (NASA Astrobiology Institute) in public domain via Wikimedia commons)</figcaption><p></p>
</figure>
</div>
<p>One may describe the collection of genomes in a population or a species in terms of the most common alleles; this is roughly what we call the human genome, or the elephant genome, or the rice genome. Once we have determined a consensus genome sequence for a species, it can be used to pose and answer questions about its heritage. If the genomes of two species are more similar to each other than to a third, it is likely that the similar pair diverged more recently than the third one. This information allows one to build <em>phylogenetic trees</em> that visually illustrate the evolutionary history of a collection of species, with each fork in the tree representing the splitting of lineages. Interpreting phylogenetic tress is fairly straightforward: species or clades (a collection of species that make an evolutionary unit) that are closely related are directly connected to a common ancestor, while the path between those that are more distantly related passes through multiple forks before reaching the common ancestor. Some trees also incorporate time information as branch lengths, with longer branches indicating that more time has passes from a divergence event.</p>
<p>Figure <span class="math inline">\(\ref{fig:ch13_phylotree}\)</span> shows the phylogenetic tree for the lifeforms on Earth, divided into the three major kingdoms: Bacteria, Archea, ane Eukaryota, the latter includes all multicellular lifeforms, including plants, animals, and fungi. The nodes (end points) show the major groupings of life existing today, and the branches show the order of evolutionary divergence of lineages, starting with the hypothesized root of the tree at the bottom, known as LUCA (last universal common ancestor) . The order of splitting and the grouping of the nodes was determined by molecular sequence data (in particular ribosomal RNA) that has become available in the past 20 years.</p>
<p>In the past, biologists studied observable traits of different life forms, such as anatomy, physiology, or developmental features, and determined similarity from these data. However, the wealth of molecular sequence data has offered a great amount of evidence which is the primary material of evolution. Phylogeny, particularly that of unicellular organisms, has been revolutionized by these data; we now know that prokaryotes are divided into kingdoms of Archea and Bacteria which are evolutionarily more distant than humans are from fungi. Sequence data are quantitative and they require mathematical models to interpret them and to infer phylogenies. In the last section of this chapter we will introduce mathematical tools that connect related sequences to the evolutionary divergence from their common ancestor.</p>
</section>
<section id="eigenvalues-and-eigenvectors" class="level2" data-number="16.2">
<h2 data-number="16.2" class="anchored" data-anchor-id="eigenvalues-and-eigenvectors"><span class="header-section-number">16.2</span> Eigenvalues and eigenvectors</h2>
<section id="basic-linear-algebra" class="level3" data-number="16.2.1">
<h3 data-number="16.2.1" class="anchored" data-anchor-id="basic-linear-algebra"><span class="header-section-number">16.2.1</span> basic linear algebra</h3>
<p>In the past two chapters we have seen matrices and learned the definition of matrix multiplication, but now we are ready to go deeper into the branch of mathematics studying matrices and their generalizations, called linear algebra. It is fundamental to both pure and applied mathematics , and its tools are used in countless applications and fields. Let us define two useful numbers that help describe the properties of a matrix:</p>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Definition
</div>
</div>
<div class="callout-body-container callout-body">
<p>The <em>trace</em> <span class="math inline">\(\tau\)</span> of a matrix <span class="math inline">\(A\)</span> is the sum of the diagonal elements: <span class="math inline">\(\tau = \sum_i A_{ii}\)</span>.</p>
<p>The <em>determinant</em> <span class="math inline">\(\Delta\)</span> of a 2 by 2 matrix <span class="math inline">\(A\)</span> is given by the following: <span class="math inline">\(\Delta = ad - bc\)</span>, where <span class="math display">\[ A = \left(\begin{array}{cc}a &amp; b \\c &amp; d\end{array}\right) \]</span></p>
</div>
</div>
<p>For larger matrices, the determinant is defined recursively in terms of 2 by 2 submatrices of the larger matrix, but we will not give the full definition here.</p>
<p>In this section we will learn to characterize square matrices by finding special numbers and vectors associated with them. At the core of this analysis lies the concept of a matrix as an operator that transforms vectors by multiplication. To be clear, in this section we take as default that the matrices <span class="math inline">\(A\)</span> are square, and that vectors <span class="math inline">\(\vec v\)</span> are column vectors, and thus will multiply the matrix on the right: <span class="math inline">\(A \times \vec v\)</span>.</p>
<p>A matrix multiplied by a vector produces another vector, provided the number of columns in the matrix is the same as the number of rows in the vector. This can be interpreted as the matrix transforming the vector <span class="math inline">\(\vec v\)</span> into another one: <span class="math inline">\(A \times \vec v = \vec u\)</span>. The resultant vector <span class="math inline">\(\vec u\)</span> may or may not resemble <span class="math inline">\(\vec v\)</span>, but there are special vectors for which the transformation is very simple.</p>
<p>*<strong>Example.</strong> Let us multiply the following matrix and vector: <span class="math display">\[
\left(\begin{array}{cc}2 &amp; 1 \\ 2&amp; 3\end{array}\right)\left(\begin{array}{c}1 \\ -1 \end{array}\right) = \left(\begin{array}{c}2 -1 \\ 2 - 3 \end{array}\right) =  \left(\begin{array}{c} 1 \\ -1 \end{array}\right)
\]</span> We see that this particular vector is unchanged when multiplied by this matrix, or we can say that the matrix multiplication is equivalent to multiplication by 1. Here is another such vector for the same matrix: <span class="math display">\[
\left(\begin{array}{cc}2 &amp; 1 \\ 2&amp; 3\end{array}\right)\left(\begin{array}{c}1 \\ 2 \end{array}\right) = \left(\begin{array}{c}2 +2 \\ 2 + 6 \end{array}\right) =  \left(\begin{array}{c} 4 \\ 8 \end{array}\right)
\]</span> In this case, the vector is changed, but only by multiplication by a constant (4). Thus the geometric direction of the vector remained unchanged.</p>
<p>Generally, a square matrix has an associated set of vectors for which multiplication by the matrix is equivalent to multiplication by a constant. This can be written down as a definition:</p>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Definition
</div>
</div>
<div class="callout-body-container callout-body">
<p>An <em>eigenvector</em> of a square matrix <span class="math inline">\(A\)</span> is a vector <span class="math inline">\(\vec v\)</span> for which matrix multiplication by <span class="math inline">\(A\)</span> is equivalent to multiplication by a constant. This constant <span class="math inline">\(\lambda\)</span> is called the <em>eigenvalue</em> of <span class="math inline">\(A\)</span> corresponding the the eigenvector <span class="math inline">\(\vec v\)</span>. The relationship is summarized in the following equation: <span class="math display">\[
A  \times  \vec v = \lambda \vec v
\]</span></p>
</div>
</div>
<p>Note that this equation combines a matrix (<span class="math inline">\(A\)</span>), a vector (<span class="math inline">\(\vec v\)</span>) and a scalar <span class="math inline">\(\lambda\)</span>, and that both sides of the equation are column vectors. This definition is illustrated in figure <span class="math inline">\(\ref{fig:ch13_eigenvector}\)</span>, showing a vector (<span class="math inline">\(v\)</span>) multiplied by a matrix <span class="math inline">\(A\)</span>, and the resulting vector <span class="math inline">\(\lambda v\)</span>, which is in the same direction as <span class="math inline">\(v\)</span>, due to scalar multiplying all elements of a vector, thus either stretching it if <span class="math inline">\(\lambda&gt;1\)</span> or compressing it if <span class="math inline">\(\lambda &lt; 1\)</span>. This assumes that <span class="math inline">\(\lambda\)</span> is a real number, which is not always the case, but we will leave that complication aside for the purposes of this chapter.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="ch13/Eigenvalue_equation.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Illustration of the geometry of a matrix <span class="math inline">\(A\)</span> multiplying its eigenvector <span class="math inline">\(v\)</span>, resulting in a vector in the same direction <span class="math inline">\(\lambda v\)</span> (figure by Lantonov under CC BY-SA 4.0 via Wikimedia Commons)</figcaption><p></p>
</figure>
</div>
<p>The definition does not specify how many such eigenvectors and eigenvalues can exist for a given matrix <span class="math inline">\(A\)</span>. There are usually as many such vectors <span class="math inline">\(\vec v\)</span> and corresponding numbers <span class="math inline">\(\lambda\)</span> as the number of rows or columns of the square matrix <span class="math inline">\(A\)</span>, so a 2 by 2 matrix has two eigenvectors and two eigenvalues, a 5x5 matrix has 5 of each, etc. One ironclad rule is that there cannot be more distinct eigenvalues than the matrix dimension. Some matrices possess fewer eigenvalues than the matrix dimension, those are said to have a degenerate set of eigenvalues, and at least two of the eigenvectors share the same eigenvalue.</p>
<p>The situation with eigenvectors is trickier. There are some matrices for which any vector is an eigenvector, and others which have a limited set of eigenvectors. What is difficult about counting eigenvectors is that an eigenvector is still an eigenvector when multiplied by a constant. You can show that for any matrix, multiplication by a constant is commutative: <span class="math inline">\(cA = Ac\)</span>, where <span class="math inline">\(A\)</span> is a matrix and <span class="math inline">\(c\)</span> is a constant. This leads us to the important result that if <span class="math inline">\(\vec v\)</span> is an eigenvector with eigenvalue <span class="math inline">\(\lambda\)</span>, then any scalar multiple <span class="math inline">\(c \vec v\)</span> is also an eigenvector with the same eigenvalue. The following demonstrates this algebraically: <span class="math display">\[ A  \times  (c \vec v) = c A  \times  \vec v = c \lambda \vec v =  \lambda (c \vec v) \]</span> This shows that when the vector <span class="math inline">\(c \vec v\)</span> is multiplied by the matrix <span class="math inline">\(A\)</span>, it results in its being multiplied by the same number <span class="math inline">\(\lambda\)</span>, so by definition it is an eigenvector.</p>
<p>Therefore, an eigenvector <span class="math inline">\(\vec v\)</span> is not unique, as any constant multiple <span class="math inline">\(c \vec v\)</span> is also an eigenvector. It is more useful to think not of a single eigenvector <span class="math inline">\(\vec v\)</span>, but of a collection of vectors that can be interconverted by scalar multiplication that are all essentially the same eigenvector. Another way to represent this, if the eigenvector is real, is that an eigenvector as a <strong>direction that remains unchanged by multiplication by the matrix</strong>, such as direction of the vector <span class="math inline">\(v\)</span> in figure <span class="math inline">\(\ref{fig:ch13_eigenvector}\)</span>. As mentioned above, this is true only for real eigenvalues and eigenvectors, since complex eigenvectors cannot be used to define a direction in a real space.</p>
<p>To summarize, eigenvalues and eigenvectors of a matrix are a set of numbers and a set of vectors (up to scalar multiple) that describe the action of the matrix as a multiplicative operator on vectors. “Well-behaved” square <span class="math inline">\(n\)</span> by <span class="math inline">\(n\)</span> matrices have <span class="math inline">\(n\)</span> distinct eigenvalues and <span class="math inline">\(n\)</span> eigenvectors pointing in distinct directions. In a deep sense, the collection of eigenvectors and eigenvalues defines a matrix <span class="math inline">\(A\)</span>, which is why an older name for them is characteristic vectors and values.</p>
</section>
<section id="calculation-of-eigenvalues-on-paper" class="level3" data-number="16.2.2">
<h3 data-number="16.2.2" class="anchored" data-anchor-id="calculation-of-eigenvalues-on-paper"><span class="header-section-number">16.2.2</span> calculation of eigenvalues on paper</h3>
<p>Finding the eigenvalues and eigenvectors analytically, that is on paper, is quite laborious even for 3 by 3 or 4 by 4 matrices and for larger ones there is no analytical solution. In practice, the task is outsourced to a computer, and we will see how to do this using R in section <span class="math inline">\(\ref{sec:comp13}\)</span>. Nevertheless, it is useful to go through the process in 2 dimensions in order to gain an understanding of what is involved. From the definition <span class="math inline">\(\ref{def:eigen}\)</span> of eigenvalues and eigenvectors, the condition can be written in terms of the four elements of a 2 by 2 matrix:</p>
<p><span class="math display">\[  \left(\begin{array}{cc}a &amp; b \\c &amp; d\end{array}\right)\left(\begin{array}{c}v_1 \\ v_2 \end{array}\right) = \left(\begin{array}{c}av_1 +b v_2\\ cv_1+ dv_2 \end{array}\right) = \lambda \left(\begin{array}{c}v_1 \\ v_2 \end{array}\right)  \]</span> This is now a system of two linear algebraic equations, which we can solve by substitution. First, let us solve for <span class="math inline">\(v_1\)</span> in the first row, to get <span class="math display">\[ v_1 = \frac{-bv_2}{a-\lambda}\]</span> Then we substitute this into the second equation and get: <span class="math display">\[\frac{-bcv_2}{a-\lambda} +(d-\lambda)v_2 = 0\]</span> Since <span class="math inline">\(v_2\)</span> multiplies both terms, and is not necessarily zero, we require that its multiplicative factor be zero. Doing a little algebra, we obtain the following, known as the <em>characteristic equation</em> of the matrix: <span class="math display">\[ -bc +(a-\lambda)(d-\lambda) = \lambda^2-(a+d)\lambda +ad-bc = 0\]</span> This equation can be simplified by using two quantities we defined at the beginning of the section: the sum of the diagonal elements called the trace <span class="math inline">\(\tau = a+d\)</span>, and the determinant <span class="math inline">\(\Delta = ad-bc\)</span>. The quadratic equation has two solutions, dependent solely on <span class="math inline">\(\tau\)</span> and <span class="math inline">\(\Delta\)</span>: <span class="math display">\[
\lambda = \frac{\tau \pm \sqrt{\tau^2-4\Delta}}{2}
\]</span></p>
<p>This is the general expression for a 2 by 2 matrix, showing there are two possible eigenvalues. Note that if <span class="math inline">\(\tau^2-4\Delta&gt;0\)</span>, the eigenvalues are real, if <span class="math inline">\(\tau^2-4\Delta&lt;0\)</span>, they are complex (have real and imaginary parts), and if <span class="math inline">\(\tau^2-4\Delta=0\)</span>, there is only one eigenvalue. This situation is known as degenerate, because two eigenvectors share the same eigenvalue.</p>
<p><strong>Example.</strong> Let us take the same matrix we looked at in the previous subsection: <span class="math display">\[ A = \left(\begin{array}{cc}2 &amp; 1 \\ 2&amp; 3\end{array}\right)\]</span> The trace of this matrix is <span class="math inline">\(\tau = 2+3 =5\)</span> and the determinant is <span class="math inline">\(\Delta = 6 - 2 = 4\)</span>. Then by our formula, the eigenvalues are: <span class="math display">\[\lambda = \frac{5 \pm \sqrt{5^2-4 \times 4}}{2}  =  \frac{5 \pm 3}{2}  = 4, 1\]</span> These are the multiples we found in the example above, as expected.</p>
</section>
<section id="calculation-of-eigenvectors-on-paper" class="level3" data-number="16.2.3">
<h3 data-number="16.2.3" class="anchored" data-anchor-id="calculation-of-eigenvectors-on-paper"><span class="header-section-number">16.2.3</span> calculation of eigenvectors on paper</h3>
<p>The surprising fact is that, as we saw in the last subsection, the eigenvalues of a matrix can be found without knowing its eigenvectors! However, the converse is not true: to find the eigenvectors, one first needs to know the eigenvalues. Given an eigenvalue <span class="math inline">\(\lambda\)</span>, let us again write down the defining equation of the eigenvector for a generic 2 by 2 matrix: <span class="math display">\[  \left(\begin{array}{cc}a &amp; b \\c &amp; d\end{array}\right)\left(\begin{array}{c}v_1 \\ v_2 \end{array}\right) = \left(\begin{array}{c}av_1 +b v_2\\ cv_1+ dv_2 \end{array}\right) = \lambda \left(\begin{array}{c}v_1 \\ v_2 \end{array}\right)  \]</span> This vector equation is equivalent to two algebraic equations: <span class="math display">\[
av_1 + b v_2 = \lambda v_1
\]</span> <span class="math display">\[
cv_1 + d v_2 = \lambda v_2
\]</span> Since we have already found <span class="math inline">\(\lambda\)</span> by solving the characteristic equation, this is two linear equations with two unknowns (<span class="math inline">\(v_1\)</span> and <span class="math inline">\(v_2\)</span>). You may remember from advanced algebra that such equations may either have a single solution for each unknown, but sometimes they may have none, or infinitely many solutions. Since there are unknowns on both sides of the equation, we can make both equations be equal to zero: <span class="math display">\[
(a-\lambda)v_1 + b v_2 = 0
\]</span> <span class="math display">\[
cv_1 + (d-\lambda ) v_2 =0
\]</span> So the first equation yields the relationship $v_1 = -v_2 b/(a-) $ and the second equation is <span class="math inline">\(v_1 = -v_2(d-\lambda)/c\)</span>, which we already obtained in the last subsection. We know that these two equations must be the same, since the ratio of <span class="math inline">\(v_1\)</span> and <span class="math inline">\(v_2\)</span> is what defines the eigenvector. So we can use either expression to find the eigenvector.</p>
<p><strong>Example.</strong> Let us return to the same matrix we looked at in the previous subsection: <span class="math display">\[
A = \left(\begin{array}{cc}2 &amp; 1 \\ 2&amp; 3\end{array}\right)
\]</span> The eigenvalues of the matrix are 1 and 4. Using our expression above, where the element <span class="math inline">\(a=2\)</span> and <span class="math inline">\(b=1\)</span>, let us find the eigenvector corresponding to the eigenvalue 1: <span class="math display">\[
v_1 = - v_2 \times  1/(2-1) = - v_2
\]</span> Therefore the eigenvector is characterized by the first and second elements being negatives of each other. We already saw in the example two subsections above that the vector <span class="math inline">\((1,-1)\)</span> is such as eigenvector, but it is also true of the vectors <span class="math inline">\((-1,1)\)</span>, <span class="math inline">\((-\pi, \pi)\)</span> and <span class="math inline">\((10^6, -10^6)\)</span>. This infinite collection of vectors, all along the same direction, can be described as the eigenvector (or eigendirection) corresponding to the eigenvalue 1.</p>
<p>Repeating this procedure for <span class="math inline">\(\lambda = 4\)</span>, we obtain the linear relationship: <span class="math display">\[ v_1 = - v_2 \times  1/(2-4) = 0.5 v_2\]</span> Once again, the example vector we saw two subsections <span class="math inline">\((2,1)\)</span> is in agreement with our calculation. Other vectors that satisfy this relationship include <span class="math inline">\((10,5)\)</span>, <span class="math inline">\((-20,-10)\)</span>, and <span class="math inline">\((-0.4,-0.2)\)</span>. This is again a collection of vectors that are all considered the same eigenvector with eigenvalue 4 which are all pointing in the same direction, with the only difference being their length.</p>
</section>
<section id="exercises" class="level3" data-number="16.2.4">
<h3 data-number="16.2.4" class="anchored" data-anchor-id="exercises"><span class="header-section-number">16.2.4</span> Exercises</h3>
<p><img src="ch10/AB_trans_diag.png" class="img-fluid" alt="Model 1">{width = 25%} <img src="ch10/CD_trans_diag.png" class="img-fluid" alt="Model 2">{width = 25%} <img src="ch10/EF_trans_diag.png" class="img-fluid" alt="Model 3">{width = 25%} <img src="ch10/GH_trans_diag.png" class="img-fluid" alt="Model 4">{width = 25%}</p>
<p>For the following two-state Markov models a) calculate the eigenvalues of the transition matrix; b) calculate the corresponding eigenvectors and explain which one corresponds to the stationary distribution; c) use R to check that each of the eigenvectors obeys the definition <span class="math inline">\(\ref{def:eigen}\)</span> with its corresponding eigenvalue.</p>
<ol type="1">
<li><p>Use the model in the transition diagram in figure <span class="math inline">\(\ref{fig:ch10_trans_diags}\)</span> (Model 1).</p></li>
<li><p>Use the model in the transition diagram in figure <span class="math inline">\(\ref{fig:ch10_trans_diags}\)</span> (Model 2).</p></li>
<li><p>Use the model in the transition diagram in figure <span class="math inline">\(\ref{fig:ch10_trans_diags}\)</span> (Model 3).</p></li>
<li><p>Use the model in the transition diagram in figure <span class="math inline">\(\ref{fig:ch10_trans_diags}\)</span> (Model 4).</p></li>
<li><p>An ion channel can be in either open or closed states. If it is open, then it has probability 0.1 of closing in 1 microsecond; if closed, it has probability 0.3 of opening in 1 microsecond.</p></li>
<li><p>An individual can be either susceptible or infected, the probability of infection for a susceptible person is 0.05 per day, and the probability an infected person becoming susceptible is 0.12 per day.</p></li>
<li><p>The genotype of an organism can be either normal (wild type) or mutant. Each generation, a wild type individual has probability 0.03 of having a mutant offspring, and a mutant has probability 0.005 of having a wild type offspring.</p></li>
</ol>
<p>Consider a two-state Markov model with the transition matrix <span class="math inline">\(M\)</span>. As we know, the probability distribution vector at time <span class="math inline">\(t+1\)</span> is the matrix <span class="math inline">\(M\)</span> multiplied by the probability distribution vector at time <span class="math inline">\(t\)</span>: <span class="math display">\[ P(t+1) = M \times P(t) \]</span> Using eigenvectors and eigenvalues, the matrix multiplication (which is difficult) can be turned into multiplication by scalar numbers (which is much simpler). Suppose that the initial probability vector <span class="math inline">\(P(0)\)</span> can be written as a weighted sum (linear combination) of the two eigenvectors of the matrix <span class="math inline">\(M\)</span>, <span class="math inline">\(v_1\)</span> and <span class="math inline">\(v_2\)</span>: $P(0) = c_1 v_1 + c_2 v_2 $. I will explain exactly how to find the constants <span class="math inline">\(c_1\)</span> and <span class="math inline">\(c_2\)</span> a few paragraphs later, but for now, let’s go with this. Multiplying the matrix <span class="math inline">\(M\)</span> and this weighted sum (matrix multiplication can be distributed), we get: <span class="math display">\[ P(1) = M \times P(0) =  M \times (c_1 \vec v_1 + c_2 \vec v_2) = c_1 M \times \vec v_1 + c_2 M \times  \vec v_2  =  \]</span> <span class="math display">\[ = c_1 \lambda_1\vec v_1 + c_2 \lambda_2 \vec v_2 \]</span></p>
<p>The last step is due to definition <span class="math inline">\(\ref{def:eigen}\)</span> of eigenvectors and eigenvalues, which transformed matrix multiplication into multiplication by the corresponding eigenvalues. To see how useful this is, let us propagate the probability vector one more step: <span class="math display">\[ P(2) = M \times P(1) =  M \times ( c_1 \lambda_1\vec v_1 + c_2 \lambda_2 \vec v_2) =  c_1 \lambda_1^2\vec v_1 + c_2 \lambda_2^2 \vec v_2 \]</span></p>
<p>It should be clear that each matrix multiplication results in one additional multiplication of each eigenvector by its eigenvalue, so this allows us to write the general expression for the probability vector any number of time steps <span class="math inline">\(t\)</span> in the future, given the weights of the initial probability vector <span class="math inline">\(c_1\)</span> and <span class="math inline">\(c_2\)</span>: <span class="math display">\[ P(t) =c_1 \lambda_1^t \vec v_1 +c_2 \lambda_2^t \vec v_2 \]</span></p>
<p>The constants <span class="math inline">\(c_1, c_2\)</span> are determined by the initial conditions, while the constants <span class="math inline">\(\lambda_1, \lambda_2\)</span> are the eigenvalues and the vectors <span class="math inline">\(\vec v_1, \vec v_2\)</span> are the eigenvectors of the matrix <span class="math inline">\(M\)</span>. This expression is also true for Markov models of any number states, except that they have as many eigenvalues and eigenvectors as the dimensionality of the transition matrix. This is a hugely important development, because it allows us to predict how quickly the probability vectors converge to the stationary distribution. First, we need to use the following theorem:</p>
<div class="callout-important callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Theorem
</div>
</div>
<div class="callout-body-container callout-body">
<p>(Frobenius) A Markov transition matrix <span class="math inline">\(M\)</span>, characterized by having all nonnegative elements between 0 and 1, and whose columns all sum up to 1, has eigenvalues that are no greater than 1 in absolute value, including at least one eigenvalue equal to 1.</p>
</div>
</div>
<p>This theorem has an immediate important consequence for the dynamics of the probability vector. According to our formula describing the time evolution of the probability vector, the eigenvalues are raised to the power <span class="math inline">\(t\)</span>, which is the number of time steps. Therefore, for any eigenvalue which is less than 1, the number <span class="math inline">\(\lambda^t\)</span> grows smaller and approaches 0 as time goes on. Since the Frobenius theorem says that the eigenvalues cannot be greater than 1, the terms in the expression for the probability vector decay, except for the ones which are equal to 1 or to -1. The eigenvectors with eigenvalue 1 correspond to the stationary distribution that we introduced in the last chapter, and true to their name, they remain unchanged by time, since <span class="math inline">\(1^t =1\)</span> for all time. The ones with eigenvalue of -1 are a strange case, because they oscillate between positive and negative values, without decaying in absolute value.</p>
<p>We now have the skills to answer the following question of practical importance: how quickly does the probability vector approach the stationary distribution? (There may be more than one stationary distribution vector, but that doesn’t change the analysis.) This depends on how fast the contributions of other, non-stationary eigenvectors decay. If one of them has an eigenvalue of -1, then its contribution never decays - we saw an example of that in the cyclic 2-state matrix in chapter 12. If all of the eigenvalues other than the stationary one are less than 1 in absolute value, then all of them decay to zero, but at different rates. The one that decays slowest is the largest of the eigenvalues which are less than one - the second-largest, sometimes called the subdominant eigenvalue. This is the eigenvalue which determines the rate of convergence to the stationary distribution because it is the “last person standing” of the non-stationary eigenvalues, after the others have all vanished into insignificance.</p>
<p><strong>Example.</strong> Consider a Markov model with the following transition matrix <span class="math inline">\(M\)</span>, with eigenvectors and eigenvalues already solved by a computational assistant: <span class="math display">\[ M = \left(\begin{array}{ccc} 0.8 &amp; 0.1  &amp; 0.1  \\ 0.1 &amp; 0.8 &amp; 0.2 \\ 0.1 &amp; 0.1 &amp; 0.7 \end{array}\right)\]</span> <span class="math display">\[ \lambda_1 = 1 \; \vec v_1= \left(\begin{array}{c} 1/3     \\  5/12  \\ 1/4 \end{array}\right) \; \lambda_2 = 0.7  \;  \vec v_2= \left(\begin{array}{c} -1   \\ 1  \\ 0 \end{array}\right) \; \lambda_3 = 0.6  \;  \vec v_3= \left(\begin{array}{c} 0  \\ -1  \\ 1 \end{array}\right)\]</span> Let us compute, using the tools of this section, how the probability distribution vector evolves starting with <span class="math inline">\(P(0) = (7/12,5/12,0)\)</span> (I chose this particular initial probability distribution because it makes the algebra simple, but you can start with any initial vector you want.) The first step is to find what is called the decomposition of the initial probability vector into its three eigenvectors: <span class="math display">\[P(0)= \left(\begin{array}{c} 7/12 \\ 5/12  \\ 0 \end{array}\right) =  c_1\left(\begin{array}{c} 1/3     \\  5/12  \\ 1/4 \end{array}\right) + c_2 \left(\begin{array}{c} -1   \\ 1  \\ 0 \end{array}\right) + c_3\left(\begin{array}{c} 0  \\ -1  \\ 1 \end{array}\right)\]</span></p>
<p>It turns out that the desired coefficients are <span class="math inline">\(c_1 = 1\)</span>, <span class="math inline">\(c_2 = -1/4\)</span>, and <span class="math inline">\(c_3 = -1/4\)</span> - you can check yourself that they add up to the initial vector we want. Therefore, after some number of time steps <span class="math inline">\(t\)</span>, the probability distribution vector can be expressed like this: <span class="math display">\[ P(t) = \left(\begin{array}{c} 1/3     \\  5/12  \\ 1/4 \end{array}\right) -\frac{1}{4} 0.7^t \left(\begin{array}{c} -1   \\ 1  \\ 0 \end{array}\right)  -\frac{1}{4} 0.6^t \left(\begin{array}{c} 0  \\ -1  \\ 1 \end{array}\right)\]</span> All three eigenvectors, multiplied by their eigenvalues to the power <span class="math inline">\(t\)</span>, are present in the expression, but the third eigenvalue (0.6) decays much faster than the second one (0.7). After 5 time steps, <span class="math inline">\(0.7^5 = 0.168\)</span>, while <span class="math inline">\(0.6^5 = 0.078\)</span>, so the contribution of the third eigenvector is about half that of the second; after 10 time steps, <span class="math inline">\(0.7^{10} = 0.028\)</span>, while <span class="math inline">\(0.6^{10} = 0.006\)</span>, so the contribution of the third eigenvector is about one-fifth that of the second; after 20 time steps, <span class="math inline">\(0.7^{20} \approx 8 \times 10^{-4}\)</span>, while <span class="math inline">\(0.6^{20} \approx 4 \times 10^{-5}\)</span>, so the contribution of the third eigenvector is about one-twentieth that of the second. The trend is clear: although 0.6 and 0.7 are not that different, raising them to higher powers makes the ratio between them get smaller, until the contribution of the smaller of the two eigenvalues is negligible, however you’d like to define that term - less than 1%? less than 0.01%? - eventually the smaller eigenvalue will reach that level of insignificance. This is illustrates why the rate of convergence to the stationary distribution <span class="math inline">\((1/3, 5/12, 1/4)\)</span> is determined by the eigenvalue 0.7, and the smaller eigenvalue can be neglected.</p>
</section>
</section>
<section id="eigenvectors-in-r" class="level2" data-number="16.3">
<h2 data-number="16.3" class="anchored" data-anchor-id="eigenvectors-in-r"><span class="header-section-number">16.3</span> Eigenvectors in R</h2>
<p>R has a standard set of functions to handle linear algebra computations. One of the most important of those is the calculation of eigenvectors and eigenvalues, also known as <em>diagonalization</em> of a matrix, for reasons you’ll understand if you take a proper linear algebra course, as I strongly recommend for anyone who intends to be involved in quantitative biology.</p>
<p>The function to calculate the special numbers and vectors is <code>eigen()</code>, and the name of the matrix goes between the parentheses. The function returns a data frame as its output, with <code>$values</code> and <code>$vectors</code> storing the eigenvalues and the eigenvectors, respectively. Here is a script to define the matrix we analyzed in the examples in section <span class="math inline">\(\ref{sec:math13}\)</span> and then calculate and print out its eigenvalues and eigenvectors:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>test.matrix <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">c</span>(<span class="dv">2</span>,  <span class="dv">2</span>, <span class="dv">1</span>, <span class="dv">3</span>),<span class="at">nrow=</span><span class="dv">2</span>)</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(test.matrix)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>     [,1] [,2]
[1,]    2    1
[2,]    2    3</code></pre>
</div>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>result <span class="ot">&lt;-</span> <span class="fu">eigen</span>(test.matrix)</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(result<span class="sc">$</span>values)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 4 1</code></pre>
</div>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(result<span class="sc">$</span>vectors)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>           [,1]       [,2]
[1,] -0.4472136 -0.7071068
[2,] -0.8944272  0.7071068</code></pre>
</div>
</div>
<p>The resulting eigenvalues and 4 and 1, just as we computed above. However, the situation with eigenvectors is trickier. In the R output they are presented as column vectors, and you may notice that they have the same ratio of elements as the calculated eigenvectors, but the numbers may appear strange. As we discussed, the eigenvector for a particular eigenvalue can be multiplied by any constant and still be a valid eigenvector. R (and other computational tools) thus has a choice about which eigenvector to output, and it typically prefers to <em>normalize</em> its eigenvectors by making sure their length (Euclidean norm) is equal to 1. You may observe that if you take square the two elements of one of the eigenvectors in the output and add them up, the result will be 1. This is convenient for some purposes, but doesn’t make for clean looking vectors in terms of the elements. But since we can multiply (or divide) the eigenvector by any constant, we can choose to make it look cleaner, for instance by making sure one of its elements is equal to 1. The next script illustrates how to make that happen by dividing each eigenvector by the value of its first element, which results in the same form of eigenvectors that we wrote down in the analytic solution:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>eigen1<span class="ot">&lt;-</span>result<span class="sc">$</span>vectors[,<span class="dv">1</span>]<span class="sc">/</span>result<span class="sc">$</span>vectors[<span class="dv">1</span>,<span class="dv">1</span>]</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(eigen1)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 1 2</code></pre>
</div>
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>eigen2<span class="ot">&lt;-</span>result<span class="sc">$</span>vectors[,<span class="dv">2</span>]<span class="sc">/</span>result<span class="sc">$</span>vectors[<span class="dv">1</span>,<span class="dv">2</span>]</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(eigen2)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1]  1 -1</code></pre>
</div>
</div>
<p>Let us now analyze a transition matrix, for example one that we investigated in the last subsection of section <span class="math inline">\(\ref{sec:math13}\)</span>. This is how I obtained the eigenvalues and eigenvectors that were used to make predictions about the evolution of the probability vector. One important issue here is how best to normalize the eigenvectors. The eigenvector corresponding the eigenvalue 1 is (a multiple of) the stationary distribution vector, but in order to make it a probability distribution vector, its elements must add up to 1. The way to ensure this property is to divide the eigenvector by the sum of its elements. The two other eigenvector are not probability vectors, in fact they both contain negative elements, so it doesn’t make sense to normalize them the same way. Any normalization of these vectors is arbitrary, so I choose to divide each by the value of one of its elements. Notice one more strange thing: the two non-stationary eigenvectors each contain a very small number on the order of <span class="math inline">\(10^{-16}\)</span>. You may remember from section <span class="math inline">\(\ref{sec:comp1}\)</span> that this is the limit of precision for storing numbers in R, so these values are not real, they are errors, and instead should be replaced with zeros.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>trans.matrix <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">c</span>(<span class="fl">0.8</span>,<span class="fl">0.1</span>,<span class="fl">0.1</span>,<span class="fl">0.1</span>,<span class="fl">0.8</span>,<span class="fl">0.1</span>,<span class="fl">0.1</span>,<span class="fl">0.2</span>,<span class="fl">0.7</span>),<span class="at">nrow=</span><span class="dv">3</span>)</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(trans.matrix)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>     [,1] [,2] [,3]
[1,]  0.8  0.1  0.1
[2,]  0.1  0.8  0.2
[3,]  0.1  0.1  0.7</code></pre>
</div>
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>result <span class="ot">&lt;-</span> <span class="fu">eigen</span>(trans.matrix)</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(result<span class="sc">$</span>values)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 1.0 0.7 0.6</code></pre>
</div>
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(result<span class="sc">$</span>vectors)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>          [,1]          [,2]          [,3]
[1,] 0.5656854  7.071068e-01 -8.047812e-16
[2,] 0.7071068 -7.071068e-01 -7.071068e-01
[3,] 0.4242641  1.311781e-16  7.071068e-01</code></pre>
</div>
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>eigen1<span class="ot">&lt;-</span>result<span class="sc">$</span>vectors[,<span class="dv">1</span>]<span class="sc">/</span><span class="fu">sum</span>(result<span class="sc">$</span>vectors[,<span class="dv">1</span>])</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(eigen1)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.3333333 0.4166667 0.2500000</code></pre>
</div>
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>eigen2<span class="ot">&lt;-</span>result<span class="sc">$</span>vectors[,<span class="dv">2</span>]<span class="sc">/</span>result<span class="sc">$</span>vectors[<span class="dv">1</span>,<span class="dv">2</span>]</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(eigen2)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1]  1.000000e+00 -1.000000e+00  1.855139e-16</code></pre>
</div>
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>eigen3<span class="ot">&lt;-</span> result<span class="sc">$</span>vectors[,<span class="dv">3</span>]<span class="sc">/</span>result<span class="sc">$</span>vectors[<span class="dv">2</span>,<span class="dv">3</span>]</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(eigen3)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1]  1.138132e-15  1.000000e+00 -1.000000e+00</code></pre>
</div>
</div>
<p>We now have the tools to calculate the eigenvalues and eigenvectors of transition matrices, and we can use them to predict two things: 1) the stationary distribution, which is the properly normalized eigenvector with eigenvalue 1, and 2) how quickly probability vectors converge to the stationary distribution, which depends on the second-largest eigenvalue. The first task was already accomplished in the script above.</p>
<p>The second question needs clarification: what does it mean to “converge”, that is, how close does the probability distribution need to be to the stationary one, before we can say it converged? The answer is necessarily arbitrary, because a probability vector never actually reaches the stationary distribution unless the initial vector is stationary. Distances between vectors are typically measured using the standard Euclidean definition of distance (the square root of the sum of the squares of differences of the elements). The following R script propagates an initial probability vector for 30 time steps, and at each step calculates the distance of the probability vector to the stationary vector (which we calculated above).</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>nstep <span class="ot">&lt;-</span> <span class="dv">30</span> <span class="co"># set number of time steps</span></span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>stat.vec <span class="ot">&lt;-</span> eigen1 <span class="co"># set the stationary distribution</span></span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a>prob.vec <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>) </span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a><span class="co"># set the initial probability vector</span></span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a>dist.vec <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="dv">0</span>,nstep) <span class="co"># initialize distance vector</span></span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a>dist.vec[<span class="dv">1</span>] <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(<span class="fu">sum</span>((prob.vec <span class="sc">-</span>stat.vec)<span class="sc">^</span><span class="dv">2</span>))</span>
<span id="cb23-7"><a href="#cb23-7" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">2</span><span class="sc">:</span>nstep) { </span>
<span id="cb23-8"><a href="#cb23-8" aria-hidden="true" tabindex="-1"></a>  <span class="co"># propagate the probability vector</span></span>
<span id="cb23-9"><a href="#cb23-9" aria-hidden="true" tabindex="-1"></a>  prob.vec <span class="ot">&lt;-</span> trans.matrix <span class="sc">%*%</span> prob.vec</span>
<span id="cb23-10"><a href="#cb23-10" aria-hidden="true" tabindex="-1"></a>  dist.vec[i] <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(<span class="fu">sum</span>((prob.vec <span class="sc">-</span>stat.vec)<span class="sc">^</span><span class="dv">2</span>))</span>
<span id="cb23-11"><a href="#cb23-11" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The distance to the stationary vector as a function of time is plotted in figure <span class="math inline">\(\ref{fig:mixing_time}\)</span> along with the exponential decay of the second leading eigenvalue <span class="math inline">\(0.7^t\)</span> over the same number of time steps. While initially the two are not identical, over time the contribution of the smaller eigenvalue becomes insignificant and the second-leading eigenvalue describes the approach of the probability vector to the stationary distribution. The effect of choosing a different initial probability vector is not very large, with the exception of the case when the initial vector is exactly the stationary distribution (in which case the distance is zero initially and remains zero) or exactly the eigenvector with the smaller eigenvalue, in which case the smallest eigenvalue governs the convergence.</p>
<div class="cell" data-layout-align="center" data-fig.asp="0.75">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(dist.vec,<span class="at">xlab=</span><span class="st">'time step'</span>, <span class="at">ylab=</span><span class="st">'distance from stationary distribution'</span>,<span class="at">cex=</span><span class="fl">1.5</span>, <span class="at">cex.axis=</span><span class="fl">1.5</span>,<span class="at">cex.lab=</span><span class="fl">1.5</span>)</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>steps<span class="ot">&lt;-</span><span class="dv">1</span><span class="sc">:</span>nstep <span class="co"># vector of time steps </span></span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a><span class="co"># plot the exponential decay of the eigenvalue</span></span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(steps,<span class="fl">0.7</span><span class="sc">^</span>steps,<span class="at">col=</span><span class="st">'red'</span>,<span class="at">lwd=</span><span class="dv">3</span>) </span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a>prob.vec <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>) <span class="co"># set the initial probability vector</span></span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a>dist.vec <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="dv">0</span>,nstep) <span class="co"># initialize distance vector</span></span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a>dist.vec[<span class="dv">1</span>] <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(<span class="fu">sum</span>((prob.vec <span class="sc">-</span>stat.vec)<span class="sc">^</span><span class="dv">2</span>))</span>
<span id="cb24-8"><a href="#cb24-8" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">2</span><span class="sc">:</span>nstep) { <span class="co"># propagate the probability vector</span></span>
<span id="cb24-9"><a href="#cb24-9" aria-hidden="true" tabindex="-1"></a>  prob.vec <span class="ot">&lt;-</span> trans.matrix <span class="sc">%*%</span> prob.vec</span>
<span id="cb24-10"><a href="#cb24-10" aria-hidden="true" tabindex="-1"></a>  dist.vec[i] <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(<span class="fu">sum</span>((prob.vec <span class="sc">-</span>stat.vec)<span class="sc">^</span><span class="dv">2</span>))</span>
<span id="cb24-11"><a href="#cb24-11" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb24-12"><a href="#cb24-12" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(dist.vec,<span class="at">xlab=</span><span class="st">'time step'</span>, <span class="at">ylab=</span><span class="st">'distance from stationary distribution'</span>, <span class="at">cex=</span><span class="fl">1.5</span>, <span class="at">cex.axis=</span><span class="fl">1.5</span>,<span class="at">cex.lab=</span><span class="fl">1.5</span>)</span>
<span id="cb24-13"><a href="#cb24-13" aria-hidden="true" tabindex="-1"></a> <span class="co"># plot the exponential decay of the eigenvalue</span></span>
<span id="cb24-14"><a href="#cb24-14" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(steps,<span class="fl">0.7</span><span class="sc">^</span>steps,<span class="at">col=</span><span class="st">'red'</span>,<span class="at">lwd=</span><span class="dv">3</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="markov_eigen_files/figure-html/comp13-5-1.png" class="img-fluid figure-img" style="width:50.0%"></p>
<p></p><figcaption class="figure-caption">Decay of the distance from the stationary distribution (circles) and the exponential decay of the second-leading eigenvalue (red lines), plotted for two different initial probability distributions.</figcaption><p></p>
</figure>
</div>
</div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="markov_eigen_files/figure-html/comp13-5-2.png" class="img-fluid figure-img" style="width:50.0%"></p>
<p></p><figcaption class="figure-caption">Decay of the distance from the stationary distribution (circles) and the exponential decay of the second-leading eigenvalue (red lines), plotted for two different initial probability distributions.</figcaption><p></p>
</figure>
</div>
</div>
</div>
</section>
<section id="molecular-evolution" class="level2" data-number="16.4">
<h2 data-number="16.4" class="anchored" data-anchor-id="molecular-evolution"><span class="header-section-number">16.4</span> Molecular evolution</h2>
<p>Substitution mutations in DNA sequences can be modeled as a Markov process, where each base in the sequence mutates independently of others with a transition matrix <span class="math inline">\(M\)</span>. Let the bases A, G, C, T correspond to states 1 through 4, respectively. A classic model for base substitution from one generation to the next is based on the assumption that all substitution mutations are equally likely, and that the fraction <span class="math inline">\(a\)</span> of the sequence will be substituted each generation. This was proposed by Jukes and Cantor to calculate the rate of divergence of DNA (or protein) sequences from their common ancestor. The model is illustrated as a transition diagram in figure <span class="math inline">\(\ref{fig:ch13_jc_model}\)</span>, with the four letters representing the states of a particular site in a DNA sequence, and all transition probabilities equal to <span class="math inline">\(a\)</span>.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="ch13/JC_trans_diag.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Transition diagram for a four-state molecular evolution model for one letter in a DNA sequence. The mutation rate <span class="math inline">\(a\)</span> is the probability that the letter is replaced by a different one over one generation. The transition probabilities between individual letters are <span class="math inline">\(a/3\)</span> (not labeled).</figcaption><p></p>
</figure>
</div>
<p>Then the probability of any particular transition, say from T to C is <span class="math inline">\(a/3\)</span>, while the probability of not having a substitution is equal to <span class="math inline">\(1-a\)</span>. This is known as the Jukes-Cantor model and it predicts that the fraction of letters in a sequence at generation <span class="math inline">\(t+1\)</span> depends on the distribution in generation <span class="math inline">\(t\)</span> as follows: <span class="math display">\[   \left(\begin{array}{c} P_A(t+1) \\ P_G(t+1) \\ P_C(t+1) \\ P_T(t+1) \end{array}\right) = \left(\begin{array}{cccc}1-a &amp; a/3 &amp; a/3 &amp; a/3 \\a/3 &amp; 1-a &amp; a/3 &amp; a/3 \\a/3 &amp; a/3 &amp; 1-a &amp; a/3 \\a/3 &amp; a/3 &amp; a/3 &amp; 1-a\end{array}\right) \left(\begin{array}{c} P_A(t) \\ P_G(t) \\ P_C(t) \\ P_T(t) \end{array}\right) \]</span> This model is very simple: it only considers substitutions, although other mutations are possible, e.g.&nbsp;insertions and deletions, although they are typically more disruptive and thus more rare, and it treats all substitutions as equally likely, which is not empirically true. The benefit is that the number <span class="math inline">\(a\)</span> is the only parameter in this model, which represents the mutation rate at each site per generation. This makes is easy to compute the eigenvectors and eigenvalues of the model in general. It turns out that the four eigenvectors do not depend on the parameter <span class="math inline">\(a\)</span>, only the eigenvalues do: <span class="math display">\[  \left(\begin{array}{c} 1/4 \\ 1/4 \\ 1/4 \\ 1/4 \end{array}\right) \; \lambda =1; \; \left(\begin{array}{c} 1/4 \\ -1/4 \\ 1/4 \\ -1/4 \end{array}\right) \lambda =1-\frac{4}{3}a \]</span> <span class="math display">\[ \left(\begin{array}{c} 1/4 \\ -1/4 \\ -1/4 \\ 1/4 \end{array}\right) \; \lambda =1-\frac{4}{3}a;  \; \left(\begin{array}{c} 1/4 \\ -1/4 \\ 1/4 \\ -1/4 \end{array}\right) \; \lambda =1-\frac{4}{3}a\]</span> Notice two things: first, the first eigenvector is the equilibrium distribution and has the same frequencies for all four bases. Second, the three eigenvectors with eigenvalues smaller than 1 have negative entries, so they cannot be probability distributions themselves (although as linear combinations with the first one, they may be, depending on the coefficients.)</p>
<p>Computationally, this allows us to predict the time evolution of a distribution of bases in a DNA sequence for any given initial distribution, by using repeated matrix multiplication as above. Figure <span class="math inline">\(\ref{fig:ch13_jc_prob_evol}\)</span> shows the probability distribution of nucleotides based on the Jukes-Cantor model starting with the nucleotide A for two different mutation rates, propagated for different lengths of time. It is evident that for a faster substitution rate the approach to the equilibrium distribution is faster. This demonstrates how the second-largest eigenvalue of the transition matrix determines the speed of convergence to the equilibrium distribution, as we postulated in section <span class="math inline">\(\ref{sec:math13}\)</span>.</p>
<div class="cell">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="markov_eigen_files/figure-html/bio14-1-1.png" class="img-fluid figure-img" style="width:49.0%"></p>
<p></p><figcaption class="figure-caption">Evolution of probability vectors in the Jukes-Cantor model with bar graphs showing proportion of letters A, G, T, C (red, green, blue, magenta): a) 1000 generations with mutation rate a = 0.001; b) 200 generations with substitution rate a = 0.01.</figcaption><p></p>
</figure>
</div>
</div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="markov_eigen_files/figure-html/bio14-1-2.png" class="img-fluid figure-img" style="width:49.0%"></p>
<p></p><figcaption class="figure-caption">Evolution of probability vectors in the Jukes-Cantor model with bar graphs showing proportion of letters A, G, T, C (red, green, blue, magenta): a) 1000 generations with mutation rate a = 0.001; b) 200 generations with substitution rate a = 0.01.</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>Thus, the Jukes-Cantor model provides a prediction of the time-dependent evolution of the probability distribution of each letter, starting with an initial distribution. In reality, we would like to answer the following question: given two DNA sequences in the present (e.g.&nbsp;from different species), what is the length of time they spent evolving from a common ancestor?</p>
<section id="time-since-divergence" class="level3" data-number="16.4.1">
<h3 data-number="16.4.1" class="anchored" data-anchor-id="time-since-divergence"><span class="header-section-number">16.4.1</span> time since divergence</h3>
<p>To do this, we need to some preliminary work. The first step is to compute the probability that a letter at a particular site remain unchanged after <span class="math inline">\(t\)</span> generations. Because all the nucleotides are equivalent in the Jukes-Cantor model, it’s the same as finding the probability that a nucleotide in state A remains in A after <span class="math inline">\(t\)</span> generations. As we saw in section <span class="math inline">\(\ref{sec:math13}\)</span>, we can calculate the frequency distribution after <span class="math inline">\(t\)</span> time steps by using the decomposition of the initial probability vector <span class="math inline">\(\vec P(0)\)</span> into a weighted sum of the eigenvectors. We take <span class="math inline">\(\vec P_0 = (1,0,0,0)\)</span> (the initial state is A) and this can be written as a sum of the four eigenvectors of the matrix <span class="math inline">\(M\)</span>: <span class="math display">\[ \left(\begin{array}{c} 1 \\ 0 \\ 0 \\ 0 \end{array}\right) = \left(\begin{array}{c} 1/4 \\ 1/4 \\ 1/4 \\ 1/4 \end{array}\right) + \left(\begin{array}{c} 1/4 \\ -1/4 \\ 1/4 \\ -1/4 \end{array}\right) +  \left(\begin{array}{c} 1/4 \\ -1/4 \\ -1/4 \\ 1/4 \end{array}\right) + \left(\begin{array}{c} 1/4 \\ -1/4 \\ 1/4 \\ -1/4 \end{array}\right) \]</span></p>
<p>Therefore, the transition matrix <span class="math inline">\(M\)</span> can be applied to each eigenvector separately, and each matrix multiplication is a multiplication by the appropriate eigenvalue. Thus, <span class="math display">\[ P(t) = M^t P(0) =  \]</span> <span class="math display">\[ = 1^t \left(\begin{array}{c} 1/4 \\ 1/4 \\ 1/4 \\ 1/4 \end{array}\right) + (1-\frac{4}{3}a)^t\left( \left(\begin{array}{c} 1/4 \\ -1/4 \\ 1/4 \\ -1/4 \end{array}\right) +  \left(\begin{array}{c} 1/4 \\ -1/4 \\ -1/4 \\ 1/4 \end{array}\right) + \left(\begin{array}{c} 1/4 \\ -1/4 \\ 1/4 \\ -1/4 \end{array}\right) \right)\]</span> The first element of <span class="math inline">\(P(t)\)</span> is the probability of a nucleotide remaining <span class="math inline">\(A\)</span> after <span class="math inline">\(t\)</span> generation, and it is: $ P_A (t) = 1/4 +3/4(1-a)^t $. For <span class="math inline">\(t=0\)</span>, the probability is 1, as it should be, and as $t $, <span class="math inline">\(P_A (t) \rightarrow 1/4\)</span>, since this is the equilibrium probability distribution. Note that the expression is the same for all the other letters, so we have found the expression for any nucleotide remaining the same after <span class="math inline">\(t\)</span> generations.</p>
<p>Now let us get to the question of calculating the time that two sequences have evolved from each other. Denote by <span class="math inline">\(m\)</span> the fraction of sites in two aligned sequences with different letters, and <span class="math inline">\(q\)</span> is the probability of a nucleotide remaining the same, which is the given by the expression for <span class="math inline">\(P_A(t)\)</span>. Thus <span class="math inline">\(m = 1 - q = 3/4 - 3/4(1-\frac{4}{3}a)^t\)</span>. This can be solved for <span class="math inline">\(t\)</span>: <span class="math display">\[ t = \frac{\log (1 -  \frac{4}{3} m)}{\log (1 -  \frac{4}{3} a)}\]</span></p>
</section>
<section id="phylogenetic-distance" class="level3" data-number="16.4.2">
<h3 data-number="16.4.2" class="anchored" data-anchor-id="phylogenetic-distance"><span class="header-section-number">16.4.2</span> phylogenetic distance</h3>
<p>However, if we do not know the mutation rate <span class="math inline">\(a\)</span>, and until recently it was rarely known with any precision, this formula is of limited practical use. Jukes and Cantor neatly finessed the problem by calculating the <em>phylogenetic distance</em> between the two sequences, which is defined as <span class="math inline">\(d = a t\)</span>, or the mean number of substitutions that occurred per nucleotide during <span class="math inline">\(t\)</span> generations, with mutation rate <span class="math inline">\(a\)</span> (substitutions per nucleotide per generation). Note that this distance is not directly measurable from the fraction of different nucleotides in the two sequences, because it counts all substitutions, including those which reverse an earlier mutation, and cause the sequence to revert to its initial letter.</p>
<p>Now, let us assume <span class="math inline">\(a\)</span> is small, as is usually the case; as you recall from section <span class="math inline">\(\ref{sec:bio3}\)</span>, for humans the rate of substitutions per generation per nucleotide is about <span class="math inline">\(10^{-8}\)</span>. By Taylor expansion of the logarithm around 1, <span class="math inline">\(\log (1 - \frac{4}{3} a) \approx - \frac{4}{3}a\)</span>. Using the formula for <span class="math inline">\(t\)</span> from above with this approximation, we find the Jukes-Cantor phylogenetic distance to be: <span class="math display">\[ d_{JC} = \frac{\log (1 -  \frac{4}{3} m)}{ - \frac{4}{3}a} a =  -\frac{3}{4}\log (1 -  \frac{4}{3} m)\]</span> This formula has the correct behavior in the two limits: when <span class="math inline">\(m = 0\)</span>, <span class="math inline">\(d_{JC} = 0\)</span> (identical sequences have zero distance), and when <span class="math inline">\(m \rightarrow 3/4\)</span>, <span class="math inline">\(d_{JC} \rightarrow \infty\)</span>, since 3/4 is the maximum possible fraction of differences under the Jukes-Cantor model. Thus, we have obtained an analytic formula for the phylogenetic distance based on the fraction of differences between two homologous sequences.</p>
<p>A useful discussion of the details of connecting the Jukes-Cantor distance to phylogenetic distances can be found on the treethinkers blog , describing several possible adjustments that can be made to the formula. It can then be used to calculate a <em>distance matrix</em> that contains distances between all pairs from collection of sequences. There are algorithms that can generate phylogenetic trees based on a distance matrix, which minimize the total phylogenetic distance of all the branches. This is one approach that biologists use to infer evolutionary relationships from molecular sequence data, but there are many others, including maximum likelihood and parsimony models.</p>
</section>
<section id="kimura-model" class="level3" data-number="16.4.3">
<h3 data-number="16.4.3" class="anchored" data-anchor-id="kimura-model"><span class="header-section-number">16.4.3</span> Kimura model</h3>
<p>One can devise more sophisticated models of base substitution. There are two classes of nucleotide bases: purines (A,G) and pyrimidines (C,T). One may consider the difference in rates of <em>transitions</em> (substitutions within the classes) and <em>transversions</em> (substitutions of purines by pyrimidines and vice versa). This is known as the Kimura model and can be written as follows as Markov chain: <span class="math display">\[   \left(\begin{array}{c} P_A \\ P_G \\ P_C \\ P_T \end{array}\right)_{t+1}  = \left(\begin{array}{cccc}1-\beta-\gamma &amp; \beta &amp; \gamma/2 &amp;  \gamma/2 \\ \beta  &amp; 1-\beta-\gamma &amp; \gamma/2 &amp; \gamma/2 \\ \gamma/2 &amp; \gamma/2 &amp; 1-\beta-\gamma&amp; \beta  \\ \gamma/2 &amp;\gamma/2 &amp; \beta  &amp; 1-\beta-\gamma \end{array}\right) \left(\begin{array}{c} P_A \\ P_G \\ P_C \\ P_T \end{array}\right)_t \]</span> where <span class="math inline">\(\beta\)</span> is the rate of transitions and <span class="math inline">\(\gamma\)</span> is the rate of transversions per generation. This model has two different parameters, and as those two rates are empirically different (transitions occur more frequently, since the bases are more chemically similar) the model is more realistic. Whether or not it is worth the additional complexity depends on the question at hand.</p>
</section>
<section id="divergence-of-human-and-chimp-genomes" class="level3" data-number="16.4.4">
<h3 data-number="16.4.4" class="anchored" data-anchor-id="divergence-of-human-and-chimp-genomes"><span class="header-section-number">16.4.4</span> divergence of human and chimp genomes</h3>
<p>The human genome sequence was reported in 2001 and the genome of our closest living inter-species relative, the chimpanzee, was published in 2005 . Comparison between the two genomes has allowed us to peer into the evolutionary history of the two lineages. Although our genomes are quite similar, several large genome changes separate us and chimps. One particularly big difference is the number of chromosomes: human genome is organized into 23 chromosomes while chimps have 24. This suggests two options: either the common ancestor had 23 chromosomes and one of them split in the chimpanzee lineage, or the common ancestor has 24 and an two of them merged in the human lineage. Determining the complete sequences has answered that questions decisively: the human chromosome 2 is a result of a fusion of two ancestral chromosomes. This is supported by multiple observations, in particular that the two halves of human chromosome 2 can be matched to contiguous sequences in two separate chimp chromosomes, as well as the presence of telomere sequences, which form caps on the ends of chromosomes, in the middle of human chromosome 2. The entire story of great ape chromosomal is still in the process of being investigated, here is one recent study that examines the history of gorilla and orangutan chromosomal modification as well .</p>
<p>Besides the large-scale changes, many point mutations have accumulated since the split of the two lineages. Large portions of the two genomes can be directly aligned, and the the differences measured as fractions of letters that do not match in a particular region. In the original report on the chimp genome , the authors calculated the divergence (mean fraction of different letters in an aligned sequence) to be 1.23%. The distribution of these divergences is shown in figure <span class="math inline">\(\ref{fig:ch13_human_chimp}\)</span>. This information allows us to calculate the likely time since the split of the two lineages to be about 6-7 million years. The model used to make that calculation is more sophisticated than the simple Jukes-Cantor substitution model, and uses a maximum-likelihood approach.</p>
<p><img src="ch13/human_chimp1.png" class="img-fluid" alt="Histogram of divergence values between human and chimp gehnomes for 1 Mb long segments of the genome (blue indicates segments from autosomes, red - from the X chromosome and green - from the Y chromosome; (figure from , used by permission)"> <img src="ch13/human_chimp2.png" class="img-fluid" alt="Divergence values between human and chimp genomes distribution for different chromosomes (figure from , used by permission)"></p>
</section>
<section id="discussion-questions" class="level3" data-number="16.4.5">
<h3 data-number="16.4.5" class="anchored" data-anchor-id="discussion-questions"><span class="header-section-number">16.4.5</span> Discussion questions</h3>
<p>The following questions refer to the study <a href="https://www.nature.com/articles/nature04072">Initial sequence of the chimpanzee genome and comparison with the human genome</a></p>
<ol type="1">
<li><p>Name some possible challenges in comparing genomes between two different species.</p></li>
<li><p>Speculate on the biological reasons for the disparities in the substitution rates in different chromosomes and in the distal parts (closer to the end) compared to the proximal (closer to the center).</p></li>
<li><p>What are the differences in the transposable elements (SINES and LINES) between the two genomes? Are there any explanations offered for this observation?</p></li>
<li><p>Explain the meaning of the <span class="math inline">\(K_A/K_S\)</span> ratio for studying the effect of natural selection.</p></li>
</ol>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./markov_stat.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Stationary distributions of Markov chains</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./summary.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Summary</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->



</body></html>