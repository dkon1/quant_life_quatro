<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.280">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Quantifying Life - 8&nbsp; Sampling distribution and estimation</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./lindiff.html" rel="next">
<link href="./bayes.html" rel="prev">
<link href="./Kondrashov_Comp.jpeg" rel="icon" type="image/jpeg">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<meta name="twitter:title" content="Quantifying Life - 8&nbsp; Sampling distribution and estimation">
<meta name="twitter:description" content="One of the most common tasks in any experimental science is to measure the value of a quantity by performing repeated experiments.">
<meta name="twitter:card" content="summary">
</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Sampling distribution and estimation</span></h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Quantifying Life</a> 
    </div>
      </div>
      <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">Preface</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./counting.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Arithmetic and variables</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./functions.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Functions and their graphs</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./descriptive.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Describing data sets</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./probdist.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Random variables and distributions</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./linreg.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Linear regression</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./independence.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Independence</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./bayes.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Prior knowledge and Bayesian thinking</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./sampling.html" class="sidebar-item-text sidebar-link active"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Sampling distribution and estimation</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./lindiff.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Linear difference equations</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./graph_odes.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Graphical analysis of ordinary differential equations</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ode_sols.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Solutions of ordinary differential equations</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./markov_model.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Markov models with discrete states</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./markov_evol.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Probability distributions of Markov chains</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./markov_stat.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Stationary distributions of Markov chains</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./markov_eigen.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Dynamics of Markov models</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./summary.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Summary</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">References</a>
  </div>
</li>
    </ul>
    </div>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#law-of-large-numbers" id="toc-law-of-large-numbers" class="nav-link active" data-scroll-target="#law-of-large-numbers"><span class="toc-section-number">8.1</span>  Law of large numbers</a>
  <ul class="collapse">
  <li><a href="#sample-mean" id="toc-sample-mean" class="nav-link" data-scroll-target="#sample-mean"><span class="toc-section-number">8.1.1</span>  sample mean</a></li>
  <li><a href="#sample-size-and-standard-error" id="toc-sample-size-and-standard-error" class="nav-link" data-scroll-target="#sample-size-and-standard-error"><span class="toc-section-number">8.1.2</span>  sample size and standard error</a></li>
  </ul></li>
  <li><a href="#central-limit-theorem" id="toc-central-limit-theorem" class="nav-link" data-scroll-target="#central-limit-theorem"><span class="toc-section-number">8.2</span>  Central limit theorem</a>
  <ul class="collapse">
  <li><a href="#normal-distribution" id="toc-normal-distribution" class="nav-link" data-scroll-target="#normal-distribution"><span class="toc-section-number">8.2.1</span>  normal distribution</a></li>
  <li><a href="#confidence-intervals" id="toc-confidence-intervals" class="nav-link" data-scroll-target="#confidence-intervals"><span class="toc-section-number">8.2.2</span>  confidence intervals</a></li>
  <li><a href="#estimating-relative-risk" id="toc-estimating-relative-risk" class="nav-link" data-scroll-target="#estimating-relative-risk"><span class="toc-section-number">8.2.3</span>  estimating relative risk</a></li>
  </ul></li>
  <li><a href="#sampling-and-confidence-intervals-in-r" id="toc-sampling-and-confidence-intervals-in-r" class="nav-link" data-scroll-target="#sampling-and-confidence-intervals-in-r"><span class="toc-section-number">8.3</span>  Sampling and confidence intervals in R</a>
  <ul class="collapse">
  <li><a href="#computing-confidence-intervals" id="toc-computing-confidence-intervals" class="nav-link" data-scroll-target="#computing-confidence-intervals"><span class="toc-section-number">8.3.1</span>  computing confidence intervals</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title d-none d-lg-block"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Sampling distribution and estimation</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<blockquote class="blockquote">
<p>Pragmatism!? Is that all you have to offer?<br>
– Tom Stoppard, <em>Rosencrantz and Guildenstern Are Dead</em></p>
</blockquote>
<p>One of the most common tasks in any experimental science is to measure the value of a quantity by performing repeated experiments. A single experiment is not sufficient because there are always random factors making any single observation imperfect: natural variation of the random variable, experimental error, etc. For example, the number of new mutations (those not present in either parent) in an offspring is a random variable, so each individual may have a different number. A set of experimental measurements is called the <em>sample</em>. In this chapter you will learn to do the following:</p>
<ul>
<li><p>understand the sample mean as a random variable</p></li>
<li><p>calculate the standard error for a sample mean</p></li>
<li><p>calculate the confidence interval for the mean</p></li>
<li><p>understand the meaning of the parameters of the normal distribution</p></li>
<li><p>simulate sampling of a random variable using R</p></li>
</ul>
<section id="law-of-large-numbers" class="level2" data-number="8.1">
<h2 data-number="8.1" class="anchored" data-anchor-id="law-of-large-numbers"><span class="header-section-number">8.1</span> Law of large numbers</h2>
<p></p>
<section id="sample-mean" class="level3" data-number="8.1.1">
<h3 data-number="8.1.1" class="anchored" data-anchor-id="sample-mean"><span class="header-section-number">8.1.1</span> sample mean</h3>
<p>The goal in this chapter is to calculate the best estimate of the true mean value of a random variable. To do this, we need to repeat the experiment several times and take the mean of the data set: if one measurement is too high while another one is too low, their average is much closer to the true value. One important consideration here is that the errors cannot be systematically <em>biased</em>. If your experiment always overestimates or underestimates the true value, then averaging the measurements will result in a biased estimate. Second consideration is that the errors cannot depend on each other. This means that if one measurement is above average, it should have no effect on the probability of other measurements being above average.</p>
<p>In statistical parlance, a perfectly representative, unbiased sample is called a <em>simple random sample</em>. Speaking precisely, for a sample with <span class="math inline">\(n\)</span> observations (size <span class="math inline">\(n\)</span>) this requires that all subsets of the population of the same size have equal probability of being selected. In practice, this is not easy to verify. Sampling bias occurs if the way a sample is selected results in some participants being systematically overrepresented, and other underrepresented. With a biased sample, any conclusions from a study are suspect, and the mathematics is no longer applicable.</p>
<p>In statistics, it is customary to distinguish between the <em>population</em> and the <em>sample</em>. The term population doesn’t always refer to a collection of people or other living creatures; instead it may refer to all possible outcomes of an experiment - essentially, the entire sample space that we introduced in the previous chapter. Out of this ocean of possibilities, an experimenter fishes out a subset called the sample, and uses it to describe the whole population. It should be clear that the sample needs to be representative of the whole population, if this endeavor is to have any hope of success.</p>
<p>As mentioned in the last section, the most common estimation involves the mean. Here we need to distinguish two concepts: the <em>true mean</em> and the <em>sample mean</em>. The true mean refers to the quantity we are trying to measure (estimate) and it is considered the mean of a probability distribution of a random variable across the entire population. Thus the true mean is a <em>parameter</em> which we are trying to estimate.</p>
<p>The sample mean <span class="math inline">\(\bar X\)</span> is the average of a sample of experimental measurements, an it is a random variable with its own probability distribution. Suppose you’ve collected a sample of size <span class="math inline">\(n\)</span> and measured its mean <span class="math inline">\(\bar X_1\)</span>. If you collect another sample of the same size, its mean <span class="math inline">\(\bar X_2\)</span> will be (most likely) a different number. Repeating this process many times will result in different values of the sample mean, all dancing around the true mean. Provided that each time the data collection was unbiased, producing simple random samples, the variation is due to a combination of variance of the random variable in the population and the <em>sampling error</em>. The different sample means result from selecting samples which may randomly contain more numbers higher than the true mean, than those lower (or the opposite).</p>
<p>In this and the following sections we will describe the variation in the sample mean. It is a remarkable fact that we can describe this distribution, called the <em>sampling distribution of the mean</em>, in general, and connect it to the true distribution of the variable in the population. The first result describes the mean of the sampling distribution:</p>
<div class="cell">
<pre class="theorem cell-code"><code>(Law of Large Numbers, part 1) For a sample of $n$ independent\index{independence!sampling} measurements with the same distribution with mean $\mu$, the sample mean approaches $\mu$ as $n$ becomes large. More formally, if $X_1, X_2, ... , X_n$ are independent, identically distributed random variables with mean $\mu$, then:
$$ \lim_{n \rightarrow \infty}  \frac{X_1 + X_2 + ... + X_n} {n} =  \lim_{n \rightarrow \infty}  \bar X = \mu $$</code></pre>
</div>
<p>This result is called the Law of Large Numbers and it is intuitive: as the size of an unbiased random sample increases, its sample mean approaches the true mean. If we are dealing with a finite population, then of course if the sample includes the entire population, its mean will be the true mean.</p>
<p>One consequence of this is that the best estimator for the true mean is the sample mean; in fact it is what is called an <em>unbiased estimator</em>, provided the sampling process is unbiased. However, this is of only of limited use to someone trying to estimate a quantity, because the theorem doesn’t say how large the sample size needs to be in order for its mean to be reasonably close to the true mean. To address this practical question, let us consider the variance of the sampling distribution.</p>
</section>
<section id="sample-size-and-standard-error" class="level3" data-number="8.1.2">
<h3 data-number="8.1.2" class="anchored" data-anchor-id="sample-size-and-standard-error"><span class="header-section-number">8.1.2</span> sample size and standard error</h3>
<p>As mentioned above, repeated samples from the same populations have varying sample means, and we would like to describe the variance. This variation depends on the sample size, as suggested by the law of large numbers, for large sample sizes the sample mean approaches the true mean. The intuitive fact is that the larger the sample size, the less variation is in the sampling distribution.</p>
<div class="cell">
<pre class="theorem cell-code"><code>**(Law of Large Numbers, part 2)** For a sample of $n$ independent measurements with the same distribution with variance $\sigma^2$, the variance of the sampling distribution for a sufficiently large $n$ approaches the variance divided by the sample size:
$$ Var(\bar X) = \frac{\sigma^2}{n}$$</code></pre>
</div>
<p>In other words, for a sample of <span class="math inline">\(n\)</span> independent measurements the variance of the sample mean is inversely proportional to the sample size. Below I will sketch a calculation to justify (if not prove) the theorem. Suppose that the distribution of experimental measurements has the expected value of <span class="math inline">\(\mu\)</span> and the variance <span class="math inline">\(\sigma^2\)</span>. Then for a sample of <span class="math inline">\(n\)</span> independent measurements <span class="math inline">\(\{X_i\}\)</span>, the variance of the sample mean is the following:</p>
<p><span class="math display">\[ Var(\bar X) = Var\left(\frac{1}{n} \sum_{i=1}^{n} X_i\right)  =
\frac{1}{n^2} \sum_{i=1}^{n}Var(X_i) =  \frac{1}{n^2} n \sigma^2 = \frac{\sigma^2}{n}\]</span> The critical step in that calculation is the second equal sign, where the variance passes inside the sum. This is true because, as we saw in section <span class="math inline">\(\ref{sec:math4_2}\)</span>, the variance of a sum of independent random variables is equal to the sum of the variances.</p>
<p>It is often useful to deal with standard deviations to describe the spread of a distribution. Since the standard deviation is the square root of variance, we have the following definition:</p>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Definition
</div>
</div>
<div class="callout-body-container callout-body">
<p>The standard deviation of the mean of a sample of <span class="math inline">\(n\)</span> independent, identically distributed random variables with standard deviation <span class="math inline">\(\sigma\)</span> is called the <em>standard error</em> of the sample mean.</p>
</div>
</div>
<p>For large sample size <span class="math inline">\(n\)</span>, according to the law of large numbers it approaches <span class="math inline">\(s= \sigma/\sqrt{n}\)</span>. The standard error represents the spread in estimation of the true mean based on the data set of size <span class="math inline">\(n\)</span>. This means that increasing the sample size by a factor of 100 will lead to a reduction in the spread of the sample mean by a factor of 10. For example, if we obtain a sample of size 1000 instead of 10, the sample mean will have much less volatility.</p>
</section>
</section>
<section id="central-limit-theorem" class="level2" data-number="8.2">
<h2 data-number="8.2" class="anchored" data-anchor-id="central-limit-theorem"><span class="header-section-number">8.2</span> Central limit theorem</h2>
<section id="normal-distribution" class="level3" data-number="8.2.1">
<h3 data-number="8.2.1" class="anchored" data-anchor-id="normal-distribution"><span class="header-section-number">8.2.1</span> normal distribution</h3>
<p>In the last section we calculated the mean and the standard deviation of sampling distributions for a large sample size. There is an even more remarkable fact about sampling distributions: they all look the same! Regardless of the distribution of the random variable being sampled, the plot of the distribution of sample means approaches the famous bell-shaped curve called the <em>normal distribution</em>. This is one of the most fundamental and useful results in all of mathematics, and is called the Central Limit Theorem.</p>
<p>The theorem is much more subtle in both its statement and implications than the law of large numbers, so I need to spell out a few preliminaries. First, the sample mean for a large sample size can take on a whole range of values, so we will think of it as a continuous random variable. Say you’re sampling from the uniform distribution of integers between 1 and 10. If your sample size is 2, the mean may be either an integer (e.g.&nbsp;5 if your sample is 6 and 4) or a fraction with denominator 2 (e.g.&nbsp;11/2 if your sample is 1 and 10). As the sample size grows larger, the denominator of the sample mean increases, and the possible means are smeared out more densely between the values of the discrete distribution, e.g.&nbsp;for sample size 100, you may observe a sample mean of 3.62. As the sample sizes become larger, it is convenient to stipulate that the range of values of the sample mean is continuous, that is contains all real numbers between the maximum and the minimum values of the original distribution.</p>
<p>This brings us to the magnificent, famous, and indispensable normal, or Gaussian distribution. More correctly, it is the <em>probability density function</em> of the normal random variable, which has the range from negative infinity to infinity, that is, any real number. Below is the mathematical form of its density function <span class="math inline">\(\rho(x)\)</span>, with parameters <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span> representing the mean and the standard deviation, respectively .</p>
<p><span class="math display">\[\rho(x) = \sqrt{\frac{1}{2\pi \sigma^2}} e^{-\frac{(x-\mu)^2}{2 \sigma^2}}\]</span></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="ch5/normal_dist_example.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Plots of normal probability densities with different parameter values. The mean <span class="math inline">\(\mu\)</span> determines the center of the distribution and the standard deviation <span class="math inline">\(\sigma\)</span> controls the width.</figcaption><p></p>
</figure>
</div>
<p>The function of the random variable <span class="math inline">\(x\)</span> is plotted in figure <span class="math inline">\(\ref{fig:ch5_normal_dist}\)</span>, for different values of the mean and the standard deviation. The shape of the distribution is the famous bell-shaped curve, and the mean indicates the position of the peak on the <span class="math inline">\(x\)</span>-axis. The standard deviation parameter is responsible for the width of the distribution: the larger <span class="math inline">\(\sigma\)</span>, the broader and more spread out the distribution. Those are the only parameters in the normal distribution, everything else about it remains the same.</p>
<p>This probability density function differs from the discrete distribution functions we saw in the last chapter in one key way: although you can plug in a particular value of the normal random variable, the number it returns is not the probability of that particular value. Properly speaking, the probability of any particular value of the normal random variable, like 3.2, is indistinguishable from zero, because there are infinitely many such values and if they all had nonzero probabilities, the total probability of the distribution would be infinite. In fact, the total probability of the normal random variable must be 1, as dictated by the axioms of probability seen in section <span class="math inline">\(\ref{sec:math4_1}\)</span>.</p>
<p>Instead, in order to extract probability from a density function, it must be integrated. The integral is the equivalent of a sum when you need to add up numbers from a continuous range. So instead of calculating the probability of one value, for continuous random variables we can calculate the probability of a range of values by taking the integral of the density function <span class="math inline">\(\rho(x)\)</span> on that range. For example, the probability of the normal random variable with <span class="math inline">\(\mu=1\)</span> and standard deviation <span class="math inline">\(\sigma=0.8\)</span> being in the range between 0 and 1 is: <span class="math display">\[ P(0 &lt; x &lt; 1) = \sqrt{\frac{1}{2\pi 0.8^2}} \int_{0} ^1 e^{-\frac{(x-1)^2}{2 \times 0.8^2}}dx\]</span> These integrals may look intimidating, and rightly so: there is no method for solving them the way you may have done in a calculus course. But although we cannot write down an algebraic formula, we can still find the answer , that is as a number. In the old days, one consulted a table in the back of the probability or statistics textbook that contained the values of these integrals. We can do this much more efficiently using R, as will be demonstrated in the computational section below.</p>
<p>Now that we have a nodding acquaintance with the normal distribution, here is its most wonderful property: the means of samples, for large sample sizes, are distributed normally.</p>
<div class="cell">
<pre class="theorem cell-code"><code>**(Central Limit Theorem)** The distribution of the mean of a sample of $n$ independent\index{independence!sampling}, identically distributed random variables with mean $\mu$ and standard deviation $\sigma$ approaches for large $n$ the \emph{normal distribution} with mean $\mu$ and standard deviation $\sigma/\sqrt{n}$. If the above conditions are met, the probability that a sample mean will fall between values $a$ and $b$ is given by the integral of the probability density function:

$$ P(a &lt; \bar X &lt; b) = \sqrt{\frac{n}{2\pi \sigma^2}} \int_{a} ^b e^{-\frac{(x-\mu)^2}{2 \sigma^2/n}}dx$$</code></pre>
</div>
<p>This theorem gifts us the ability to predict how widely the means of a sample can vary around the true mean. If you know the true mean <span class="math inline">\(\mu\)</span>, the true standard deviation <span class="math inline">\(\sigma\)</span>, and the sample size <span class="math inline">\(n\)</span> is large enough (ignoring for a second what that means exactly) then the probability of the sample mean being off by 0.1 from the true mean in either direction is: <span class="math display">\[ P(\mu-0.1&lt; \bar X &lt; \mu+0.1) = \sqrt{\frac{n}{2\pi \sigma^2}} \int_{\mu-0.1} ^{\mu+0.1} e^{-\frac{(x-\mu)^2}{2 \sigma^2/n}}dx\]</span></p>
<p>To evaluate this integral (numerically) we need to know the values of the three parameters: <span class="math inline">\(\mu\)</span>, <span class="math inline">\(\sigma\)</span>, and <span class="math inline">\(n\)</span>. In practice, only the sample size <span class="math inline">\(n\)</span> is known, while <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span> are parameters of the true distribution, and can be only estimated. The next section is devoted to sorting out the statistical details.</p>
</section>
<section id="confidence-intervals" class="level3" data-number="8.2.2">
<h3 data-number="8.2.2" class="anchored" data-anchor-id="confidence-intervals"><span class="header-section-number">8.2.2</span> confidence intervals</h3>
<p>The central limit theorem is a purely mathematical result, but it is used in a wide range of practical applications, from public opinion polling to medical risk assessments. In this section we journey from the abstract land of probability to the data-driven domain of statistics on a quest for correct estimation of means. When reporting any experimental measurement, it is mandatory to include <em>error bars</em> around the mean of a data set to indicate a range of plausible values of the estimated quantity, usually in the form of <span class="math inline">\(\bar X \pm \epsilon\)</span>. The meaning of these error bars varies: sometimes the standard deviation of the measurements (<span class="math inline">\(\sigma_X\)</span>) is used, other times the standard error, but the correct way to report uncertainty in estimation is to calculate the confidence interval .</p>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Definition
</div>
</div>
<div class="callout-body-container callout-body">
<p>A <em>confidence interval</em> is a range of values calculated from a data set to estimate the true value of a quantity (e.g.&nbsp;mean). The associated <em>confidence level</em> <span class="math inline">\(\alpha\)</span> (between 0 and 1) is the likelihood that the confidence interval contains the true value.</p>
</div>
</div>
<p>The meaning of confidence intervals is pretty subtle. This is in large part because of the word likelihood in the definition, which in everyday language is interchangeable with probability, but as mathematical terms they are not. Here is the wrong way to think about them: a confidence interval for a mean at <span class="math inline">\(\alpha\)</span> level does not mean that the true mean has probability <span class="math inline">\(\alpha\)</span> of being in that interval. The true mean is not a random variable, it is a parameter of the probability distribution that we assume exists. Instead, it means that if sampling were repeated many times, out of the resulting confidence intervals fraction <span class="math inline">\(\alpha\)</span> would contain the true mean.</p>
<p>You can see from this definition that there is more than one confidence interval one can report from a single data set, because its size depends on the confidence level. At first glance, you might be tempted to make <span class="math inline">\(\alpha\)</span> as large as possible - after all, you’d like to have maximum confidence in your estimate. Unfortunately, it is impossible to provide a confidence interval in which the true mean is guaranteed to reside, short of making the interval infinitely wide. However, one generally wants the estimate to be precise, which means making the confidence internal as narrow as possible. This brings us to the cruel fact of the estimation business: the goal of making a useful (precise) estimate is in opposition to the goal of making a confidence interval with a large confidence level. One can always make a very precise estimate, but it will have a lower confidence level, or one may increase the confidence by making the interval larger.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="ch5/standard-deviation-tikz.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Illustration of the normal probability density in terms of standard deviations from the mean.</figcaption><p></p>
</figure>
</div>
<p>So, given a data set, how do we construct a confidence interval? First, we need to choose the desired confidence level, for example <span class="math inline">\(\alpha=0.95\)</span>, which is commonly used. The data set has a sample mean <span class="math inline">\(\bar X\)</span>, sample standard deviation <span class="math inline">\(\sigma_X\)</span>, and sample size <span class="math inline">\(n\)</span>, which are at our disposal. The central limit theorem tells us that the distribution of sample means is normal (provided <span class="math inline">\(n\)</span> is large enough) with mean <span class="math inline">\(\mu\)</span> and standard deviation <span class="math inline">\(s=\sigma/\sqrt n\)</span>. One can calculate the deviation around the <span class="math inline">\(\mu\)</span> that define a confidence interval using fancy integrals or computers. Figure <span class="math inline">\(\ref{fig:ch5_sigma_levels}\)</span> shows the probability of falling within a a certain deviation from the mean for integer multiples of standard deviation <span class="math inline">\(\sigma\)</span>; for example, the probability of falling within one standard deviation of the mean for a normal random variable is approximately 68%. Luckily, these probability levels are the same for all normal distributions, so they can be used to calculate the confidence interval for any mean. For instance, the range of the normal random variable that contains 95% probability is approximately <span class="math inline">\((\mu - 1.96 s, \mu + 1.96 s)\)</span>, that is between 1.96 times the standard deviation to the left of the mean to 1.96 the standard deviation to the right of the mean. So to construct the confidence interval one needs the mean <span class="math inline">\(\mu\)</span>, the standard error <span class="math inline">\(s\)</span>, and the 95% confidence level.</p>
<p>If you were not too bedazzled by different symbols, you might have noticed that there was a bait-and-switch in the argument above. The data set provides the sample mean <span class="math inline">\(\bar X\)</span>, but the confidence interval requires the true mean <span class="math inline">\(\mu\)</span>. The law of large numbers says that the former approaches the latter for large <span class="math inline">\(n\)</span>, but they are not the same. However, there is no choice but to use the imperfect sample mean <span class="math inline">\(\bar X\)</span> as the central point for the confidence interval - because <span class="math inline">\(\mu\)</span> is what we are trying to estimate! Further, the standard error <span class="math inline">\(s\)</span> actually depends on the true standard deviation <span class="math inline">\(\sigma\)</span>, but we have to make do with the sample standard deviation <span class="math inline">\(\sigma_X\)</span>, for the exact same reason: the true value is not available. (The topic of proper estimation of standard deviation is another kettle of fish, which I leave aside.) These approximations are not always sufficiently appreciated, but they can lead to notable discrepancies between the theoretical confidence level and the actual confidence level, which will be illustrated in the computational assignments in section <span class="math inline">\(\ref{sec:proj5}\)</span>.</p>
<p>To summarize, here is the recipe for calculating a confidence interval for the true mean at a given confidence <span class="math inline">\(\alpha\)</span> based on a data set <span class="math inline">\(X\)</span> of size <span class="math inline">\(n\)</span>:</p>
<ol type="1">
<li><p>Compute the sample mean <span class="math inline">\(\bar X\)</span>, which is the best estimate we have of the true mean <span class="math inline">\(\mu\)</span>.</p></li>
<li><p>Compute the sample standard deviation <span class="math inline">\(\sigma_{X}\)</span>, which is the best estimate we have of the actual standard deviation <span class="math inline">\(\sigma\)</span></p></li>
</ol>
<p>3.Compute the standard error <span class="math inline">\(s = \sigma_X/\sqrt{n}\)</span></p>
<ol start="4" type="1">
<li><p>Use a table or R to calculate the multiple called <span class="math inline">\(z_\alpha\)</span> for the desired confidence level <span class="math inline">\(\alpha\)</span>. For example, <span class="math inline">\(z_{0.9} \approx 1.65\)</span>, <span class="math inline">\(z_{0.95} \approx 1.96\)</span>, <span class="math inline">\(z_{0.99} \approx 2.58\)</span>.</p></li>
<li><p>Build the confidence interval by adding and subtracting <span class="math inline">\(z_\alpha s\)</span> from the sample mean <span class="math inline">\(\bar X\)</span>. For example, the 95% confidence interval is <span class="math inline">\(\bar X \pm z_{0.95} s= \bar X \pm 1.96\sigma_{X}/\sqrt{n}\)</span>. \end{enumerate}</p></li>
</ol>
<p>There are a couple of important points about error bars that make them different in practice from their theoretical setup. The definition states that if you take a bunch of sample and compute their 95% confidence intervals, about 95% of them will contain the true mean. If you actually perform this experiment using R, you will find out that this is not the case, especially for small sample sizes. The reason for this is that the theoretical definition assumes that we use the true mean and true standard deviation to calculate the confidence interval. But the best we can do is to use the sample mean and sample stander deviation, so of course the confidence interval will be off. This is an important caution about being overly confident about confidence intervals. Another caution comes from the fact that they are based on the assumption of perfect independence between different measurements. Bear this is mind when you see a 95% confidence interval reported in a paper, especially one with a small sample size.</p>
</section>
<section id="estimating-relative-risk" class="level3" data-number="8.2.3">
<h3 data-number="8.2.3" class="anchored" data-anchor-id="estimating-relative-risk"><span class="header-section-number">8.2.3</span> estimating relative risk</h3>
<p>Clinical trials to evaluate medical treatments or drugs usually proceed by dividing a group of people into two subgroups: one which receives the treatment and one which does not (control group). If this is done in an unbiased random manner, it is called a randomized controlled trial (RCT), which is typically considered the best study design (for most purposes). The two groups are then compared for the outcomes, such as mortality or morbidity (illness). The comparison is usually done in the form of <em>relative risk</em>, or the ratio between the fractions of those with an undesirable outcome in one group and the other. In an idealized case, if the relative risk is 1, there is no difference between the groups, and thus the treatment has no effect. If relative risk is not 1, then it makes a difference (either good or bad).</p>
<p>The astute reader has likely noted that this idealization has no practical value. Relative risk is almost never exactly 1 even if the treatment does nothing, due to chance alone. The actual relative risk from a study may be 1.12 or 0.96. Is this sufficiently different from 1 to say that the treatment has an effect? If only we had a way of computing a range of values to estimate the true value of a quantity… But wait, we do! It’s called a confidence interval.</p>
<p>The statistics necessary to compute confidence intervals for relative risk are different than what we have seen, because relative risk is not distributed normally. It turns out that its distribution is log-normal (under some assumptions). We will not delve into the details here, but the basic ideas are the same: choose the confidence level, calculate the confidence interval based on the distribution (in this case log-normal) and the statistics of the data. The main question for a clinical trial is, does this confidence interval include 1 or not? If not, then the treatment has an effect (whether positive or negative) at that confidence level.</p>
<p>The following discussion questions are based on the paper <a href="https://jamanetwork.com/journals/jama/fullarticle/2275444">“Autism occurrence by MMR vaccine status among children with older siblings with and without autism”</a>. Read the paper and discuss with your peers or colleagues using the following questions as a starting point:</p>
<ol type="1">
<li><p>How are confidence intervals used in the study to conclude that there is no effect of MMR vaccination on development of autism spectrum disorders (ASD)?</p></li>
<li><p>What are some limitations of the study? How does its design differ from a classic RCT?</p></li>
<li><p>Does dividing children by whether or not they have a sibling with ASD make the study stronger?</p></li>
</ol>
</section>
</section>
<section id="sampling-and-confidence-intervals-in-r" class="level2" data-number="8.3">
<h2 data-number="8.3" class="anchored" data-anchor-id="sampling-and-confidence-intervals-in-r"><span class="header-section-number">8.3</span> Sampling and confidence intervals in R</h2>
<p>As we saw in section <span class="math inline">\(\ref{sec:comp4}\)</span>, R can be used to generate random numbers from a particular distribution, for example the uniform distribution of real numbers between 0 and 1. This distribution differs from the discrete uniform distribution discussed in section <span class="math inline">\(\ref{sec:math4_2}\)</span>, because the values of the random variable can be any real number between 0 and 1, so it is a continuous variable. The true mean of this distribution is 0.5, but what is the mean of a sample of numbers drawn from a uniform distribution? We can perform this numerical experiment using the R function <code>runif()</code>, which generates a specified number of random numbers from the uniform distribution. If you repeat the same command, you will get a different set of values, since they are generated randomly every time. The following script produces two random samples of size 10 and prints out the means of the samples:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>sample <span class="ot">&lt;-</span> <span class="fu">runif</span>(<span class="dv">10</span>)</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(sample)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code> [1] 0.1212673 0.2620580 0.8249934 0.5459261 0.2270833 0.4073850 0.7045749
 [8] 0.9098576 0.6863541 0.7801683</code></pre>
</div>
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(sample)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.5469668</code></pre>
</div>
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>sample <span class="ot">&lt;-</span> <span class="fu">runif</span>(<span class="dv">10</span>)</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(sample)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code> [1] 0.959771324 0.744979969 0.725296286 0.541356429 0.147889531 0.801968701
 [7] 0.918518508 0.005788951 0.971596977 0.301761608</code></pre>
</div>
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(sample)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.6118928</code></pre>
</div>
</div>
<p>If you copy this script and run it yourself, you will obtain different numbers and if you run it several times you will notice that the mean of sample of 10 values is prone to considerable volatility. This leads to two related questions: 1) how large does the sample need to be in order to obtain a good estimate of the true mean? 2) given a random sample and its sample mean, what is the reasonable range of values for the estimate of the true mean? We addressed these questions theoretically in section <span class="math inline">\(\ref{sec:math5_1}\)</span>, and here we investigate them by generating multiple random samples using R.</p>
<p>The script used to produce figure <span class="math inline">\(\ref{fig:data_samplemeans}\)</span> generates 100 random samples of size 10 (using the uniform random number generator), saves the means of the ten samples into a vector variable, and plots its histogram; then it does the same thing for 100 random samples of size 100. The histograms of sample means in figure <span class="math inline">\(\ref{fig:data_samplemeans}\)</span> demonstrate the effect of sample size that we previously discussed theoretically. As a smaller sample size, the sample means vary more widely, in other words, the distribution of sample means has a larger variance for smaller sample sizes. Although the sampling process is random, and every time the script is run it produces a new set of sample means, in the figure you can see clearly that sample means for samples of size 10 are much more spread out than those for sample size 100.</p>
<div class="cell" data-layout-align="center" data-fig.asp="0.75">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>numsamples <span class="ot">&lt;-</span> <span class="dv">100</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>samplemeans<span class="ot">&lt;-</span><span class="fu">rep</span>(<span class="cn">NA</span>, numsamples)</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>samplesize<span class="ot">&lt;-</span><span class="dv">10</span></span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>numsamples) {</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>    sample <span class="ot">&lt;-</span> <span class="fu">runif</span>(samplesize)</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>    samplemeans[i] <span class="ot">&lt;-</span> <span class="fu">mean</span>(sample)</span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>title_text <span class="ot">&lt;-</span> <span class="fu">paste</span>(numsamples, <span class="st">"means of samples of size"</span>, samplesize)</span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a><span class="fu">hist</span>(samplemeans, <span class="at">main=</span>title_text,<span class="at">cex.axis=</span><span class="fl">1.5</span>,<span class="at">cex.lab=</span><span class="fl">1.5</span>)</span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a>samplemeans<span class="ot">&lt;-</span><span class="fu">rep</span>(<span class="cn">NA</span>, numsamples)</span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a>samplesize<span class="ot">&lt;-</span><span class="dv">100</span></span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>numsamples) {</span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a>    sample <span class="ot">&lt;-</span> <span class="fu">runif</span>(samplesize)</span>
<span id="cb12-15"><a href="#cb12-15" aria-hidden="true" tabindex="-1"></a>    samplemeans[i] <span class="ot">&lt;-</span> <span class="fu">mean</span>(sample)</span>
<span id="cb12-16"><a href="#cb12-16" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb12-17"><a href="#cb12-17" aria-hidden="true" tabindex="-1"></a>title_text <span class="ot">&lt;-</span> <span class="fu">paste</span>(numsamples, <span class="st">"means of samples of size"</span>, samplesize)</span>
<span id="cb12-18"><a href="#cb12-18" aria-hidden="true" tabindex="-1"></a><span class="fu">hist</span>(samplemeans, <span class="at">main=</span>title_text,<span class="at">cex.axis=</span><span class="fl">1.5</span>,<span class="at">cex.lab=</span><span class="fl">1.5</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="sampling_files/figure-html/sample-dist-1.png" class="img-fluid figure-img" style="width:50.0%"></p>
<p></p><figcaption class="figure-caption">Distribution of means of samples drawn from the uniform distribution for different sample sizes: 10 (left) and 100 (right).</figcaption><p></p>
</figure>
</div>
</div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="sampling_files/figure-html/sample-dist-2.png" class="img-fluid figure-img" style="width:50.0%"></p>
<p></p><figcaption class="figure-caption">Distribution of means of samples drawn from the uniform distribution for different sample sizes: 10 (left) and 100 (right).</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>The central limit theorem predicts that for large sample size <span class="math inline">\(n\)</span> the distribution of sample means is close to the normal distribution. To demonstrate this, the scripts below generate 1000 samples of size 200 and 500 from the uniform random variable and plot the histograms of sample means shown in figure @(fig:sample-dist). In addition to the histograms, we also overlay plots of the normal distribution with mean <span class="math inline">\(\mu=0.5\)</span> and standard deviation <span class="math inline">\(s = \sigma/\sqrt{n}\)</span>, where <span class="math inline">\(\sigma\)</span> is the standard deviation of the continuous uniform distribution between 0 and 1, which happens to be <span class="math inline">\(1/\sqrt{12}\)</span>. Notice that for sufficiently many samples (1000), the histograms are clearly bell-shaped, as predicted, despite the random samples being generated from the uniform distribution. The normal distribution curve matches the distribution of samples of size 1000 somewhat better than those of size 100, but the difference is not dramatic. The most important difference is in the spread of the sampling distribution, which is noticeably smaller for the larger sample size. This illustrates how much more accurate an estimate of the mean from sample of size 1000 is compared to sample of size 100.</p>
<div class="cell" data-layout-align="center" data-fig.asp="0.75">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="sampling_files/figure-html/clt-demo-1.png" class="img-fluid figure-img" style="width:50.0%"></p>
<p></p><figcaption class="figure-caption">Central limit theorem in action: comparison of histograms of sample means and the normal distribution: on the left is a histogram of means of samples of size 100, on the right means of samples of size 1000</figcaption><p></p>
</figure>
</div>
</div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="sampling_files/figure-html/clt-demo-2.png" class="img-fluid figure-img" style="width:50.0%"></p>
<p></p><figcaption class="figure-caption">Central limit theorem in action: comparison of histograms of sample means and the normal distribution: on the left is a histogram of means of samples of size 100, on the right means of samples of size 1000</figcaption><p></p>
</figure>
</div>
</div>
</div>
<section id="computing-confidence-intervals" class="level3" data-number="8.3.1">
<h3 data-number="8.3.1" class="anchored" data-anchor-id="computing-confidence-intervals"><span class="header-section-number">8.3.1</span> computing confidence intervals</h3>
<p>Now let us compute a confidence interval for the mean based on a sample, following the recipe at the end of section <span class="math inline">\(\ref{sec:math5_2}\)</span>. To do this we will use the function <code>qnorm()</code>, which is the inverse normal density function. This means that for a given probability level <span class="math inline">\(p\)</span>, the function returns the value <span class="math inline">\(x\)</span> of the standard normal random variable with mean <span class="math inline">\(\mu=0\)</span> and standard deviation <span class="math inline">\(\sigma=1\)</span>, such that the total probability of the distribution being less than <span class="math inline">\(x\)</span> is equal to <span class="math inline">\(p\)</span>. For instance, since the standard normal is a symmetric distribution with mean 0, the probability of the random variable being below 0 is 0.5, as shown by the command in the script below. The second command in the script calculates the value of the standard normal so that 95% of the probability is to its left.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="fu">qnorm</span>(<span class="fl">0.5</span>) <span class="co"># the value that divides the density function in two</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0</code></pre>
</div>
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="fu">qnorm</span>(<span class="fl">0.95</span>) <span class="co"># the value such that 95% of density is to its left </span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 1.644854</code></pre>
</div>
</div>
<p>The function <code>qnorm()</code> is necessary for computing the <span class="math inline">\(z_\alpha\)</span> values for a given confidence value <span class="math inline">\(\alpha\)</span>. The <span class="math inline">\(z_\alpha\)</span> is the value of the standard normal such that the random variable <span class="math inline">\(z\)</span> had probability <span class="math inline">\(\alpha\)</span> of being within <span class="math inline">\(z_\alpha\)</span> of the mean <span class="math inline">\(\mu=0\)</span>; in other words, it requires that the probability of being outside of the interval <span class="math inline">\((-z_\alpha, z_\alpha)\)</span> is <span class="math inline">\(1-\alpha\)</span>. The function <code>qnorm(p)</code> returns the value <span class="math inline">\(x\)</span> so that the standard normal has probability <span class="math inline">\(p\)</span> of being less than <span class="math inline">\(x\)</span>; in other words, that probability being greater than <span class="math inline">\(x\)</span> is <span class="math inline">\(1-p\)</span>. There are two ways of being outside the interval <span class="math inline">\((-z_\alpha, z_\alpha)\)</span>: less than <span class="math inline">\(-z_\alpha\)</span> (called the left tail of the distribution) and greater than <span class="math inline">\(z_\alpha\)</span> (called the right tail of the distribution). The values <span class="math inline">\(z_\alpha\)</span>, by definition, represent tails of <span class="math inline">\((1-\alpha)/2\)</span>, because there are two tails, and the normal distribution is symmetric.</p>
<p>The script below generates a sample from the uniform distribution, computes its mean and standard deviation, and the standard error. Then it calculates the value <span class="math inline">\(z_\alpha\)</span> based on the defined <span class="math inline">\(\alpha\)</span> value, and calculates the variables right and left, the respective boundaries of the confidence interval for the true mean. The script shows confidence intervals calculated from a sample of size 10 and a sample of size 100.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>size <span class="ot">&lt;-</span> <span class="dv">10</span> <span class="co"># sample size</span></span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>alpha <span class="ot">&lt;-</span> <span class="fl">0.95</span> <span class="co"># significance level</span></span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>sample<span class="ot">&lt;-</span><span class="fu">runif</span>(size)</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>s <span class="ot">&lt;-</span> <span class="fu">sd</span>(sample)<span class="sc">/</span><span class="fu">sqrt</span>(size) <span class="co"># standard error</span></span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>z <span class="ot">&lt;-</span> <span class="fu">qnorm</span>((<span class="dv">1</span><span class="sc">-</span>alpha)<span class="sc">/</span><span class="dv">2</span>) <span class="co"># z-value</span></span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a>left <span class="ot">&lt;-</span> <span class="fu">mean</span>(sample)<span class="sc">+</span>s<span class="sc">*</span>z</span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a>right <span class="ot">&lt;-</span> <span class="fu">mean</span>(sample)<span class="sc">-</span>s<span class="sc">*</span>z</span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(right)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.6885998</code></pre>
</div>
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(left)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.3079434</code></pre>
</div>
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>size <span class="ot">&lt;-</span> <span class="dv">100</span> <span class="co"># sample size</span></span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>sample<span class="ot">&lt;-</span><span class="fu">runif</span>(size)</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>s <span class="ot">&lt;-</span> <span class="fu">sd</span>(sample)<span class="sc">/</span><span class="fu">sqrt</span>(size) <span class="co"># standard error</span></span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a>z <span class="ot">&lt;-</span> <span class="fu">qnorm</span>((<span class="dv">1</span><span class="sc">-</span>alpha)<span class="sc">/</span><span class="dv">2</span>) <span class="co"># z-value</span></span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a>right <span class="ot">&lt;-</span> <span class="fu">mean</span>(sample)<span class="sc">+</span>s<span class="sc">*</span>z</span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a>left <span class="ot">&lt;-</span> <span class="fu">mean</span>(sample)<span class="sc">-</span>s<span class="sc">*</span>z</span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(right)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.4187058</code></pre>
</div>
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(left)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.53606</code></pre>
</div>
</div>
<p>Every time you run this script, it will produce new samples and therefore different confidence intervals, but most of the time (theoretically, 95% of the time, although that is not practically true) the confidence interval will contain the true mean of 0.5. As you would expect, the confidence interval for sample size 10 is much wider than the one for sample size 100. To obtain a reliable estimate of the true mean, you must obtain a sufficient number of measurements.</p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./bayes.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Prior knowledge and Bayesian thinking</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./lindiff.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Linear difference equations</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->



</body></html>