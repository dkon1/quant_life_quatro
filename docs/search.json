[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Quantifying Life",
    "section": "",
    "text": "Preface\nThis is an online book to help biologists and biology-adjacent folks learn quantitative skills through the practice of programming in R. These skills can be roughly sorted into four types:\nThese skills interface, intertwine, and reinforce each other in the practice of biological research and are thus presented concurrently in this book, instead of being corralled into separate courses taught by different departments, like mathematics, statistics, and computer science. Here I combine ideas and skills from all of these disciplines into an educational narrative organized by increasing exposure to programming concepts."
  },
  {
    "objectID": "index.html#a-brief-motivation-of-mathematical-modeling",
    "href": "index.html#a-brief-motivation-of-mathematical-modeling",
    "title": "Quantifying Life",
    "section": "A brief motivation of mathematical modeling",
    "text": "A brief motivation of mathematical modeling\nA mathematical model is a representation of some real object or phenomenon in terms of quantities (numbers). The goal of modeling is to create a description of the object in question that may be used to pose and answer questions about it, without doing hard experimental work. A good analogy for a mathematical model is a map of a geographic area: a map cannot record all of the complexity of the actual piece of land, because then the map would need to be size of the piece of land, and then it wouldn’t be very useful! Maps, and mathematical models, need to sacrifice the details and provide a birds-eye view of reality in order to guide the traveler or the scientist. The representation of reality in the model must be simple enough to be useful, yet complex enough to capture the essential features of what it is trying to represent.\nMathematical modeling has long been essential in physics: for instance, it is well known that distance traveled by an object traveling at constant speed \\(v\\) is proportional to the time traveled (called \\(t\\)). This mathematical model can be expressed as an equation:\n\\[d = vt\\]\nSince the time of Newton, physicists have been very successful at using mathematics to describe the behavior of matter of all sizes, ranging from subatomic particles to galaxies. However, mathematical modeling is a new arrow in a biologist’s quiver. Many biologists would argue that living systems are much more complex than either atoms or galaxies, since even a single cell is made up of a mind-boggling number of highly dynamic, interacting entities. That is true, but new advances in experimental biology are producing data that make quantitative methods indispensable for biology.\nThe advent of genetic sequencing in the 1970s and 80s has allowed us to determine the genomes of different species, and in the last few years next-generation sequencing has reduced sequencing costs for an individual human genome to a few thousand dollars. The resulting deluge of quantitative data has answered many outstanding questions, and also led to entirely new ones. We now understand that knowledge of genomic sequences is not enough for understanding how living things work, so the burgeoning field of systems biology investigates the interactions between genes, proteins, or other entities. The central question is to understand how a network of interactions between individual molecules can lead to large-scale results, such as the development of a fertilized egg into a complex organism. The human mind is not suited for making correct intuitive judgements about networks comprised of thousands of actors. Addressing questions of this complexity requires quantitative modeling."
  },
  {
    "objectID": "index.html#purpose-of-this-book",
    "href": "index.html#purpose-of-this-book",
    "title": "Quantifying Life",
    "section": "Purpose of this book",
    "text": "Purpose of this book\nThis textbook is intended for a college-level course for biology and pre-medicine majors, or more established scientists interested in learning the applications of mathematical methods to biology. The book brings together concepts found in mathematics, computer science, and statistics courses to provide the student a collection of skills that are commonly used in biological research. The book has two overarching goals: one is to explain the quantitative language that often is a formidable barrier to understanding and critically evaluating research results in biological and medical sciences. The second is to teach students computational skills that they can use in their future research endeavors. The main premise of this approach is that computation is critical for understanding abstract mathematical ideas.\nThese goals are distinct from those of traditional mathematics courses that emphasize rigor and abstraction. I strongly believe that understanding of mathematical concepts is not contingent on being able to prove all of the underlying theorems. Instead, premature focus on abstraction obscures the ideas for most students; it is putting the theoretical cart before the experiential horse. I find that students can grasp deep concepts when they are allowed to experience them tangibly as numbers or pictures, and those with an abstract mindset can generalize and add rigor later. As I demonstrate in part 3 of the book, Markov chains can be explained without relying on the machinery of measure theory and stochastic processes, which require graduate level mathematical skills. The idea of a system randomly hopping between a few discrete states is far more accessible than sigma algebras and martingales. Of course, some abstraction is necessary when presenting mathematical ideas, and I provide correct definitions of terms and supply derivations when I find them to be illuminating. But I avoid rigorous proofs, and always favor understanding over mathematical precision.\nThe book is structured to facilitate learning computational skills. Over the course of the text students accumulate programming experience, progressing from assigning values to variables in the first chapter to solving nonlinear ODEs numerically by the end of the book. Learning to program for the first time is a challenging task, and I facilitate it by providing sample scripts for students to copy and modify to perform the requisite calculations. Programming requires careful, methodical thinking, which facilitates deeper understanding of the models being simulated. In my experience of teaching this course, students consistently report that learning basic scientific programming is a rewarding experience, which opens doors for them in future research and learning.\nIt is of course impossible to span the breadth of mathematics and computation used for modeling biological scenarios. This did not stop me from trying. The book is broad but selective, sticking to a few key concepts and examples which should provide enough of a basis for a student to go and explore a topic in more depth. For instance, I do not go through the usual menagerie of probability distributions in chapter 4, but only analyze the uniform and the binomial distributions. If one understands the concepts of distributions and their means and variances, it is not difficult to read up on the geometric or gamma distribution if one encounters it. Still, I omitted numerous topics and entire fields, some because they require greater mathematical sophistication, and others because they are too difficult for beginning programmers, e.g. sequence alignment and optimization algorithms. I hope that you do not end your quantitative journey with this book!\nI take an even more selective approach to the biological topics that I present in every chapter. The book is not intended to teach biology, but I do introduce biological questions I find interesting, refer the reader to current research papers, and provide discussion questions for you to wrestle with. This requires a basic explanation of terms and ideas, so most chapters contain a broad brushstrokes summary of a biological field, e.g. measuring mutation rates, epidemiology modeling, hidden Markov models for gene structure, and limitations of medical testing. I hope the experts in these fields forgive my omitting the interesting details that they spend their lives investigating, and trust that I managed to get the basic ideas across without gross distortion."
  },
  {
    "objectID": "index.html#organization-of-the-book",
    "href": "index.html#organization-of-the-book",
    "title": "Quantifying Life",
    "section": "Organization of the book",
    "text": "Organization of the book\nA course based on this textbook can be tailored to fit the quantitative needs of a biological sciences curriculum. At the University of Chicago the course I teach has replaced the last quarter of calculus as a first-year requirement for biology majors. This material could be used for a course without a calculus pre-requisite that a student takes before more rigorous statistics, mathematics, or computer science courses. It may also be taught as an upper-level elective course for students with greater maturity who may be ready to tackle the eigenvalues and differential equations chapters. My hope is that it may also prove useful for graduate students or established scientists who need an elementary but comprehensive introduction to the concepts they encounter in the literature or that they can use in their own research. Whatever path you traveled to get here, I wish you a fruitful journey through biomathematics and computation!"
  },
  {
    "objectID": "counting.html#sec:bio1",
    "href": "counting.html#sec:bio1",
    "title": "1  Arithmetic and variables",
    "section": "1.1 Blood circulation and mathematical modeling",
    "text": "1.1 Blood circulation and mathematical modeling\nGalen was one of the great physicians of antiquity. He studied how the body works by performing experiments on humans and animals. Among other things, he was famous for a careful study of the heart and how blood traveled through the body. Galen observed that there were different types of blood: arterial blood that flowed out of the heart, which was bright red, and venous blood that flowed in the opposite direction, which was a darker color. This naturally led to questions: what is the difference between venous and arterial blood? where does each one come from and where does it go?\nYou, a reader of the 21st century, likely already know the answer: blood circulates through the body, bringing oxygen and nutrients to the tissues through the arteries, and returns back through the veins carrying carbon dioxide and waste products, as shown in figure \\(\\ref{fig:circulation}\\). Arterial blood contains a lot of oxygen while venous blood carries more carbon dioxide, but otherwise they are the same fluid. The heart does the physical work of pushing arterial blood out of the heart, to the tissues and organs, as well as pushing venous blood through the second circulatory loop that goes through the lungs, where it picks up oxygen and releases carbon dioxide, becoming arterial blood again. This may seem like a very natural picture to you, but it is far from easy to deduce by simple observation.\n\n\n\nHuman blood circulates throughout the body and returns to the heart, veins shown in blue and arteries in red. Circulatory System en by LadyofHats in public domain via Wikimedia Commons.\n\n\n\n1.1.1 Galen’s theory of blood\nGalen came up with a different explanation based on the notion of humors, or fluids, that was fundamental to the Greek conception of the body. He proposed that the venous and arterial blood were different humors: venous blood, or natural spirits, was produced by the liver, while arterial blood, or vital spirits, was produced by the heart and carried by the arteries, as shown in figure \\(\\ref{fig:galen_blood}\\). The heart consisted of two halves, and it warmed the blood and pushed both the natural and vital spirits out to the organs; the two spirits could mix through pores in the septum separating its right and left halves. The vital and natural spirits were both consumed by the organs, and regenerated by the liver and the heart. The purpose of the lungs was to serve as bellows, cooling the blood after it was heated by the heart.\n\n\n\nIllustration of Galen’s conception of the blood system, showing different spirits traveling in one direction, but not circulating. Reproduced by permission of Barbara Becker.\n\n\nIs this a good theory of how the heart, lungs, and blood work? Doctors in Europe thought so for over one thousand years! Galen’s textbook on physiology was the standard for medical students through the 17th century. The theory seemed to make sense, and explain what was observable. Many great scientists and physicians, including Leonardo DaVinci and Avicenna, did not challenge the inaccuracies such as the porous septum in the heart, even though they could not see the pores themselves. It took both better observations and a quantitative testing of the hypothesis to challenge the orthodoxy.\n\n\n1.1.2 Mathematical testing of the theory\nWilliam Harvey was born in England and studied medicine in Padua under the great physician Hieronymus Fabricius. He became famous, and would perform public demonstrations of physiology, using live animals for experiments that would not be approved today. He also studied the heart and the blood vessels, and measured the volume of the blood that can be contained in the human heart. He was quite accurate in estimating the correct volume, which we now know to be about 70 ml (1.5 oz). What is even more impressive is that he used this quantitative information to test Galen’s theory.\nLet us assume that all of the blood that is pumped out by the heart is consumed by the tissues, as Galen proposed; let us further assume that the heart beats at constant rate of 60 beats per minute, with a constant ejection volume of 70 ml. Then over the course of a day, the human body would consume about \\[\\mathrm{Volume} = 70 \\ \\mathrm {mL} \\times 60 \\ \\mathrm {(beats \\ per \\ minute)} \\times 60 \\ \\mathrm {(minutes \\ per \\ hour)}  \\times 24 \\ \\mathrm {(hours \\ per \\ day)}\\] or over 6,000 liters of blood! You may quibble over the exact numbers (some hearts beat faster or slower, some hearts may be larger or smaller) but the impact of the calculation remains the same: it is an absurd conclusion. Galen’s theory would require the human being to consume and produce a quantity of fluid many times the volume of the human body (about 100 liters) in a day! This is a physical impossibility, so the only possible conclusion in that Galen’s model is wrong.\nThis led Harvey to propose the model that we know today: that blood is not consumed by the tissues, but instead returns to the heart and is re-used again . This is why we call the heart and blood vessels part of the circulatory system of the body. This model was controversial at the time - some people proclaimed they would ``rather be wrong with Galen, than right with Harvey’’ - but eventually became accepted as the standard model. What is remarkable is that Harvey’s argument, despite being grounded in empirical data, was strictly mathematical. He adopted the assumptions of Galen, made the calculations, and got a result which was inconsistent with reality. This is an excellent example of how mathematical modeling can be useful, because it can provide clear evidence against a wrong hypothesis."
  },
  {
    "objectID": "counting.html#sec:math1",
    "href": "counting.html#sec:math1",
    "title": "1  Arithmetic and variables",
    "section": "1.2 Parameters and variables in models",
    "text": "1.2 Parameters and variables in models\nMany biologists remain skeptical of mathematical modeling. The criticism can be summarized like this: a theoretical model either agrees with experiment, or it does not. In the former case, it is useless, because the data are already known; in the latter case, it is wrong! As I indicated above, the goal of mathematical modeling is not to reproduce experimental data; otherwise, indeed, it would only be of interest to theoreticians. The correct question to ask is, does a theoretical model help us understand the real thing? There are at least three ways in which a model can be useful:\n\nA model can help a scientist make sense of complex data, by testing whether a particular mechanism explains the observations. Thus, a model can help clarify our understanding by throwing away the non-essential features and focusing on the most important ones.\nA mathematical model makes predictions for situations that have not been observed. It is easy to change parameters in a mathematical model and calculate the effects. This can lead to new hypotheses that can be tested by experiments.\nModel predictions can lead to better experimental design. Instead of trying a whole bunch of conditions, the theoretical model can suggest which ones will produce big effects, and thus can save a lot of work for the lab scientist.\n\nIn order to make a useful model of a complex living system, you have to simplify it. Even if you are only interested in a part of it, for instance a cell or a single molecule, you have to make simplifying choices. A small protein has thousands of atoms, a cell consists of millions of molecules, which all interact with each other; keeping track mathematically of every single component is daunting if not impossible. To build a useful mathematical model one must choose a few quantities which describe the system sufficiently to answer the questions of interest. For instance, if the positions of a couple of atoms in the protein you are studying determine its activity, those positions would make natural quantities to include in your model. You will find more specific examples of models later in this chapter.\nOnce you have decided on the essential quantities to be included in the model, these are divided into variables and parameters. As suggested by the name, a variable typically varies over time and the model tracks the changes in its value, while parameters usually stay constant, or change more slowly. However, that is not always the case. The most important difference is that variables describe quantities within the system being modeled, while parameters usually refer to quantities which are controlled by something outside the system.\nAs you can see from this definition, the same quantity can be a variable or a parameter depending on the scope of the model. Let’s go back to our example of modeling a protein: usually the activity (and the structure) of a protein is influenced by external conditions such as pH and temperature; these would be natural parameters for a model of the molecule. However, if we model an entire organism, the pH (e.g. of the blood plasma) and temperature are controlled by physiological processes within the organism, and thus these quantities will now be considered variables.\nPerhaps the clearest way to differentiate between variables and parameters is to think about how you would present a data set visually. We will discuss plotting graphs of functions in chapter 2, and plotting data sets in chapter 3, but the reader has likely seen many such plots before. Consider which of the quantities you would to plot to describe the system you are modeling. If the quantity belongs on either axis, it is a variable, since it is important to describe how it changes. The rest of the quantities can be called parameters. Of course, depending on the question you ask, the same quantity may be plotted on an axis or not, which is why this classification is not absolute.\nAfter we have specified the essential variables for your model, we can describe a complex and evolving biological system in terms of its state. This is a very general term, but it usually means the values of all the variables that you have chosen for the model, which are often called state variables. For instance, an ion channel can be described with the state variable of conformation, which may be in a open state or in a closed state. The range, or collection of all different states of the system is called the state space of the model. Below you will find examples of models of biological systems with diverse state spaces.\n\n1.2.1 discrete state variables: genetics\nThere are genes which are present in a population as two different versions, called *alleles} - let us use letters \\(A\\) and \\(B\\) to label them. One may describe the genetic state of an individual based on which allele it carries. If this individual is haploid, e.g. a bacterium, then it only carries a single copy of the genome, and its state can be described by a single variable with the state space of \\(A\\) or \\(B\\).\nA diploid organism, like a human, possesses two copies of each gene (unless it is on one of the sex chromosomes, X or Y); each copy may be in either state \\(A\\) or \\(B\\). This may seem to suggest that there are four different values in the genetic state space, but if the order of the copies does not matter (which is usually the case), then \\(AB\\) and \\(BA\\) are effectively the same, so the state space consists of three values: \\(AA\\), \\(BB\\), and \\(AB\\).\n\n\n1.2.2 discrete state variables: population\nConsider the model of a population of individuals, with the variable of number of individuals (populations size) and parameters being the birth and death rates. The state space of this model is all integers between 0 and infinity.\nConsider the model of a population of individuals who may get infected. Assume that the total number of individuals does not change (that is, there are no births and deaths) and that these individuals can be in one of two states: healthy or sick (in epidemiology these are called susceptible or infectious). There are typically two parameters in such models: the probability of infection and the probability of recovery. Since the total population is fixed at some number \\(N\\), the space space of the model is all pairs of integers between 0 and \\(N\\) that add up to \\(N\\).\n\n\n1.2.3 continuous state variables: concentration\nSuppose that a biological molecule is produced with a certain rate and degraded with a different rate, and we would like to describe the quantity of the molecule, usually expressed as concentration. The relevant variables here are concentration and time, and you will see those variables on the axes of many plots in biochemistry. Concentration is a ratio of the number of molecules and the volume, so the state space can be any positive real number (although practically there is a limit as to how many molecules can fit inside a given volume, but for simplicity we can ignore this).\nGoing even further, let us consider an entire cell, which contains a large number of different molecules. We can describe the state of a cell as the collection of all the molecular concentrations, with the parameters being the rates of all the reactions going on between those molecules. The state space for this model with \\(N\\) different molecules is \\(N\\) positive real numbers.\n\n\n1.2.4 multiple variables in medicine\nDoctors take medical history from patients and measure vital signs to get a picture of a patient’s health. These can be all be thought of as variables in a model of a person that physicians construct. Some of these variables are discrete, for instance whether there is family history of hypertension, which has only two values: yes or no. Other variables are numbers with a range, such as weight and blood pressure. The state space of this model is a combination of categorical values (such as yes/no) and numerical values (within a reasonable range).\n\n\n1.2.5 Discussion questions\nSeveral biological models are indicated below. Based on what you know, divide the quantities into variables and parameters and describe the state space of the model. Note that there may be more than one correct interpretation\n\nThe volume of blood pumped by the heart over a certain amount of time, depending on the heart rate and the ejection volume.\nThe number of wolves in a national forest depending on the number of wolves in the previous year, the birth rate, the death rate, and the migration rate.\nThe fraction of hemes in hemoglobin (a transport protein in red blood cells) which are bound to oxygen depending on the partial pressure of oxygen and the binding cooperativity of hemoglobin.\nThe number of mutations that occur in a genome, depending on the mutation rate, the amount of time, and the length of the genome.\nThe concentration of a drug in the blood stream depending on the dose, time after administration, and the rate of metabolism (processing) of the drug.\nDescribing an outbreak of an infectious disease in a city in terms of the fractions of infected, healthy, and recovered people, depending on the rate of infection, rate of recovery, and the mortality rate of the disease."
  },
  {
    "objectID": "counting.html#first-steps-in-r",
    "href": "counting.html#first-steps-in-r",
    "title": "1  Arithmetic and variables",
    "section": "1.3 First steps in R",
    "text": "1.3 First steps in R\n A central goal of this book is to help you, the reader, gain experience with computation, which requires learning some programming (cool kids call it “coding”). Programming is a way of interacting with computers through a symbolic language, unlike the graphic user interfaces that we’re all familiar with. Basically, programming allows you to make a computer do exactly what you want it to do.\nThere is a vast number of computer languages with distinct functionalities and personalities. Some are made to talk directly to the computer’s “brain” (CPU and memory), e.g. Assembly, while others are better suited for human comprehension, e.g. python or Java. Programming in any language involves two parts: 1) writing a program (code) using the commands and the syntax for the language; 2) running the code by using a compiler or interpreter to translate the commands into machine language and then making the computer execute the actions. If your code has a mistake in it, the compiler or interpreter should catch it, and return an error message to you instead of executing the code. Sometimes, though, the code may pass muster with the interpreter/compiler, but it may still have a mistake (bug). This can be manifested in two different ways: either the code execution does not produce the result that you intended, or it hangs up or crashes the computer (the latter is hard to do with the kind of programming we will be doing). We will discuss errors and how to prevent and catch these bugs as you develop your programming skills.\nIn this course, our goal is to compute mathematical models and to analyze data, so we choose a language that is designed specially for these tasks, which is called R. To proceed, you’ll need to download and install R, which is freely available here. In addition to downloading the language (which includes the interpreter that allows you to run R code on your computer) you will need to download a graphic interface for writing, editing, and running R code, called R Studio (coders call this an IDE, or an Integrated Developer Environment), which is also free and available here.\n\n1.3.1 R Markdown and R Studio\nIn this course you will use R using R Studio and R Markdown documents, which are text files with the extension .Rmd. Markdown is a simple formatting syntax for creating reports in HTML, PDF, or Word format by incorporating text with code and its output. More details on using R Markdown are here. In fact, this whole book is written in R Markdown files and then compiled to produce the beautiful (I hope you agree) web book that you are reading.\nIf you open an Rmd file in R Studio, you will see a Knit button on top of the Editor window. Clicking it initiates the processing of the file into an output document (in HTML, PDF, or Word format) that includes the text as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:\n\nprint(\"Hello there!\") \n\n[1] \"Hello there!\"\n\n\nTo run the code inside a single R code chunk, click the green arrow in the top right of the chunk. This will produce an output, in this case the text “Hello there!”. Inside the generated output file, for example the web book you may be reading, the output of code chunks is shown below the box with the R code and indicated by two hashtags.\nYou can make text bold or italic like so. You can also use mathematical notation called LaTeX, which you’ll see used below to generate nice-looking equations. LaTeX commands are surrounded by dollar signs, for example $e^x$ generates \\(e^x\\). Mathematical types love LaTeX, but you can use R Markdown without it.\n\n\n1.3.2 numbers and arithmetic operations\nWhen you get down to the brass tacks, all computation rests on performing arithmetic operations: addition, subtraction, multiplication, division, exponentiation, etc. The symbols used for arithmetic operations are what you’d expect: +, -, *, / are the four standard operations, and ^ is the symbol for exponentiation. For example, type 2^3 in any R code chunk and execute it:\n\n2^3\n\n[1] 8\n\n\nYou see that R returns the result by printing it out on the screen. The number in square brackets [1] is not important for now; it is useful when the answer contains many numbers and has to be printed out on many rows. The second number is the result of the calculation.\nFor numbers that are either very large or very small, it’s too cumbersome to write out all the digits, so R, like most computational platforms, uses the scientific notation. For instance, if you want to represent 1.4 billion, you type in the following command; note that 10 to the ninth power is represented as e+09 and the prefix 1.4 is written without any multiplication sign:\n\n1.4*10^9\n\n[1] 1.4e+09\n\n\nThere are also certain numbers built into the R language, most notably \\(\\pi\\) and \\(e\\), which can be accessed as follows:\n\npi\n\n[1] 3.141593\n\nexp(1)\n\n[1] 2.718282\n\n\nThe expression exp() is an example of a function, which we will discuss in section \\(\\ref{sec:comp2}\\); it returns the value of \\(e\\) raised to the power of the number in parenthesis, hence exp(1) returns \\(e\\). Notice that although both numbers are irrational, and thus have infinitely many decimal digits, R only prints out a few of them. This doesn’t mean that it doesn’t have more digits in memory, but it only displays a limited number to avoid clutter. The number of digits to be displayed can be changed, for example to display 10 digits, type in options(digits=10).\nComputers are very good at computation, as their name suggests, but they have limitations. In order to manipulate numbers, they must be stored in computer memory, but computer memory is finite. There is a limit to the length of the number that is feasible to store on a computer. This has implications for both very large numbers and to very small numbers, which are close to zero, because both require many digits for storage.\nAll programming languages have an upper limit on the biggest number it will store and work with. If an arithmetic operation results in a number larger than that limit, the computer will call it an overflow error. Depending on the language, this may stop the execution of the program, or else produce a non-numerical value, such as NaN (not a number) or Inf (infinite). Do exercise \\(\\ref{ex:overflow}\\) to investigate the limitations of R for large numbers.\nOn the other hand, very small numbers present their own challenges. As with very large numbers, a computer cannot store an arbitrary number of digits after the decimal (or binary) point. Therefore, there is also the smallest number that a programming language will accept and use, and storing a smaller number produces an underflow error. This will either cause the program execution to stop, or to return the value 0 instead of the correct answer. Do exercise \\(\\ref{ex:underflow}\\) to investigate the limitations of R for small numbers.\nThis last fact demonstrates that all computer operations are imprecise, as they are limited by what’s called the machine precision, which is illustrated in exercise \\(\\ref{ex:mach_prec}\\). For instance, two similar numbers, if they are within the machine precision of one another, will be considered the same by the computer. Modern computers have large memories, and their machine precision is very good, but sometimes this error presents a problem, e.g. when subtracting two numbers. A detailed discussion of machine error is beyond the scope of this text, but anyone performing computations must be aware of its inherent limitations.\n\n\n1.3.3 R Coding Exercises\n\nCalculate the value of \\(\\pi\\) raised to the 10th power.\nUse the scientific notation to multiply four billion by \\(\\pi\\).\nUse the scientific notation with large exponents (e.g. 1e+100, 1e+500, etc.) to find out what happens when you give R a number that is too large for it to handle. Approximately at what order of magnitude does R produce an overflow error?\nIn the same fashion, find out what happens when you give R a number that is too small for it to handle. Approximately at what order of magnitude does R produce an underflow error?\nHow close can two numbers be before R thinks they are the same? Subtract two numbers which are close to each other, like 24 and 24.001, and keep making them closer to each other, until R returns a difference of zero. Report at what value of the actual difference this happens.\n\n\n\n1.3.4 variable assignment\nVariables in programming languages are used to store and access numerical or other information. After assigning} it a value for the first time (initializing), a variable name can be used to represent the value we assigned to it. Invoking the name of variable recalls the stored value from computer’s memory. There are a few rules about naming variables: a name cannot be a number or an arithmetic operator like +, in fact it cannot contain symbols for operators or spaces inside the name, or else confusion would reign. Variable names may contain numbers, but not as the first character. When writing code it is good practice to give variables informative names, like height or city_pop. \nThe symbol ‘=’ is used to assign a value to a variable in most programming languages, and can be used in R too. However, it is customary for R to use the symbols <- together to indicate assignment, like this:\n\nvar1 <- 5\n\nAfter this command the variable var1 has the value 5, which you can see in the upper right frame in R Studio called Environment. In order to display the value of the variable as an output on the screen, use the special command print() (it’s actually a function, which we will discuss in the next chapter). The following two commands show that the value of a variable can be changed after it has been initialized:\n\nvar1 <- 5\nvar1 <- 6\nprint(var1)\n\n[1] 6\n\n\nWhile seemingly contradictory, the commands are perfectly clear to the computer: first var1 is assigned the value 5 and then it is assigned 6. After the second command, the first value is forgotten, so any operations that use the variable var1 will be using the value of 6.\nEntire expressions can be placed on the right hand side of an assignment command: they could be arithmetic or logical operations as well as functions, which we will discuss later on. For example, the following commands result in the value 6 being assigned to the variable var2:\n\nvar1 <- 5\nvar2 <- var1+1\nprint(var2)\n\n[1] 6\n\n\nEven more mind-blowing is that the same variable can be used on both sides of an assignment operator! The R interpreter first looks on the right hand side to evaluate the expression and then assigns the result to the variable name on the left hand side. So for instance, the following commands increase the value of var1 by 1, and then assign the product of var1 and var2 to the variable var2:\n\nvar1 <- var1 + 1\nprint(var1)\n\n[1] 6\n\nvar2 <- var1-1\nprint(var2)\n\n[1] 5\n\nvar2 <- var1*var2\nprint(var2)\n\n[1] 30\n\n\nWe have seen example of how to assign values to variables, so here is an example of how NOT to assign values, with the resulting error message:\n\nvar1 + 1 <- var1\n\nThe left-hand side of an assignment command should contain only the variable to which you are assigning a value, not an arithmetic expression to be performed.\n\n\n1.3.5 R Coding Exercises\nThe following commands or scripts do not work as intended. Find the errors and correct them, then run them to make sure they do what they are intended to do:\n\n\n1.3.6 Exercises:\nThe following R commands or short scripts contain errors; your job is to fix them so they runs as described. (Remove the # at the start of each line to “uncomment” the code first.)\n\nAssign the value -10 to a variable\n\n\nneg -> -10\n\n\nAssign a variable the value 5 and then increase its value by 3:\n\n\n2pac <- 5\n2pac <- 2pac + 3\n\n\nAssign the values 4 and 7 to two variables, then add them together and assign the sum to a new variable:\n\n\ntotal <- part1 + part2\npart1 <- 4\npart2 <- 7\n\n\nAdd 5 and 3 and save it into variable my.number\n\n\n5 + 3 <- my.number\n\n\nPrint the value of my.number on the screen:\n\n\nprint[my.number]\n\n\nReplace the value of my.number with 5 times its current value\n\n\nmy.number <- 5my.number \n\n\nAssign the values of 7 and 8 to variables a and b, respectively, multiply them and save the results in variable x\n\n\na<-7\nb<-8\nx<-ab\nprint(x)\n\n\nAssign the value 42 to a variable, then increase it by 1\n\n\nage <- 42\nage + 1 <- age\n\n\nAssign the value 10 to variable radius, then calculate the area of the circle with that radius using the formula \\(A = \\pi r^2\\):\n\n\nr <- 10\narea <- pir^2"
  },
  {
    "objectID": "functions.html#sec:model2",
    "href": "functions.html#sec:model2",
    "title": "2  Functions and their graphs",
    "section": "2.1 Dimensions of quantities",
    "text": "2.1 Dimensions of quantities\nWhat distinguishes a mathematical model from a mathematical equation is that the quantities involved have a real-world meaning. Each quantity represents a measurement, and associated with each one are the units of measurement. The number 173 is not enough to describe the height of a person - you are left to wonder 173 what? meters, centimeters, nanometers, light-years? Obviously, only centimeters make sense as a unit of measurement for human height; but if we were measuring the distance between two animals in a habitat, meters would be a reasonable unit, and it were the distance between molecules in a cell, we would use nanometers. Thus, any quantity in a mathematical model must have associated units, and any graphs of these quantities must be labeled accordingly.\nIn addition to units, each variable and parameter has a meaning, which is called the dimension of the quantity. For example, any measurement of length or distance has the same dimension, although the units may vary. The value of a quantity depends on the units of measurement, but its essential dimensionality does not. One can convert a measurement in meters to that in light-years or cubits, but one cannot convert a measurement in number of sheep to seconds - that conversion has no meaning.\nThus leads us to the fundamental rule of mathematical modeling: terms that are added or subtracted must have the same dimension. This gives mathematical modelers a useful tool called dimensional analysis, which involves replacing the quantities in an equation with their dimensions. This serves as a check that all dimensions match, as well as allowing to deduce the dimensions of any parameters for which the dimension was not specified. \nExample. As we saw in chapter 1, the relationship between the amount blood pumped by a heart in a certain amount of time is expressed in the following equation, where \\(V_{tot}\\) and \\(V_s\\) are the total volume and stroke volume, respectively, \\(R\\) is the heart rate, and \\(t\\) is the time: \\[\nV_{tot} = V_sRt\n\\] The dimension of a quantity \\(X\\) is denoted by \\([X]\\); for example, if \\(t\\) has the dimension of time, we write \\([t] = time\\). The dimension of volume is \\([V_{tot}] = length^3\\), the dimension of stroke volume is \\([V_s] = volume/beat\\) and the dimension of time \\(t\\) is time, so we can re-write the equation above in dimensional form:\n\\[length^3 = length^3/ beat \\times R \\times time\\]\nSolving this equation for R, we find that it must have the dimensions of \\([R] = beats/time\\). It can be measured in beats per minute (typical for heart rate), or beats per second, beats per hour, etc. but the dimensionality of the quantity cannot be changed without making the model meaningless.\nThere are also dimensionless quantities, or pure numbers, which are not tied to a physical meaning at all. Fundamental mathematical constants, like \\(\\pi\\) or \\(e\\), are classic examples, as are some important quantities in physics, like the Reynolds number in fluid mechanics. Quantities with a dimension can be made dimensionless by dividing them by another quantity with the same dimension and “canceling” the dimensions. For instance, we can express the height of a person as a fraction of the mean height of the population; then the height of a tall person will become a number greater than 1, and the height of a short one will become less than 1. This new dimensionless height does not have units of length - they have been divided out by the mean height. This is known as rescaling the quantity, by dividing it by a preferred scale. There is a fundamental difference between rescaling and changing the units of a quantity: when changing the units, e.g. from inches to centimeters, the dimension remains the same, but if one divides the quantity by a scale, it loses its dimension.\nExample. The model for a population of bacteria that doubles every hour is described by the equation, where \\(P_0\\) is initial number of bacteria and \\(P\\) is the population after \\(t\\) hours: \\[ P = P_0 2^t \\] Let us define the quantity \\(R=P/P_0\\), so we can say that population increased by a factor of \\(R\\) after \\(t\\) hours. This ratio is a dimensionless quantity because \\(P\\) and \\(P_0\\) have the same dimension of bacterial population, which cancel out. The equation for \\(R\\) can be written as follows: \\[ R= 2^t \\] According to dimensional analysis, both sides of the equation have to be dimensionless, so \\(t\\) must also be a dimensionless variable. This is surprising, because \\(t\\) indicates the number of hours the bacterial colony has been growing. This reveals the subtle fact that \\(t\\) is a rescaled variable obtained by dividing the elapsed time by the length of the reproductive cycle. Because of the assumption that the bacteria divide exactly once an hour, \\(t\\) counts the number of hours, but if they divided once a day, \\(t\\) would denote the number of days. So \\(t\\) doesn’t have units or dimensions, but instead denotes the dimensionless number of cell divisions.\n\n2.1.1 Exercises\nFor each biological model below determine the dimensions of the parameters, based on the given dimensions of the variables.\n\nModel of number of mutations \\(M\\) as a function of time \\(t\\): \\[ M(t) = M_0 + \\mu t\\]\nModel of molecular concentration \\(C\\) as a function of time \\(t\\): \\[ C(t) = C_0 e^{-kt} \\]\nModel of tree height \\(H\\) (length) as a function of age \\(a\\) (time): \\[ H(a) = \\frac{b a}{c + a}\\]\nModel of cooperative binding of ligands, with fraction of bound receptors \\(\\theta\\) as a function of ligand concentration \\(L\\): \\[ \\theta (L) = \\frac{L^n}{L^n + K_d}\\]\nModel of concentration of a gene product \\(G\\) (concentration) as a function of time \\(t\\): \\[ G(t) = G_m (1 - e^{-\\alpha t})\\]\nMichaelis-Menten model of enzyme kinetics, \\(v\\) is reaction rate (1/time) and \\(S\\) is substrate concentration: \\[ v(S) = \\frac{v_{max} S}{K_m + S}\\]\nLogistic model of population growth, \\(P\\) is population size and time \\(t\\): \\[ P(t) = \\frac{A e^{kt}}{1 + B(e^{kt} -1)} \\]"
  },
  {
    "objectID": "functions.html#sec:math2",
    "href": "functions.html#sec:math2",
    "title": "2  Functions and their graphs",
    "section": "2.2 Functions and their graphs",
    "text": "2.2 Functions and their graphs\nA relationship between two variables addresses the basic question: when one variable changes, how does this affect the other? An equation, like the examples in the last section, allows one to calculate the value of one variable based on the other variable and parameter values. In this section we seek to describe more broadly how two variables are related by using the mathematical concept of functions.\n\n\n\n\n\n\nDefinition\n\n\n\nA function is a mathematical rule which has an input and an output. A function returns a well-defined output for every input, that is, for a given input value the function returns a unique output value.\n\n\nIn this abstract definition of a function it doesn’t have to be written as an algebraic equation, it only has to return a unique output for any given input value. In mathematics we usually write them down in terms of algebraic expressions. As in mathematical models, you will see two different kinds of quantities in equations that define functions: variables and parameters. The input and the output of a function are usually variables, with the input called the independent variable and the output called the dependent variable.\nThe relationship between the input and the output can be graphically illustrated in a graph, which is a collection of paired values of the independent and dependent variable drawn as a curve in the plane. Although it shows how the two variables change relative to each other, parameters may change too, which results in a different graph of the function. While graphing calculators and computers can draw graphs for you, it is very helpful to have an intuitive understanding about how a function behaves, and how the behavior depends on the parameters. Here are the three questions to help picture the relationship (assume \\(x\\) is the independent variable and it is a nonnegative real number):\n\nwhat is the value of the function at \\(x=0\\)?\nwhat does the function do when \\(x\\) becomes large (\\(x \\to \\infty\\))?\nwhat does the function do between the two extremes?\n\nBelow you will find examples of fundamental functions used in biological models with descriptions of how their parameters influence their graphs.\n\n2.2.1 linear and exponential functions\nThe reader is probably familiar with linear and exponential functions from algebra courses. However, they are so commonly used that it is worth going over them to refresh your memory and perhaps to see them from another perspective.\n\n\n\n\n\n\nDefinition\n\n\n\nA linear function \\(f(x)\\) is one for which the difference in two function values is the same for a specific difference in the independent variable.\n\n\nIn mathematical terms, this can be written an equation for any two values of the independent variable \\(x_1\\) and \\(x_2\\) and a difference \\(\\Delta x\\):\n\\[ f(x_1 + \\Delta x) - f(x_1) = f(x_2 + \\Delta x) - f(x_2) \\] The general form of the linear function is written as follows:\n\\[\\begin{equation}\nf(x) = ax + b\n\\label{eq:linear_funk}\n\\end{equation}\\]\nThe function contains two parameters: the slope \\(a\\) and the y-intercept \\(b\\). The graph of the linear function is a line (hence the name) and the slope \\(a\\) determines its steepness. A positive slope corresponds to the graph that increases as \\(x\\) increases, and a negative slope corresponds to a declining function. At \\(x=0\\), the function equals \\(b\\), and as \\(x \\to \\infty\\), the function approaches positive infinity if \\(a>0\\), and approaches negative infinity if \\(a<0\\).\n\n\n\n\n\n\nDefinition\n\n\n\nAn exponential function \\(f(x)\\) is one for which the ratio of two function values is the same for a specific difference in the independent variable.\n\n\nMathematically speaking, this can be written as follows for any two values of the independent variable \\(x_1\\) and \\(x_2\\) and a difference \\(\\Delta x\\): \\[ \\frac{f(x_1 + \\Delta x)}{f(x_1)} = \\frac{f(x_2 + \\Delta x)}{f(x_2)}\\]\nExponential functions can be written using different symbolic forms, but they all have a constant base with the variable \\(x\\) in the exponent. I prefer to use the constant \\(e\\) (base of the natural logarithm) as the base of all the exponential functions, for reasons that will become apparent in chapter 15. This does not restrict the range of possible functions, because any exponential function can be expressed using base \\(e\\), using a transformation: \\(a^x = e^{x \\ln(a)}\\). So let us agree to write exponential functions in the following form:\n\\[\\begin{equation}\nf(x) = a e^{rx}\n\\label{eq:exp_funk}\n\\end{equation}\\]\nThe function contains two parameters: the \\(r\\) and the multiplicative constant \\(a\\). The graph of the exponential function is a curve which crosses the y-axis at \\(y=a\\) (plug in \\(x=0\\) to see that this is the case). As \\(x\\) increases, the behavior of the graph depends on the sign of the rate constant \\(r\\). If \\(r>0\\), the function approaches infinity (positive if \\(a>0\\), negative if \\(a<0\\)) as \\(x \\to \\infty\\). If \\(r<0\\), the function decays at an ever-decreasing pace and asymptotically approaches zero as \\(x \\to \\infty\\). Thus the graph of \\(f(x)\\) is a curve either going to infinity or a curve asymptotically approaching 0, and the steepness of the growth or decay is determined by \\(r\\).\n\n\n\n\n\nPlots of two linear functions (left) and two exponential functions (right). Can you identify which linear function has the positive slope and which one negative? Which exponential function has a positive rate constant and which one negative?\n\n\n\n\n\n\n\nPlots of two linear functions (left) and two exponential functions (right). Can you identify which linear function has the positive slope and which one negative? Which exponential function has a positive rate constant and which one negative?\n\n\n\n\n\n\n2.2.2 Exercises\nAnswer the questions below, some of which refer to the function graphs in figure @ref(ch2-funk1).\n\nWhich of the linear graphs in the first figure corresponds to \\(f(x) = 5x\\) and which corresponds to \\(f(x) = 10-x\\)? State which parameter allows you to connect the function with its graph and explain why.\nWhich of the exponential graphs in the second figure corresponds to \\(f(x) = 0.1e^{0.5x}\\) and which corresponds to \\(f(x) = 12e^{-0.2x}\\)? State which parameter allows you to connect the function with its graph and explain why.\nDemonstrate algebraically that a linear function of the form given in equation \\(\\ref{eq:linear_funk}\\) satisfies the property of linear functions from definition .\nDemonstrate algebraically that an exponential function of the form given in equation \\(\\ref{eq:exp_funk}\\) satisfies the property of exponential functions from definition .\nModify the exponential function by adding a constant term to it \\(f(x) = a e^{rx} + b\\). What is is the value of this function at \\(x=0\\)?\nHow does the function defined in the previous exercise, \\(f(x) = a e^{rx} + b\\), how does it behave as \\(x \\to \\infty\\) if \\(r>0\\)?\nHow does the function \\(f(x) = a e^{rx} + b\\) behave as \\(x \\to \\infty\\) if \\(r<0\\)?\n\n\n\n2.2.3 rational and logistic functions\nLet us now turn to more complex functions, made up of simpler components that we understand. Consider a ratio of two polynomials, called a rational function. The general form of such functions can be written down as follows, where ellipsis stands for terms with powers lower than \\(n\\) or \\(m\\): \\[\\begin{equation}\nf(x) = \\frac{a_0 + ... + a_n x^n}{b_0 + ... + b_m x^m}\n\\label{eq:rational_funk}\n\\end{equation}\\] The two polynomials may have different degrees (highest power of the terms, \\(n\\) and \\(m\\)), but they are usually the same in most biological examples. The reason is that if the numerator and the denominator are ``unbalanced’’, one will inevitably overpower the other for large values of \\(x\\), which would lead to the function either increasing without bound to infinity (if \\(n>m\\)) or decaying to zero (if \\(m>n\\)). There’s nothing wrong with that, mathematically, but rational functions are most frequently used to model quantities that approach a nonzero asymptote for large values of the independent variable.\nFor this reason, let us assume \\(m=n\\) and consider what happens as \\(x \\to \\infty\\). All terms other than the highest-order terms become very small in comparison to \\(x^n\\) (this is something you can demonstrate to yourself using R), and thus both the numerator and the denominator approach the terms with power \\(n\\). This can be written using the mathematical limit notation \\(\\lim_{x \\to \\infty}\\) which describes the value that a function approaches when the independent variable increases without bound: \\[  \\lim_{x \\to \\infty} \\frac{a_0 + ... + a_n x^n}{b_0 + ... + b_n x^n} = \\frac{ a_n x^n}{ b_n x^n}  =  \\frac{ a_n}{ b_n}  \\] Therefore, the function approaches the value of \\(a_n /b_n\\) as \\(x\\) grows.\nSimilarly, let us consider what happens when \\(x=0\\). Plugging this into the function results in all of the terms vanishing except for the constant terms, so \\[ f(0) =  \\frac{ a_0}{ b_0} \\] Between 0 and infinity, the function either increases or decreases monotonically, depending on which value (\\(a_n /b_n\\) or \\(a_0/b_0\\)) is greater. Two examples of plots of rational functions are shown in figure \\(\\ref{fig:ch2_sigmoidal_plots}\\), which shows graphs increasing from 0 to 1. Depending on the degree of the polynomials in a rational function, it may increase more gradually (solid line) or more step-like (dashed line).\n The following model, called the Hill equation , describes the fraction of receptor molecules which are bound to a ligand, which is a chemical term for a free molecule that binds to another, typically larger, receptor molecule. \\(\\theta\\) is the fraction of receptors bound to a ligand, \\(L\\) denotes the ligand concentration, \\(K_d\\) is the dissociation constant, and \\(n\\) called the binding cooperativity or Hill coefficient: \\[ \\theta = \\frac{L^n}{ L^n +K_d}\\]\nThe Hill equation is a rational function, and Figure \\(\\ref{fig:ch2_sigmoidal_plots}\\) shows plots of the graphs of two such function in the right panel. This model is further explored in exercise 2.2.10.\nExample. A common model of population over time is the logistic function. There are variations on how it is written down, but here is one general form: \\[\\begin{equation}\nf(x) = \\frac{a e^{rx} }{b+e^{rx}}\n\\label{eq:logistic_funk}\n\\end{equation}\\]\nThe numerator and denominator both contain exponential functions with the same power. If \\(r>0\\) when \\(x \\to \\infty\\), the denominator approaches \\(e^{rx}\\), since it becomes much greater than \\(b\\), and we can calculate: \\[  \\lim_{x \\to \\infty}  =  \\frac{a e^{rx} }{e^{rx}} = a; \\; \\mathrm{if} \\; r>0 \\]\nOn the other hand, if \\(r<0\\), then the numerator approaches zero as \\(x \\to \\infty\\), and so does the function \\[  \\lim_{x \\to \\infty}  =  \\frac{0}{b} = 0; \\; \\mathrm{if} \\; r<0 \\]\nNotice that switching the sign of \\(r\\) has the same effect as switching the sign of \\(x\\), since they are multiplied. Which means that for positive \\(r\\), if \\(x\\) is extended to negative infinity, the function approaches 0. This is illustrated in the second plot in Figure \\(\\ref{fig:ch2_sigmoidal_plots}\\), which shows two logistic functions increasing from 0 to a positive level, one with \\(a=20\\) (solid line) and the second with \\(a=10\\) (dashed line). The graph of logistic functions has a characteristic sigmoidal (S-shaped) shape, and its steepness is determined by the rate \\(r\\): if \\(r\\) is small, the curve is soft, if \\(r\\) is large, the graph resembles a step function.\n\n\n\n\n\nExamples of two graphs of logistic functions (left) and two Hill functions (right).\n\n\n\n\n\n\n\nExamples of two graphs of logistic functions (left) and two Hill functions (right).\n\n\n\n\n\n\n2.2.4 Exercises:\nFor each biological model below answer the following questions in terms of the parameters in the models, assuming all are nonnegative real numbers. 1) what is the value of the function when the independent variable is 0? 2) what value does the function approach when the independent variable goes to infinity? 3) verbally describe the behavior of the functions between 0 and infinity (e.g., function increases, decreases).\n\nModel of number of mutations \\(M\\) as a function of time \\(t\\): \\[ M(t) = M_0 + \\mu t\\]\nModel of molecular concentration \\(C\\) as a function of time \\(t\\): \\[ C(t) = C_0 e^{-kt} \\]\nModel of cooperative binding of ligands, with fraction of bound receptors \\(\\theta\\) as a function of ligand concentration \\(L\\): \\[ \\theta = \\frac{L^n}{L^n + K_d}\\]\nModel of tree height \\(H\\) (length) as a function of age \\(a\\) (time): \\[ H(a) = \\frac{b a }{c + a}\\]\nModel of concentration of a gene product \\(G\\) (concentration) as a function of time \\(t\\): \\[ G(t) = G_m (1 - e^{-\\alpha t})\\]\nMichaelis-Menten model of enzyme kinetics, \\(v\\) is reaction rate (1/time) and \\(S\\) is substrate concentration: \\[ v(S) = \\frac{v_{max} S}{K_m + S}\\]\nLogistic model of population growth, \\(P\\) is population size and time \\(t\\): \\[ P(t) = \\frac{A e^{kt}}{1 + B(e^{kt} -1)} \\]"
  },
  {
    "objectID": "functions.html#vectors-and-plotting-in-r",
    "href": "functions.html#vectors-and-plotting-in-r",
    "title": "2  Functions and their graphs",
    "section": "2.3 Vectors and plotting in R",
    "text": "2.3 Vectors and plotting in R\n\n\n2.3.1 writing scripts and calling functions\nProgramming means arranging a number of commands in a particular order to perform a task. Typing them one at a time into the command line is inefficient and error-prone. Instead, the commands are written into a file called a program or script (the name depends on the type of language; since R is a scripting language you will be writing scripts), which can be edited, saved, copied, etc. To open a new script file, in R Studio, go to File menu, and choose New R Script. This will open an editor window where you can type your commands. To save the script file (do this often!!), click the Save button (with the little floppy disk icon) or select Save from the File menu. You will also see small buttons at the top of the window that say Run, Re-run, and Source. The first two will run either the current line or a selected region of the script, while the Source button will run the entire file. Now that you know how to create a script, you should never type your R code into the command line, unless you’re testing a single command to see what it does, or looking up help.\nR comes equipped with many functions that correspond to standard mathematical functions. As we saw in section \\(\\ref{sec:comp1}\\), exp() is the exponential function that returns \\(e\\) raised to the power of the input value. Other common ones are: sqrt() returns the square root of the input value; sin() and cos() return the sine and the cosine of the input value, respectively. Note that all of these function names are followed by parentheses, which is a hallmark of a function (in R as well as in mathematics). This indicates that the input value has to go there, for example exp(5). To compute the value of \\(e^5\\), save it into a variable called var1 and then print out the value on the screen, you can create the following script:\n\nvar1 <- exp(5)\nprint(var1)\n\n[1] 148.4132\n\n\nIf you run the above code chunk in R Studio you will see two things happen: a variable named var1 appears in the Environment window (top right) with the value 148.41… and the same value is printed out in the command line window (bottom left).\nThe most important principle of the procedural brand of programming (which includes R) is this: the computer (that is, the compiler or interpreter) evaluates the commands from top to bottom, one at a time. The variables are used with the values that they are currently assigned. If one variable (var1) was assigned in terms of another (var2), and then var2 is changed later, this does not change the value of var2. Here is an illustration of how this works:\n\nvar2 <- 20\nvar1 <- var2/20\nprint(var2)\n\n[1] 20\n\nvar2 <- 10\nprint(var1)\n\n[1] 1\n\n\nNotice that var1 doesn’t change, because the R interpreter reads the commands one by one, and does not go back to re-evaluate the assignment for var1 after var2 is changed. Learning to think in this methodical, literal manner is crucial for developing programming skills.\n\n\n2.3.2 vector variables\nVariables may contain more than a single number, they can also store a bunch of numbers, which is then called an array. When numbers in an array are organized as a single ordered list, this is called a vector. There are several ways of producing a vector of numbers in R.\n\n2.3.2.1 c() function\nThe most direct method of making a vector is to put together several values by listing them inside the function c() and assigning the output to a variable, e.g. my.vec:\n\nmy.vec<-c(pi,45,912.8, 0)\nprint(my.vec)\n\n[1]   3.141593  45.000000 912.800000   0.000000\n\n\nThis variable my.vec is now a vector variable that contains four different numbers. Each of those numbers can be accessed individually by referencing its position in the vector, called the index. In the R language the the index for the first number in a vector is 1, the index for the second number is 2, etc. The index is placed in square brackets after the vector name, as follows:\n\nprint(my.vec[1])\n\n[1] 3.141593\n\nprint(my.vec[2])\n\n[1] 45\n\nprint(my.vec[3])\n\n[1] 912.8\n\nprint(my.vec[4])\n\n[1] 0\n\n\n\n\n2.3.2.2 the colon operator\nAnother way to generate a sequence of numbers in a particular order is to use the colon operator, which produces a vector of integers from the first number to the last, inclusive. Here are two examples:\n\nmy.vec1<-1:20\nprint(my.vec1)\n\n [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20\n\nmy.vec2<-0:-20\nprint(my.vec2)\n\n [1]   0  -1  -2  -3  -4  -5  -6  -7  -8  -9 -10 -11 -12 -13 -14 -15 -16 -17 -18\n[20] -19 -20\n\n\nYou can also access some but not all of the values stored in a vector simultaneously. To do this, enter a vector of positive integers inside the square brackets, either using the colon operator or using the c() function. Here are two examples, the first prints out the 4th through the 10th element of the vector my.vec1, while the second prints out the 1st, 5th, and 11th elements of the vector my.vec2:\n\nprint(my.vec1[4:10])\n\n[1]  4  5  6  7  8  9 10\n\nprint(my.vec2[c(1,5,11)])\n\n[1]   0  -4 -10\n\n\n\n\n2.3.2.3 seq() function\nIf you want to generate a sequence of numbers with a constant difference other than 1, you’re in luck: R provides a function called seq(). It takes three inputs: the starting value, the ending value, and the step (difference between successive elements). For example, to generate a list of numbers starting at 20 up to 50, with a step size of 3, type the first command; to obtain the same sequence in reverse, use the second command:\n\nmy.vec1<-seq(20,50,3)\nprint(my.vec1)\n\n [1] 20 23 26 29 32 35 38 41 44 47 50\n\nmy.vec2<-seq(50,20,-3)\nprint(my.vec2)\n\n [1] 50 47 44 41 38 35 32 29 26 23 20\n\n\n\n\n2.3.2.4 rep() function\nSometimes you want to create a vector of repeated values. For example, you can create a variable with 20 zeros, you can use rep() like this:\n\nzeros <- rep(0,20)\nprint(zeros)\n\n [1] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n\n\nYou can repeat any value, say create a vector by repeating the number pi:\n\npies <- rep(pi,7)\nprint(pies)\n\n[1] 3.141593 3.141593 3.141593 3.141593 3.141593 3.141593 3.141593\n\n\nYou can even repeat another vector, like the vector my.vec that was assigned above:\n\nmy.vecs <- rep(my.vec, 5)\nprint(my.vecs)\n\n [1]   3.141593  45.000000 912.800000   0.000000   3.141593  45.000000\n [7] 912.800000   0.000000   3.141593  45.000000 912.800000   0.000000\n[13]   3.141593  45.000000 912.800000   0.000000   3.141593  45.000000\n[19] 912.800000   0.000000\n\n\n\n\n\n2.3.3 calculations with vector variables\n\nNewVec <- 2*my.vec\nprint(NewVec)\n\n[1]    6.283185   90.000000 1825.600000    0.000000\n\n\nYou can also perform calculations with multiple vector variables, but this requires extra care. R can perform any arithmetic operation with two vector variables, for instance adding two vectors results in a vector containing the sum of corresponding elements of the two vectors:\n\nmy.vec1<-1:5\nmy.vec2<-0:4\nprint(my.vec1)\n\n[1] 1 2 3 4 5\n\nprint(my.vec2)\n\n[1] 0 1 2 3 4\n\nsum.vec<-my.vec1+my.vec2\nprint(sum.vec)\n\n[1] 1 3 5 7 9\n\n\nOne needs to take care that the two vectors have the same number of elements (length). If you try to operate on (e.g. add) two vectors of different lengths, R will return a warning and the result will not be what you expect:\n\nmy.vec1<-1:2\nmy.vec2<-0:4\nprint(my.vec1)\n\n[1] 1 2\n\nprint(my.vec2)\n\n[1] 0 1 2 3 4\n\nsum.vec<-my.vec1+my.vec2\n\nWarning in my.vec1 + my.vec2: longer object length is not a multiple of shorter\nobject length\n\nprint(sum.vec)\n\n[1] 1 3 3 5 5\n\n\n\n\n2.3.4 Exercises\nThe following R commands or short scripts contain errors; your job is to fix them so they runs as described.\n\nAssign a vector of three numbers to a variable:\n\n\ndate_num <- (3,8,16)\n\n\nAssign a range of values to a vector variable and print out the third one:\n\n\nthe.vals <- 0:10\nprint[the.vals(3)]\n\n\nAssign a range of values to a vector variable and print out the fourtieth and sixty-first values:\n\n\nall.the.vals <- 0:100\nprint(all.the.vals[40,61])\n\n\nTake the two vectors assigned above and assign their product to another vector:\n\n\nproduct <- the.vals*all.the.vals\n\n\nCreate a vector vec1 of ten integers and print the second and the eighth elements:\n\n\nvec1 <- 11:20 \nprint(vec1[2:8])\n\n\nCreate a vector vec1 and then multiply all of its elements by 20 and assign it to another vector:\n\n\nvec1<-seq(-3,2,0.1)\nvec2 <- 20vec1\n\n\nCreate a vector vec1, a vector vec2 and print out all the elements of the first divided by the second:\n\n\nvec1 <- 0:5\nvec2 <- 3:8\nprint[vec1/vec2]\n\n\n\n2.3.5 Plotting with vectors\n\ncurve(x^2,0,10,lwd=3,xlab='x', ylab='quadratic',cex.axis=1.5,cex.lab=1.5)\ncurve(20*exp(-0.5*x),0,5,lwd=3,xlab='x', ylab='exponential',cex.axis=1.5,cex.lab=1.5)\n\n\n\n\nTwo examples of plots using curve: quadratic (\\(y=x^2\\)) and exponential (\\(y=20*e^{-0.5x}\\))\n\n\n\n\n\n\n\nTwo examples of plots using curve: quadratic (\\(y=x^2\\)) and exponential (\\(y=20*e^{-0.5x}\\))\n\n\n\n\nThere are several ways of creating plots of mathematical functions or data R. If you want to plot a mathematical function, the simplest function is curve(). You can tell that this is a function, because it uses parentheses; the first input is an expression for the function, and the next two define the range of the independent variable over which to plot the graph. Two examples of plotting a quadratic function over the range from 0 and 5, and an exponential variation over the range of 0 to 10 are shown in figure \\(\\ref{fig:ch2-plot1}\\).\nOne can change the default look of the plot produced by curve by setting different options, which are optional inputs into the curve function, One is the line width lwd which can be increased from the default value of 1 to produce thicker curves, as demonstrated in the example above. One can add labels on the x and y axes with xlab and ylab options, respectively; note that these are strings of characters, and thus must be put in quotes to differentiate them from a variable name. There is one very important option not shown above: that of overlaying a curve on top of an existing plot, which is done by typing add=TRUE. This option takes logical (Boolean) values TRUE and FALSE, which must be typed in all caps and without quotes.\n\n2.3.5.1 plot() function\nIn addition to curve, one can use the function plot() in R to create two dimensional graphs from two vector-valued variables of the same length, e.g. plot(x,y). The first input variable corresponds to the independent variable (e.g. x), which is plotted on the x-axis, and the second variable corresponds to the dependent variable (e.g. y) which is plotted on the y-axis. In figure \\(\\ref{fig:ch2-plot2}\\) you see graphs of exponential and logistic function plotted using plot().\nThe following chunk creates a vector variable time, then calculates a new variable quad using time in a single operation:\n\ntime <- 0:10\nquad <- (time - 5)^2\nprint(time)\n\n [1]  0  1  2  3  4  5  6  7  8  9 10\n\nprint(quad)\n\n [1] 25 16  9  4  1  0  1  4  9 16 25\n\n\nThis chunk plots the two vector variables quad as a function of time, and adds a title to the plot\n\nplot(time,quad, main = 'Quadratic function of time')\n\n\n\n\nThe default plot style in R uses circles to indicate each plotted point. To change it, you need to set the option t (type), for example, setting t='l' (the lowercase letter L) produces a continuous line connecting the individual data points.\n\nplot(time,quad, main = 'Quadratic function of time', type = 'l', xlab='time', ylab = 'y = f(t)' )\n\n\n\n\nplot() is a versatile function that has many options function has many options which can be changed to determine the color, the style, and other attributes of the plot. For a full list type help(plot) in the console or type plot in the search bar of the Help pane in the bottom right window.\n\n\n2.3.5.2 using lines() or points()\nYou may also want to plot multiple graphs on the same figure. The plot() function creates a new plot window, so if you want to add another plot on top of the first one, you have to use another function. There are two ones available: lines() which produces continuous curves connecting the points, and points() which plots individual symbols at every point.\nLet us illustrate this by plotting two different exponential functions on one plot, and two different logistic functions on the second one, which were discussed in section \\(\\ref{sec:math2}\\). When you’ve got multiple plots on the same figure, they need to be distinct and labeled. To distinguish them, below I use the option col to specify the color of the plot, and I add a legend describing the parameters of each plot to the figure \\(\\ref{fig:ch2-plot2}\\). The function has a lot of options, so if you want to understand the details, type help(legend) in the prompt or go to Help tab in the lower right frame of R Studio and type legend.\n\nx<-seq(0,10,0.5)\ny<-10+20*exp(-0.5*x)\nplot(x,y, xlab='x', ylab='exponential',col=1,lwd=3)\ny<-10+20*exp(-2*x)\nlines(x,y,col=2,lwd=3)\nleg.txt=c(\"b=10,a=20,r=-0.5\", \"b=10,a=20,r=-2\")\nlegend(\"topright\", leg.txt, col=1:2, pch=c(1,NA), lty=c(0,1), lwd=3)\nx<-seq(-10,10,1)\ny<-20*exp(0.5*x)/(1+exp(0.5*x))\nplot(x,y,xlab='x',ylab='logistic',col=4,lwd=3)\ny<-20*exp(1.5*x)/(1+exp(1.5*x))\nlines(x,y,col=2,lwd=3)\nleg.txt=c(\"a=20,b=1,r=0.5\", \"a=20,b=1,r=1.5\")\nlegend(\"topleft\", leg.txt, col=c(4,2), pch=c(1,NA), lty=c(0,1), lwd=3)\n\n\n\n\nOverlaying multiple plots in R: two exponential functions of the form \\(y=b+ae^{rx}\\) on the left, two logistic functions of the form \\(y= ae^{rx}/(b+e^{rx})\\) on the right.\n\n\n\n\n\n\n\nOverlaying multiple plots in R: two exponential functions of the form \\(y=b+ae^{rx}\\) on the left, two logistic functions of the form \\(y= ae^{rx}/(b+e^{rx})\\) on the right.\n\n\n\n\n\n\n\n2.3.6 Exercises\nThe following R commands or short scripts contain errors; your job is to fix them so they runs as described.\n\nMultiply a vector by a constant and add another constant and assign the result to a vector:\n\n\nnew.vals <- 5 + 8the.vals\n\n\nAssign range to be a sequence of values from 0 to 100 with step of 0.1, and calculate the vector variable result as the square of the vector variable range:\n\n\nrange <- seq(0,0.1,100)\nresult <- square(range)\n\n\nPlot result as a function of range:\n\n\nplot(result, range)\n\n\nPlot the graph of the function \\(f(x) = (45-x)/(4x+3)\\) over the range of 0 to 100:\n\n\ncurve((45-x)/(4x+3), 0, 100)\n\n\nPlot a quadratic function with specified coefficients \\(a\\), \\(b\\), \\(c\\) over a given range of independent variable \\(x\\):\n\n\na<-10\nb<- -15\nc<- 5\ny<-a*x^2+b*x+c\nx<-seq(-0.5,2,0.01) \nplot(x,y,type='l')\n\n\nOverlay two different plots of the logistic function with different values of the parameter \\(r\\):\n\n\ntime<-0:100\na<-1000\nb<-50 \nr<-0.1\nPopulation<-a*exp(r*time)/(b+exp(r*time)) \nplot(time,Population,type='l') \nr<-10 \nlines(time,Population,col=2)"
  },
  {
    "objectID": "functions.html#rates-of-biochemical-reactions",
    "href": "functions.html#rates-of-biochemical-reactions",
    "title": "2  Functions and their graphs",
    "section": "2.4 Rates of biochemical reactions",
    "text": "2.4 Rates of biochemical reactions\n\nLiving things are dynamic, they change with time, and much of mathematical modeling in biology is interested in describing these changes. Some quantities change fast and others slowly, and every dynamic quantity has a rate of change, or rate for short. Usually, the quantity that we want to track over time is the variable, and in order to describe how it changes we introduce a rate parameter. If we are describing changes over time, all rate parameters have dimensions with time in the denominator. As a simple example, the velocity of a physical object describes the change in distance over time, so its dimension is \\([v] = length/time\\).\nOn the most fundamental level, the work of life is performed by molecules. The protein hemoglobin transports oxygen in the red blood cells, while neurotransmitter molecules like serotonin carry signals between neurons. Enzymes catalyze reactions, like those involved in oxidizing sugar and making ATP, the energy currency of life. Various molecules bind to DNA to turn genes on and off, while myosin proteins walk along actin fibers to create muscle contractions.\nIn order to describe the activity of biological molecules, we must measure and quantify them. However, they are so small and so numerous that it is not usually practical to count individual molecules (although with modern experimental techniques it is sometimes possible). Instead, biologists describe their numbers using concentrations. Concentration has dimensions of number of molecules per volume, and the units are typically molarity, or moles (\\(\\approx 6.022*10^{23}\\) molecules) per liter. Using concentrations to describe molecule rests on the assumption that there are many molecules and they are well-mixed, or homogeneously distributed throughout the volume of interest.\nMolecular reactions are essential for biology, whether they happen inside a bacterial cell or in the bloodstream of a human. Reaction kinetics refers to the description of the rates, or the speed, of chemical reactions. Different reactions occur with different rates, which may be dependent on the concentration of the reactant molecule. Consider a simple reaction of molecule \\(A\\) (called the substrate) turning into molecule \\(B\\) (called the product), which is usually written by chemists with an arrow: \\[\n  A \\xrightarrow{k} B\n\\] But how fast does the reaction take place? To write down a mathematical model, we need to define the quantities involved. First, we have the concentration of the molecule \\(A\\), with dimensions of concentration. Second, we have the rate of reaction, let us call it \\(v\\), which has dimension of concentration per time (just like velocity is length per time). How are the two quantities related?\n\n2.4.1 Constant (zeroth-order) kinetics\n In some circumstances, the reaction rate \\(v\\) does not depend on the concentration of the reactant molecule \\(A\\). In that case, the relationship between the rate constant \\(k\\) and the actual rate \\(v\\) is: \\[\\begin{equation}\nv = k\n\\label{eq:kinetics_0th_order}\n\\end{equation}\\]\nDimensional analysis insists that the dimension of \\(k\\) must be the dimension of \\(v\\), or concentration/time. This is known as constant, or zero-order kinetics, and it is observed at concentrations of \\(A\\) when the reaction is at its maximum velocity: for example, ethanol metabolism by ethanol dehydrogenase in human liver cannot proceed any faster than about 1 drink per hour.\n\n\n2.4.2 First-order kinetics\n. In other conditions, it is easy to imagine that increasing the concentration of the reactant \\(A\\) will speed up the rate of the reaction. A simple relationship of this type is linear: \\[\\begin{equation}\nv = kA\n\\label{eq:kinetics_1st_order}\n\\end{equation}\\]\nIn this case, the dimension of the rate constant \\(k\\) is 1/time. This is called first-order kinetics, and it usually describes reactions when the concentration of \\(A\\) is small, and there are plenty of free enzymes to catalyze more reactions.\n\n\n2.4.3 Michaelis-Menten model of enzyme kinetics \nHowever, if the concentration of the substrate molecule \\(A\\) is neither small nor large, we need to consider a more sophisticated model. An enzyme is a protein which catalyzes a biochemical reaction, and it works in two steps: first it binds the substrate, at which point it can still dissociate and float away, and then it actually catalyzes the reaction, which is usually practically irreversible (at least by this enzyme) and releases the product. The enzyme itself is not affected or spent, so it is free to catalyze more reactions. Let denote the substrate (reactant) molecule by \\(A\\), the product molecule by \\(B\\), the enzyme by \\(E\\), and the complex of substrate and enzyme \\(AE\\). The classic chemical scheme that describes these reactions is this: \\[\nA + E \\underset{k_{-1}}{\\overset{k_1}{\\rightleftharpoons}} AE  \\xrightarrow{k_2} E + B\n\\]\nYou could write three different kinetic equations for the three different arrows in that scheme. Michaelis and Menten used the simplifying assumptions that the binding and dissociation happens much faster than the catalytic reaction, and based on this they were able to write down an approximate, but extremely useful Michaelis-Menten model of an enzymatic reaction: \\[\\begin{equation}\nv = \\frac{v_{max} A}{K_M+A}\n\\label{eq:kinetics_MM_kinetics}\n\\end{equation}\\] Here \\(v\\) refers to the rate of the entire catalytic process, that is, the rate of production of \\(B\\), rather than any intermediate step. Here the reaction rate depends both on the concentration of the substrate \\(A\\) and on the two constants \\(v_{max}\\), called the maximum reaction rate, and the constant \\(K_M\\), called the Michaelis constant. They both depend on the rate constants of the reaction, and \\(v_{max}\\) also depends on the concentration of the enzyme. The details of the derivation are beyond us for now, but you will see in the following exercises how this model behaves for different values of \\(A\\)."
  },
  {
    "objectID": "descriptive.html#mutations-and-their-rates",
    "href": "descriptive.html#mutations-and-their-rates",
    "title": "3  Describing data sets",
    "section": "3.1 Mutations and their rates",
    "text": "3.1 Mutations and their rates\n\nAll Earth-based lifeforms receive an inheritance from their parent(s): a string of deoxyribonucleic acids ( DNA) called the genetic sequence, or genome of an individual. The information to produce all the necessary components to build and run the organism is encoded in the sequence of the four different nucleotides: adenine, thymine, guanine, and cytosine (abbreviated as A, T, G, C). Different parts of the genome play different roles; some discrete chunks called genes contain the instructions to build proteins, the workhorses of biology. To make a protein from a gene, the information is transcribed from DNA into messenger ribonucleic acid ( mRNA), which is then translated into a string of amino acids which constitute the protein. The genetic code determines the translation, using three nucleic acids in DNA and RNA to represent a single amino acid in a protein. Thus, a sequence of DNA results in a specific sequence of amino acids, which determine the structure and function of the protein.\n\n\n\nDifferent types of substitution point mutations are distinguished by their effects on the gene products; image by Jonsta247 in public domain via Wikimedia Commons.\n\n\nThe above processes involve copying and transferring information. As we know from experience, copying information inevitably means introducing errors. This is particularly important when passing information from parent to offspring, because then an entire organism has to develop and live based on a faulty blueprint. Changes introduced in the genome of an organism are called mutations, and they can be caused either by errors in copying DNA when making a new cell (replication) or through damage to DNA through physical means (e.g. ionizing radiation) or chemical mechanisms (e.g. exogenous molecules that react with DNA). The simplest mutation involve a single nucleotide and are called point mutations. A nucleotide may be deleted, an extra nucleotide inserted, or a new one substituted instead: the three different types of substitution mutations are shown in figure \\(\\ref{fig:ch3_mutation}\\). Large-scale mutations may involve whole chunks of the genome that are cut out and pasted in a different location, or copied and inserted in another position, but they are typically much more rare than point mutations.\nMutations can have different effects on the mutant organism, although acquisition of super-powers has not been observed. Usually, point mutations have either little observable effect or a negative effect on the health of the mutant. A classic example is sickle-cell disease, in which the molecules of the protein hemoglobin, responsible for carrying oxygen in the blood from the lungs to the tissues, tends to stick together and clump, resulting in sickle-shaped red blood cells. The disease is caused by a single substitution mutation in the gene that codes for one of the two components of hemoglobin, called \\(\\beta\\)-globin. The substitution of a single nucleotide in the DNA sequence changes one amino acid in the protein from glutamate to valine, which causes the proteins to aggregate. This missense}* mutation (see figure \\(\\ref{fig:ch3_mutation}\\)) is carried by a fraction of the human population, and those who inherit the allele allele from both parents develop the painful and sometimes deadly disease. Such mutations that are present in some but not all of a population are called polymorphisms, to distinguish them from mutations that occurred in evolutionary lineages and differentiate species from each other.\nOne of the central questions of evolutionary biology is how frequently do mutations occur? Since mutations are generally undesirable, most living things have developed ways to minimize the frequency of errors in copying DNA, and to repair DNA damage. But although mutations are rare, they occur spontaneously in all organisms because molecular processes such as copying a DNA molecule are subject to random noise arising from thermal motion. So mutations are fundamentally a random process and we need to use descriptive statistics to analyze data with inherent randomness."
  },
  {
    "objectID": "descriptive.html#describing-data-sets",
    "href": "descriptive.html#describing-data-sets",
    "title": "3  Describing data sets",
    "section": "3.2 Describing data sets",
    "text": "3.2 Describing data sets\n\n\n3.2.1 central value of a data set\nA data set is a collection of measurements. These measurements can come from many kinds of sources, and can represent all sorts of quantities. One big distinction is between numerical and categorical data sets. Numerical data sets contain numbers, either integers or real numbers. Some examples: number of individuals in a population, length, blood pressure, concentration. Categorical data sets may contain numbers, symbols, or words, limited to a discrete, usually small, number of values. The word categorical is used because this kind of data corresponds to categories or states of the subject of the experiment. Some examples: genomic classification of an individual on the basis of one locus (e.g. wild type or mutant), the state of an ion channel (open or closed), the stage of a cell in the cell cycle.\nA data set contains more than one measurement, the number of them is called the size of the data set and is usually denoted by the letter \\(n\\). To describe a data set numerically, one can use numbers called statistics (not to be confused with the branch of science of the same name). The most common statistics aim to describe the central value of the data set to represent a typical measurement. If you order all of the measurements from highest to lowest and then take the the middle value, you have found the median (if there is an even number of values, take the average between the middle two). Precisely half of the data values are less than the median and the other half are greater, so it represents the true “middle” value of the measurement. Note that the median can be calculated either for numerical or categorical data, as long as the categories can be ordered in some fashion.\nThe value that occurs most frequently in the data set is called its . For some data sets, particularly those which are symmetric, the mode coincides with the mean (see next paragraph) and the median, but for many others it is distinct. The mode is the most visual of the three statistics, as it can be picked out from the histogram plot of a data set (which is described in subsection 3.2.3) as the value corresponding to the maximum frequency. The mode can also be used for both categorical and numerical data.\nThe average or mean of a data set is the sum of all the values divided by the number of values. It is also called the expected value (particularly in the context of probability, which we will discuss later) because it allows to simply predict the sum of a large number of measurements with a given mean, by multiplying the mean by the number. The mean can be calculated only for a numerical data set, since we cannot add non-numerical values.\n\n\n\n\n\n\nDefinition\n\n\n\nThe mean of a data set \\(X\\), also known as the average or the arithmetic mean is usually indicated with a bar over the variable symbol, and defined as the sum of the values divided by the number of values:\n\n\n\\[\\begin{equation}\n  \\bar X  = \\frac{1}{n} \\sum_{i=1}^n x_i\n\\label{eq:ch3_mean_def}\n\\end{equation}\\]\nThe mean, unlike the median, is not the middle value of the data set, instead it represents the center of mass of the measured values . Another way of thinking of the mean is as a weighted sum of the values in the data set. The weights represent the frequency of occurrence of each numeric value in the data set, which we will further discuss in subsection 3.2.3.\nThe mean is the most frequently used statistic, but it is not always interpreted correctly. Very commonly the mean is reported as the most representative value of a data set, but that is often misleading. Here are at least two situations in which the mean can be tricky: 1) data sets with a small number of discrete values; 2) data sets with outliers, or isolated numbers very far from the mean.\nExamples of misleading means. Mean quantities for data sets with a few quantities are not the typical value, such as in the number of children born in a year per individual, also known as the birth rate. The birth rate per year in 2013 for both the United States and Russia is 1.3% per person, but you will have to look for a long time to find any individual who gave birth to 1.3% of a child. While this point may be obvious, it is often overlooked when interpreting mean values.\nOutliers are another source of trouble for means. For example, a single individual (let’s call him or her B.G.) with a wealth of $50 billion moves into a town of 1000 households with average wealth of $100,000. Although none of the original residents’ assets have changed, the mean wealth of the town improves dramatically, as you can calculate in one of the exercises at the end of the chapter. One can site the improved per capita (per individual) in the town as evidence of economic growth, but that is obviously misleading. In cases with such dramatic outliers, the median is more informative as representation of a typical value of the data set.\n\n\n3.2.2 Exercises\nFor the (small) data sets given below, calculate the mean and the median (by hand or using a calculator) and compare the two measures of the center.\n\nData set of the population of the city of Chicago (in millions) in the last 4 census years (2010, 2000, 1990, 1980): {2.7, 2.9, 2.8, 3.0}.\nData set of the numbers of the fish blacknose dace (Rhinichthys atratulus) collected in 6 different streams in the Rock Creek watershed in Maryland: {76, 102, 12, 55, 93, 98}.\nData set of tuberculosis incidence rates (per 100,000 people) in the 5 largest metropolitan areas in the US in 2012: {5.2, 6.6, 3.2, 5.5, 4.5}.\nData set of ages of mothers at birth for five individuals: {19, 20, 22, 32, 39}.\nData set of ages of fathers at birth for five individuals: {22, 23, 25, 36, 40}.\nData set of the number of new mutations found on maternal chromosomes for five individuals: {9, 10, 11, 26, 15}.\nData set of the number of new mutations found on paternal chromosomes for five individuals: {39, 43, 51, 53, 91}.\nConsider the hypothetical town with 1000 households with mean and median wealth of $100,000 and one person with assets for $50 billion. Calculate the mean value of the combined data set, and compare it to the new median value.\nSuppose you’d like to add a new observation to a data set; e.g. the 6-th largest metropolitan area (Philadelphia) to the tuberculosis incidence data set, which is 3.0. Calculate the mean of the 6-values data set, without using the 5 values in the original data set, but only using the mean of the 5-value data set and the new value. Generalize this to calculating the sample mean for any \\(n\\)-value data set, given the mean of the \\(n-1\\) values, plus one new value.\n\n\n\n3.2.3 spread of a data set\nThe center of a data set is obviously important, but so is the spread around the center. Sometimes the spread is caused by noise or error, for example in a data set of repeated measurements of the same variable under the same conditions. Other times the variance is due to real changes in the system, or due to inherent randomness of the system, and the size of the spread, as well as the shape of the histogram are important for understanding the mechanism. The simplest way to describe the spread of a numerical data set is to look at the difference between the maximum and minimum values, called the range. However, it is obviously influenced by outliers, since the extreme values are used. To describe the typical spread, we need to use all the values in the data set, and see how far each one is from the center, measured by the mean.\nThere is a problem with the naive approach: if we just add up all the differences of data values from the mean, the positives will cancel the negatives, and we’ll get an artificially low spread. One way to correct this is to take the absolute value of the differences before adding them up. However, for somewhat deep mathematical reasons, the standard measure of spread uses not absolute values, but squares of the differences, and then divides that sum not by the number of data points \\(n\\) but by \\(n-1\\).\n\n\n\n\n\n\nDefinition\n\n\n\nThe variance of a data set \\(X\\) with \\(n\\) values is the sum of the squared differences of each value of the variable from the mean, divided by \\(n-1\\):\n\n\n\\[\\begin{equation}\nVar(X) = \\frac{1}{n-1} \\sum_{i=1}^n (\\bar X - x_i)^2\n\\end{equation}\\]\nThe variance is a sum of square differences, so its dimension is the square of the dimensions of the measurements in \\(X\\). In order to obtain a measure of the spread comparable to the values of \\(X\\), we take the square root of variance and call it the standard deviation of the data set \\(X\\):\n\\[\\begin{equation}\n\\sigma(X) = \\sqrt{\\frac{1}{n-1} \\sum_{i=1}^n (\\bar X - x_i)^2}\n\\end{equation}\\]\nJust as the mean is a weighted average of all of the values in the data set, the variance is a weighted average of all the squared deviations of the data from the mean.\n\n\n3.2.4 Exercises:\nFor the (small) data sets below, calculate the range, variance, and standard deviation (by hand or using a calculator). Compare the range and the standard deviation for each case: which one is larger? by how much?\n\nData set of the population of the city of Chicago (in millions) in the last 4 census years (2010, 2000, 1990, 1980): {2.7, 2.9, 2.8, 3.0}.\nData set of the numbers of the fish blacknose dace (Rhinichthys atratulus) collected in 6 different streams in the Rock Creek watershed in Maryland: {76, 102, 12, 55, 93, 98}.\nData set of tuberculosis incidence rates (per 100,000 people) in the 5 largest metropolitan areas in the US in 2012: {5.2, 6.6, 3.2, 5.5, 4.5}.\nData set of ages of mothers at birth for five individuals: {19, 20, 22, 32, 39}.\nData set of ages of fathers at birth for five individuals: {22, 23, 25, 36, 40}.\nData set of the number of new mutations found on maternal chromosomes for five individuals: {9, 10, 11, 26, 15}.\nData set of the number of new mutations found on paternal chromosomes for five individuals: {39, 43, 51, 53, 91}.\nConsider the hypothetical town with 1000 households with mean and median wealth of $100,000 and one person with assets for $50 billion. Calculate the mean value of the combined data set, and compare it to the new median value.\n(harder) Suppose that a data set has a fixed range (e.g. all values have to lie between 0 and 1). What is the greatest possible standard deviation for any data set within the range? Hint: think about how to place the points as far from the mean as possible. How do the data sets above relate to your prediction?}\n\n\n\n3.2.5 describing data sets in graphs\nData sets can be presented visually to indicate the frequency of different values. This can be done in a number of ways, depending on the kind of data set. For a data set with only a few values, e.g. a categorical data set, a good way to represent it is with a pie chart. Each category is represented by a slice of the pie with the area of the same share of the pie as the fraction of the data set in the category. There is some evidence, however, that pie charts can be misleading to the eye, so R does not recommend using them.\nFor a numerical data set it is useful to plot the frequencies of a range of values, which is called a histogram. Its independent axis has the values of the data variable, and the dependent axis has the frequency of those values. If the data set consists of real numbers that range across an interval, that interval is divided into subintervals (usually of equal size), called bins, and the number of measurements in each bin is indicated on the y-axis. In order to be visually informative, there should be a reasonable number (usually no more than a few dozen, although it varies) of bins. The most frequent measurements are represented as the highest bars or points on the histogram. Histograms can denote either the counts of measurements in each bin, or to show the fraction of the total number of measurements in each bin. The only difference between those two kinds of histogram is the scale of the y-axis, and, confusingly, both can be called frequencies.\n\n\n\n\n\nFigure 3.1: Length of bacteria Bacillus subtilis measured under the microscope as discrete values with step of 0.5; data from citep{watkins-intro-stats}\n\n\n\n\nA histogram of the measured lengths of the bacterium Bacillus subtilis is shown in figure \\(\\ref{fig:ch3_bacillus}\\). The data set was measured in increments in half a micron, with numbers varying between 1.5 and 4.5 microns. The histogram shows that the most common measurement (the mode) is 2 \\(\\mu m\\). Adding up all of the frequencies in the histogram tells us that there are approximately 200 total values in the data set. This allows us to find the median value by counting the frequencies of the first few bins until we get to 100 (the median point), which resides in the bin for 2.5 \\(\\mu m\\). It is a little bit more difficult to estimate the mean, but it should be clear that the center of mass of the histogram is also near 2.5 (it is actually 2.49). Finally, the hardest task is estimating the spread of the data set, such as the the standard deviation, based on the histogram. The range of the data set is \\(4.5-1.5 = 3\\), so we know for sure that it is less than 1.5. The histogram shows that the deviations from the mean value of 2.5 range from 2 (rarely) to 0.5 (most prevalent). This should give you an idea that the weighted average of the deviations is less than 1. Indeed, the correct standard deviation is about 0.67.\nThere are different ways of plotting data sets that have more than one variable. For instance, a data set measured over time is called a time series. If the values are plotted with the corresponding times on the x-axis, then it is called a time plot. This is useful to show the changes of the values of your variable over time. If the data set doesn’t undergo any significant changes over time, it makes more sense to represent it as a pie chart or histogram. More generally, one may plot two variables measured together on a single plot, which is called a scatterplot. We will explore such plots and the relationships between two measured variables in chapter 4.\n\n\n3.2.6 Exercises\nAnswer the following questions, based on the histograms in figure \\(\\ref{fig:ch3_mut}\\) (mutation data) and in figure \\(\\ref{fig:ch3_HR}\\) (heart rate data).\n\nHow many people in the mutation data have fathers either younger than 20 or older than 40? How many have more than 80 new mutations? \nEstimate the median and mean of the two variables in the mutation data set.\nState the range of each data set, and estimate the standard deviation of the two variables in the mutation data set.\nHow many people in the heart rate data have heart rates greater than 80 bpm? How many have body temperature less that 97 F?\nEstimate the median and mean of the two variables in the heart rate data set.\nState the range of each data set, and estimate the standard deviation of the two variables in the heart rate data set.\n\n\nmy_data<-read.table('data/HR_temp.txt', header=TRUE)\nhist(my_data$HR,col='gray',main='Heart rate data', xlab='heart rate (bpm)')\nhist(my_data$Temp,col='gray',main='Body temperature data', xlab= 'temperature (F)')\n\n\n\n\nFigure 3.2: Histograms of heart rates and body temperatures\n\n\n\n\n\n\n\nFigure 3.3: Histograms of heart rates and body temperatures\n\n\n\n\n\n\n\n\n\nFigure 3.4: Histograms of paternal ages and the number of new mutations from 73 families; data from citep{kong_rate_2012}\n\n\n\n\n\n\n\nFigure 3.5: Histograms of paternal ages and the number of new mutations from 73 families; data from citep{kong_rate_2012}"
  },
  {
    "objectID": "descriptive.html#working-with-data-in-r",
    "href": "descriptive.html#working-with-data-in-r",
    "title": "3  Describing data sets",
    "section": "3.3 Working with data in R",
    "text": "3.3 Working with data in R\n\n\n3.3.1 reading in data into data frames\nOne way to input data into R is to read in a text file, where several variables are stored in columns. For instance, the file HR_temp.txt contains three variables: body temperature (in Fahrenheit), sex (1 for male, 2 for female), and heart rate (in beats per minute). The values for the variables are arranged in columns, while first row of the file contains the names of the variables (Temp, Sex, and HR, respectively). Note that the data file has to be saved into the same folder as the .Rmd file week1.Rmd for this to work.\n\nvitals <- read.table(file = \"data/HR_temp.txt\", header = TRUE)\nplot(vitals$HR, vitals$Temp, main = 'Body temp as function of heart rate', xlab= 'heart rate (bpm)', ylab= 'body temperature (F)')\nmTemp <- mean(vitals$Temp)\nsdTemp <- sd(vitals$Temp)\nabline(mTemp,0)\n\n\n\nmean(vitals$HR)\n\n[1] 73.76154\n\nsd(vitals$HR)\n\n[1] 7.062077\n\n\nThe R command read.table() reads this file and and puts it into a data frame called data. The three variables are stored inside the data frame, and can be accessed by appending the dollar sign and variable name to the data frame, so data$HR refers to only the heart rates, and data$Temp refers to the body temperatures. The plot shows the relationship of the two data variables, and the function abline(98.6,0) plots a line with the intercept 98.6a and slope 0 on top of the scatterplot.\nYou can also load data from a package, e.g. HistData, which contains many classic data sets. Got to the Packages tab in the lower right window in R Studio, click Install and type HistData. We will use the data set called Galton that contains the heights of parents (the mean of mother’s and father’s) and their children, in variables parent and child. The script below plots the two variables, with parent as the independent (explanatory) variable and child as the dependent (response) variable.\n\nlibrary(HistData)\nplot(Galton$parent, Galton$child, main = 'Height of child vs average height of parents', xlab= 'parent height (inches)', ylab= 'child height (inches)')\n\n\n\n\n\n\n\nsummary(Galton$parent)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  64.00   67.50   68.50   68.31   69.50   73.00 \n\nsummary(Galton$child)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  61.70   66.20   68.20   68.09   70.20   73.70 \n\n\n\n\n3.3.2 descriptive statistics\nOne can also calculate basic descriptive statistics as follows:\n\npaste(\"The mean parental height is:\", mean(Galton$parent))\n\n[1] \"The mean parental height is: 68.3081896551724\"\n\npaste(\"The mean child height is:\", mean(Galton$child))\n\n[1] \"The mean child height is: 68.0884698275862\"\n\npaste(\"The standard deviation of parental height is:\", sd(Galton$parent))\n\n[1] \"The standard deviation of parental height is: 1.78733340172202\"\n\npaste(\"The standard deviation of  child height is:\", sd(Galton$child))\n\n[1] \"The standard deviation of  child height is: 2.51794136627677\"\n\n\nWhy do you think the standard deviation of parental height is much smaller?\nR has histogram function hist(), which does a passable job of representing the distribution of a variable such as child height or parent height. Compare the width of the two distributions and consider why they are different.\n\nhist(Galton$child)\nhist(Galton$parent)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.3.3 Exercises:\nThe following code chunks contain errors. Find and fix them so they work as intended.\n\nCalculate the mean and standard deviation of the heart rates of the first 30 individuals in the data frame vitals:\n\n\nmean(vitals$HR[30])\n\n[1] 64\n\nsd(vitals$HR[30])\n\n[1] NA\n\n\n\nCalculate the mean and standard deviation of the ratio of heart rates to body temperatures for the data set vitals:\n\n\nmean(vitals$HR/Temp)\nsd(vitals$HR/Temp)\n\n\nPlot a scatterplot of the child heights as the response variable and the parent heights and the explanatory variable, and overlay the line y=x on top.\n\n\nplot(parent, child, main = 'Height of child vs average height of parents', xlab= 'child height (inches)', ylab= 'parent height (inches)')\nabline(1,0)\n\n\nCalculate the median of both parent and child heights:\n\n\nmedian(Galton)\n\n\nPlot the histogram of parent heights of the first half of the group:\n\n\nhist(Galton$parent/2)\n\n\nPlot the histogram for the ratio of parent and child heights for the entire data set and calculate its mean and variance:\n\n\nhist(Galton$parent/child)\nmean(Galton$parent/child)\nsd(Galton$parent/child)"
  },
  {
    "objectID": "probdist.html#random-variables-and-distributions",
    "href": "probdist.html#random-variables-and-distributions",
    "title": "4  Random variables and distributions",
    "section": "4.1 Random variables and distributions",
    "text": "4.1 Random variables and distributions\n\n\n4.1.1 definition of probability\nIn this section we will develop the terminology used in the mathematical study of randomness called probability. This begins with a random experiment which is a very broad term that can describe any natural or theoretical process whose outcome cannot be predicted with certainty. If the outcomes are numeric, they may be discrete (can be counted by integers) or continuous (corresponding to real numbers); they may also be categorical, meaning that they do not have a numeric meaning, like eye color. We will stick to experiments that have discrete outcomes in this chapter, but many important experiments produce continuous outcomes. The first step for studying a random process is to describe all of the outcomes it can produce:\n\n\n\n\n\n\nDefinition\n\n\n\nThe collection of all possible outcomes of an experiment is called its sample space \\(\\Omega\\). An event is a subset of the sample space, which means an event may contain one or more experimental outcomes.\n\n\n\n\n\nAn illustration of the sample space of all people with two events: tall people and those who like tea.\n\n\nExample. You can ask a person two questions: how tall are you (and classify them either as short or tall) and do you like tea (yes or no), and you’ve performed a random experiment. The randomness comes not from the answers (assuming the person doesn’t randomly lie) but from the selection of the respondent. We will discuss randomly selecting a sample from a population in the next chapter. This random experiment has four outcomes: tall person who likes tea, tall person who does not like tea, short person who likes tea, and short person who does not like tea. This sample space and events is illustrated in figure \\(\\ref{fig:ch4_sample_space}\\) with a Venn diagram, which uses geometric shapes as representations of events as subsets of the entire sample space. These outcomes can be grouped into events by one of the responses: e.g. tall person (\\(A\\)) or person who doesn’t like tea (\\(-B\\)).\nExample. A random experiment with two outcomes, called a Bernoulli trial (after the famous Swiss mathematician), can describe a variety of situations: a coin toss (heads or tails), a competition with two outcomes (win or loss), the allele of a gene (normal or mutant). The sample space for a single Bernoulli trial consists of just two outcomes: \\(\\{H,T\\}\\) (for a coin toss). If the experiment is performed repeatedly, the sample space gets more complicated. For two Bernoulli trials there are four different outcomes \\(\\{HH, HT, TH, TT \\}\\). One can define different events for this sample space: the event of getting two heads in two tosses contains one outcome: \\(\\{HH\\}\\), the event of getting a single head contains two: \\(\\{TH, HT\\}\\).\nIn order to to describe the composition of a sample space, we need to define the word probability . While it is familiar to everyone from everyday usage, it is difficult to define without using other similar words, such as likelihood or plausibility, which are also in need of definition. It is accepted that something with a high probability happens often, while something with a low frequency is seldom observed. The other notion is that probability can range between 0 (meaning something that never occurs) and 1 (something that occurs every time). These notions lead to the commonly accepted definition:\n\n\n\n\n\n\nDefinition\n\n\n\nThe probability of an outcome or event in the sample space of a random experiment is the fraction of experiments with this outcome out of many repeated experiments.\n\n\nThis definition is at the heart of the frequentist view of probability, due to the underlying assumption that the experiment can be repeated as many times as necessary to observe the frequency of outcomes. There is an alternative view that focuses on what is previously known about the experiment (or about systems that produce that kind of experiment) that is called the Bayesian view:\n\n\n\n\n\n\nDefinition\n\n\n\nThe probability of an outcome or event in the sample space of a random experiment is the degree of certainty or belief that this outcome will occur based on prior experience.\n\n\nWe will investigate the Bayesian approach in chapter 12. Most of traditional probability and classical statistics is based on the frequentist view, as it grew out of attempts to understand games of chance, like cards and dice, which can be easily repeated, or simple experiments like those in agriculture, where many plots can be planted and observed. These easily repeatable simple experiments can be described with mathematical distributions that we will describe in this chapter. However, many contemporary research problems are not so easily repeated, and often require a Bayesian approach that does not yield to neat mathematical description and can be addressed using computation.\n\n\n4.1.2 axioms of probability\nOne we have defined the probability of an outcome, one can calculate the probability of a collection of outcomes according to rules that ensure the results are self-consistent. These rules are called the axioms of probability:\n\n\n\n\n\n\nDefinition\n\n\n\nThe probability \\(P(A)\\) of an event \\(A\\) in a sample space \\(\\Omega\\) is a number between 0 and 1, which obeys the following rules, called the axioms of probability:\n\n\\(P(\\Omega) = 1\\)\n\\(P(\\emptyset) = 0\\)\n\\(P(A \\cup B) = P(A) + P(B) - P(A \\cap B)\\)\n\n\n\nLet us define some notation for sets: \\(A \\cup B\\) is called the union of two sets, which contains all outcomes that belong to either \\(A\\) or \\(B\\), this is equivalent to the logical OR operator because it is true if either A or B is true. \\(A\\cap B\\) is called the intersection of two sets, which contains all outcomes that are in both \\(A\\) and \\(B\\), this is is equivalent to the logical AND operator because it is true if both A and B are true. The \\(\\emptyset\\) denotes the empty set. Any event \\(A\\) has its complement, denoted \\(-A\\), which contains all outcomes of \\(\\Omega\\) which are not in \\(A\\).\nApplying them to the sample space and events in figure \\(\\ref{fig:ch4_sample_space}\\), the union of the two sets \\(A \\cup B\\) are all people who are either tall or like tea, the intersection of the two sets \\(A\\cap B\\) are all the tall people who like tea, and the intersection of the first set with the complement of the second \\(A \\cup - B\\) are all tall people who do not like tea.\n\n\n\nAn illustration of the operation of intersection of sets A and B.\n\n\n\n\n\nAn illustration of the operation of the union of sets A and B.\n\n\n\n\n\nAn illustration of the intersection of A with -B\n\n\nThe first two axioms connect easily with our intuition about probability: the first axiom says that the probability of some outcome from the sample space occurring is 1, while the second says that the probability of nothing in the sample space occurring is 0. The intuition behind axiom three is less transparent, but it can be see in a Venn diagram of two subsets \\(A\\) and \\(B\\) of the larger set \\(\\Omega\\), as in figure \\(\\ref{fig:ch4_sample_space}\\). Compare the size of the union of \\(A\\) and \\(B\\) and the sum of the sizes of sets \\(A\\) and \\(B\\) separately, and you will see that the intersection \\(A\\cap B\\) occurs in both \\(A\\) and \\(B\\), but is only counted once in the union. This is why it needs to be subtracted from the sum of \\(P(A)\\) and \\(P(B)\\).\nThere are several useful rules that immediately follow from the axioms. First, if two events are mutually exclusive, meaning their intersection is empty (\\(A\\cap B = \\emptyset\\)), then the probability of either of them happening is the sum of their respective probabilities: \\(P(A \\cup B) = P(A) + P(B)\\) (from axiom 3). Further, since an event \\(A\\) and its complement \\(-A\\) are mutually exclusive, their union is the entire sample space \\(\\Omega\\): \\(P(A) + P(-A) = P(A \\cup -A) = P(\\Omega) = 1\\), therefore \\(P(A) = 1-P(-A)\\).\nExample. Assume one is using a fair coin, so the probability of a single head and a single tail is 1/2. The probability of getting two heads in a row is 1/4, because exactly half of those coins that come up heads once will come up heads again. In fact, the probability of getting any particular sequence of two coin toss results is 1/4. Here are some examples of what we can calculate:\n\nthe probability of getting one head of out of two tosses is \\(1-1/4-1/4=1/2\\) (by the complement rule).\nthe probability of getting two heads is \\(1-1/4 = 3/4\\) (by the complement rule).\n\nthe probability of getting either 0, 1, or 2 heads is 1 (by axiom 1).\nthe probability of getting three heads is 0 (since this event is not in the sample space).\n\nExample. Suppose one is testing people for a mutation which has the probability (prevalence) of 0.2 in the population, so for each person there are two possible outcomes: normal or mutant. The probability of drawing two mutants in a row is \\(0.2*0.2=0.04\\) by the same argument as above; the probability of drawing two normal people is \\(0.8*0.8 =0.64\\). Based on this, we can calculate the following\n\nthe probability of one mutant of out two people is \\(1-0.04-0.64=0.32\\) (by the complement rule).\nthe probability of not having two mutants is \\(1-0.04 = 0.96\\) (by the complement rule).\n\nthe probability of either 0, 1, or 2 mutants is 1 (by axiom 1).\nthe probability of getting three mutants is 0 (since this event is not in the sample space).\n\nExample (from Danny and Gaines Sarcastic fringeheads are a tropical ocea fish that engage in aggressive mouth-wrestling matches for their rocky residences. Let us treat each match as a stochastic experiment with two outcomes: win or loss. Then the sample space is equivalent to our coin-tossing experiment, e.g. for two matches the sample space is \\(\\{ WW, WL, LW, LL \\}\\). However, the probability distribution may different, for example if a particular fringehead wins 3/4 of its matches, then the probability distribution would be: \\(P(\\{ WW \\}) = 9/16\\), \\(P(\\{ LW \\}) = P(\\{ WL \\}) = 3/16\\), and $ P({ LL }) = 1/16$. Thus, the same sample space may have different probability distributions defined on it.\n\n\n4.1.3 random variables\nThe outcomes of experiments may be expressed in numbers or words, but we generally need numbers in order to report and analyze results. One can describe this mathematically as a function (recall its definition form section \\(\\ref{sec:math2}\\)) that assigns numbers to random outcomes . In practice, a random variable describes the measurement that one makes to describe the outcomes of a random experiment.\n\n\n\n\n\n\nDefinition\n\n\n\nA random variable is a number or category associated to each outcome in a sample space \\(\\Omega\\). This association has to follow the rules of a function as defined in chapter 2.\n\n\nExample. Define the random variable to be the number of heads out of two coin tosses. This random variable will return numbers 0, 1, or 2, corresponding to different events. The random variable of the number of mutants out of two people (assuming there are only two outcomes, mutant and normal) has the same set of values. This random variable is a function on the sample space because it returns a unique value for each outcome.\nExample. (Danny and Gaines) Suppose that our sarcastic fringehead, upon losing a wrestling match, has to search for another home for three hours. Then we can define the random variable of time wasted over two wrestling matches, which can be either 0, 3, or 6 hours, depending on the events defined above. Once again, this is a function because there is an unambiguous number associated with each outcome.\nA random variable has a set of possible values, and each of those values may come up more or less frequently in an random experiment. The frequency of each measurement corresponds to the probability of the outcomes in the sample space that produce that particular value of the random variable. One can describe the behavior of the random variable in terms of the collection of the probabilities of its outcomes.\n\n\n\n\n\n\nDefinition\n\n\n\nThe probability of a random variable \\(X\\) taking some value \\(a\\), written as \\(P(X=a)\\), but usually simplified to \\(P(a)\\) is the probability of the event corresponding to the value \\(a\\) of the random variable. This function \\(P(a)\\) is called the probability distribution of the random variable \\(X\\).\n\n\nOne important property of probability distribution functions for a discrete random variable is that all of its values have to add up to 1:\n\\[\\sum_{i=1}^N P(a_i) =1\\] The graph of a probability distribution function lies above zero because all probabilities are between 0 and 1. The graph of a probability distribution is very similar to a histogram, in that it represents the frequency of occurrence of each value of the random variable. A histogram of a variable from a data set can be thought is an approximation of the true probability distribution based on the sample. For a large sample size, the histogram approaches the graph of the probability distribution function, something which we will discuss in chapter 9.\nExample. Assuming that each coin toss has probability 1/2 of resulting in heads, the probability distribution function for the number of heads out of two coin tosses is \\(P(0) = 1/4; \\; P(1) = 1/2; \\; P(2) = 1/4\\) (as we computed in the example in the previous section). Note that the probabilities add up to 1, as they should.\nExample. For the random variable of the number of mutants out of two people, for mutation prevalence of 0.2, the probability distribution function is \\(P(0) = 0.64; \\; P(1) = 0.32; \\; P(2) = 0.04\\) (as we computed in the example in the previous section). Note that the probabilities add up to 1, as they should.\nExample. For the time wasted by a fringehead, the distribution is \\(P(0)= 9/16; \\; P(3) = 3/16; \\; P(6) = 1/16\\). Note that other values of the random variable have probability 0, because they correspond to the empty set in sample space.\n\n\n4.1.4 expectation of random variables\n\n\n\n\n\n\nDefinition\n\n\n\nThe expected value (or mean) of a discrete random variable \\(X\\) with probability distribution \\(P(X)\\) is defined as: \\[ E(X) = \\mu_X = \\sum_{i=1}^N  a_i P(a_i)\\]\n\n\nThis sum is over all values \\(\\{a_i\\}\\) that the random variable \\(X\\) can take, multiplied by the probability of the random variable taking that value (meaning the probability of the event in sample space that corresponds to that value). This corresponds to the definition of the mean of a data set given in section \\(\\ref{sec:math3}\\), if you consider \\(P(a_i)\\) to be the number of times \\(a_i\\) occurs divided by the number of total measurements \\(N\\). As in the case of the histogram and the distribution function, the mean of a sample for a large sample size \\(N\\) approaches the mean of the random variable, which we will discuss in more detail in the next chapter. Sometimes we will use the more concise \\(\\mu_X = E(X)\\) to represent the mean (expected) value. Here are some mathematical properties of the expectation:\n\nExpectation of a random variable which is always constant (\\(c\\)) is equal to \\(c\\), since the probability of \\(c\\) is 1: \\(E(c) = cP(c) = c\\)\nExpectation of a constant multiple of a random variable is:\n\n\\[E(cX) = \\sum_i c x_iP(x_i) = c \\sum_i x_iP(x_i) = c \\mu_X\\]\n\nExpectation of a sum of two random variables is the sum of their expectations. This is a more complicated argument, so let us break it down. First, all possible values of the random variable \\(X+Y\\) come from going through the possible values of \\(X\\) (\\(a_i\\)) and \\(Y\\) (\\(b_i\\)), and each combination of values has its own probability (called the joint probability distribution) \\(P(a_i, b_j)\\):\n\n\\[E(X+Y) = \\sum_i \\sum_j (a_i+b_j) P(a_i, b_j)\\] We can split the sum into two terms by the distributive property of multiplication and then take out the values \\(a_i\\) and \\(b_j\\) out of the sum that they do not depend on:\n\\[E(X+Y) = \\sum_i \\sum_j a_i P(a_i, b_j) + \\sum_i \\sum_j b_j P(a_i, b_j)=\\] \\[=\\sum_i a_i  \\sum_j  P(a_i, b_j) +  \\sum_j b_j \\sum_i P(a_i, b_j) \\] The joint distributions added up over all values of one variable, become single-variable distributions, so this leaves us with two sums which are the two separate expected values:\n\\[E(X+Y) =  \\sum_i a_i P(a_i) +  \\sum_j b_j P(b_j) = E(X) + E(Y) \\]\nExample. The expected value of the number of heads out of two coin tosses can be calculated using the probability distribution function we found above: \\[ E(X) = 0\\times P(0) + 1 \\times P(1) + 2 \\times P(2) = 0+1/2+2 \\times 1/4 = 1\\] The expected number of heads out of 2 is 1, if each head comes up with probability 1/2, which I think you will find intuitive.\nExample. The expected value of the number of mutants out of two people can be calculated using the probability distribution function we found above: \\[ E(X) = 0 \\times P(0) + 1 \\times P(1) + 2 \\times P(2) = 0+1 \\times 0.32+2 \\times 0.04 = 0.4\\] The expected number of mutants in a sample of two people is 0.4, which may seem a bit strange. Recall that mean or expected values do not have to coincide with values that are possible, as we discussed in section \\(\\ref{sec:math3}\\), but are instead a weighted average of values, according to their frequencies or probabilities.\nExample. Find the expected value of the number of wins out of two matches for a fringehead which has the probability of winning of 3/4.\n\\[E(X) = 0 \\times 1/16 + 1 \\times 6/16 + 2 \\times 9/16 = 24/16 = 3/2\\]\n\n\n4.1.5 variance of random variables\nKnowledge of the expected value says nothing about how the random variable actually varies: expectation does not distinguish between a random variable which is constant and one which can deviate far from the mean. In order to quantify this variation, one might be tempted to compute the mean differences from the mean value, but it does not work:\n\\[ E(X-\\mu_X) =  \\sum_i (x_i-\\mu_x)P(x_i) = \\sum_i x_i P(x_i) - \\mu_x \\sum_i P(x_i) = \\mu_x - \\mu_x = 0\\] The problem is, if we add up all the differences from the mean, the positive ones end up canceling the negative ones and the expected value of those deviations is exactly zero. This is why it makes sense to square the differences and add them up:\n\n\n\n\n\n\nDefinition\n\n\n\nThe variance of a discrete random variable \\(X\\) with probability distribution \\(P(x)\\) is \\[ Var(X) = E((X-\\mu_X)^2) = \\sum_{i=1}^N (x_i-\\mu_x)^2P(x_i)\\]\n\n\nOne useful property of the variance is: \\[ Var(X) = \\sum_i (x_i^2 - 2x_i\\mu_x + \\mu_x^2)P(x_i) =\\] \\[= \\sum_i x_i^2 P(x_i) - 2\\mu_x\\sum_i x_i P(x_i) + \\mu_x^2 \\sum_i P(x_i) = E(X^2) - E(X)^2 \\] So variance can be calculated as the difference between the expectation of the variable squared and the squared expectation. Note that the variance is given in units of the variable squared, so in order to measure the spread of the variable in the same units, we take the square root of the variance and call it the standard deviation: \\[\\sigma_x = \\sqrt{Var(X)}\\] While the expectation of a sum of random variables is the sum of their expectations, for any random variables, the same is not true for the variance. However, there is a special condition under which this is true. First, let us write the variance of a sum of two random variables \\(X\\) and \\(Y\\):\n\\[Var(X+Y) = E \\left[ (X+Y)-(\\mu_X+\\mu_Y) \\right]^2 =\\]\n\\[ = E[ (X-\\mu_X)^2 +(Y-\\mu_Y)^2 - 2(X-\\mu_X)(Y-\\mu_Y)] = \\] \\[=E (X-\\mu_X)^2 +  E(Y-\\mu_Y)^2   -2 E[(X-\\mu_X)(Y-\\mu_Y)] = \\]\n\\[ = Var(X) + Var(Y)  -2 E[(X-\\mu_X)(Y-\\mu_Y)] \\] If you write out the last term as a sum, it is none other than the covariance of the two random variables \\(X\\) and \\(Y\\), which we saw in the chapter on linear regression. So for any two random variables that have zero covariance, their variance is additive!\nExample. The variance of the number of heads out of two coin tosses can be calculated using its probability distribution function and the expected value (1) from above: \\[ Var(X) = (0-1)^2 \\times P(0) + (1-1)^2 \\times P(1) + (2-1)^2 \\times P(2) = 1/4+0+1/4 = 1/2\\] Since the variance is 1/2, the standard deviation, or the expected distance from the mean value is \\(\\sigma= \\sqrt{1/2}\\).\nExample. The variance of the number of mutants out of two people can be calculated using its probability distribution function and the expected value (0.4) from above: \\[ E(X) = (0-0.4)^2 \\times P(0) + (1-0.4)^2 \\times P(1) + (2-0.4)^2 \\times P(2) =\\] \\[ = 0.4^2 \\times 0.64+0.6^2 \\times 0.32+1.6^2 \\times 0.04 = 0.32\\] Since the variance is 0.32, the standard deviation, or the expected distance from the mean value is \\(\\sigma= \\sqrt{0.32}\\).\nExample. We have computed the expected value for the number of wins in two fringehead fights, so now let us find the variance and standard deviation. We already know the possible values of \\(X\\), and the associated probabilities, so we calculate: \\[ E(X^2) = 0^2 \\times 1/16 + 1^2 \\times 6/16 + 2^2 \\times 9/16 = 42/16\\] Then the variance is: \\[ Var(X) = E(X^2)  - E(X)^2 = 42/12 - 9/4 = (42-27)/16 = 15/16\\] and the standard deviation is \\(\\sigma = \\sqrt{15}/4\\) or just under 1.\n\n\n4.1.6 Exercises\nCalculate the expected values and variances of the following probability distributions, where the possible values of the random variable are in curly brackets, and the probability of each value is indicated as \\(P(x)\\).\n\n\\(X=\\{0, 1\\}\\) and \\(P(0) = 0.1, P(1) = 0.9\\).\n\\(X=\\{1,2,3\\}\\) and \\(P(1) = P(2) = P(3)=1/3\\).\n\\(X=\\{10, 15, 100\\}\\) and \\(P(10) = 0.5, P(15) = 0.3, P(100)=0.2\\).\n\\(X=\\{0, 1, 2, 3, 4\\}\\) and \\(P(0) = 1/8, P(1) = P(2) = P(3) = 1/4, P(4) = 1/8\\).\n\\(X=\\{-1.5, -0.4, 0.3, 0.9\\}\\) and \\(P(-1.5) = 0.4, P(-0.4) = 0.2, P(0.3) = 0.35, P(0.9) = 0.05\\)."
  },
  {
    "objectID": "probdist.html#examples-of-distributions",
    "href": "probdist.html#examples-of-distributions",
    "title": "4  Random variables and distributions",
    "section": "4.2 Examples of distributions",
    "text": "4.2 Examples of distributions\n\n\n4.2.1 uniform distribution\nPerhaps the simplest random variable (besides a constant, which is not really random) is the uniform random variable, for which every outcome has equal probability. The distribution of a fair coin is uniform with two values, \\(H\\) or \\(T\\), or 0 and 1, each with probability 1/2. More generally, a discrete uniform random variable has \\(N\\) outcomes and each one has probability \\(1/N\\). This is what people often mean when they use the word random - an experiment where each outcome is equally likely.\nWe can calculate the expectation and variance of a uniform random variable \\(U\\):\n\\[\nE(U) = \\sum_{i=1}^n  a_i P(a_i) = \\frac{1}{n}  \\sum_{i=1}^n  a_i\n\\] So the expected value is the mean of all the values of the uniform random variable.\nExample. In the special case of the uniform distribution of \\(n+1\\) integers between 0 and \\(n\\) (\\(a_i = i\\), for \\(i=0,..., n\\)), each value has probability \\(P = 1/(n+1)\\). The expected value is the average of the maximum and minimum values (using the fact that \\(\\sum_{i=0}^n i = n(n+1)/2\\)): \\[ E(U) = \\frac{n(n+1)}{2(n+1)} = \\frac{n}{2} \\]\nGeneralizing, for a random variable on integers between \\(a\\) and \\(b\\), the expectation is \\[ E(U) = \\frac{a+b}{2}\\]\nWe can also write down the expression for the variance of the discrete uniform distribution as follows:\n\\[ Var(U) = E(U^2) - E(U)^2 =  \\frac{1}{n} \\sum_{i=1}^n  a_i^2  -  \\frac{1}{n^2} \\left(\\sum_{i=1}^n  a_i \\right)^2\\] Example. In the special case of the uniform distribution of \\(n+1\\) integers between 0 and \\(n\\) (\\(a_i = i\\), for \\(i=0,..., n\\)), each value has probability \\(P = 1/(n+1)\\). The variance can be calculated using the formula for the sum of squares: \\(\\sum_{i=0}^n i^2 =n(n+1)(2n+1)/6\\).\n\\[ Var(U) = \\frac{(n+1)(2n+1)n}{6(n+1)} - \\frac{n^2}{4} =   \\frac{2n^2+n}{6} - \\frac{n^2}{4} = \\frac{n(n+2)}{12}\n\\]\nThis can be generalize to a uniform random variable on integers between \\(a\\) and \\(b\\) (omitting the algebraic details) so the variance for that uniform random variable is:\n\\[ Var(U) = \\frac{(b-a+1)^2 - 1}{12} = \\frac{(b-a)^2 + 2(b-a)}{12}\n\\]\n\n\n\n\n\nTwo uniform random distributions with integer values with different ranges.\n\n\n\n\n\n\n\nTwo uniform random distributions with integer values with different ranges.\n\n\n\n\n\n\n4.2.2 binomial distribution\nWe have introduced binary or Bernoulli trials in section \\(\\ref{sec:math4_1}\\). Assume that the two values of the random variable \\(X\\) are 0 and 1, with probability \\(1-p\\) and \\(p\\), respectively. Then we can calculate the expectation and variance of a single Bernoulli trial:\n\\[E(X) = 0 \\times (1-p) + 1 \\times p = p\\] \\[ Var(X) = E(X^2) - E(X)^2 = 0^2 \\times (1-p) + 1^2 \\times p - p^2= p(1-p)\\] The first result is likely intuitive, but the second deserves a comment. Note that depending on the probability of 1, the variance, or the spread in outcomes of a Bernoulli trial is different. The highest variance occurs when \\(p=1/2\\), or equal probability of 0 or 1, but when \\(p\\) approaches 0 or 1, the variance approaches 0. Thus, as the probability approaches zero or one the random variable approaches a constant (either always 1 or 0); hence, no variance.\nOne can extend this scenario and ask what happens in a string of Bernoulli trials, for instance, in a string of 10 coin tosses, or in testing 20 randomly selected people for a mutation. The mathematical problem is to calculate the probability distribution of the number of success out of many trials. This is known as the binomial random variable, which is defined as the sum of \\(n\\) independent, identical Bernoulli random variables.\n\n\n\n\n\n\nDefinition\n\n\n\nGiven \\(n\\) independent Bernoulli trials \\(X\\) with the same probability of success \\(p\\), the binomial random variable is defined as: \\[B = \\sum_{i=1}^n X_i\\] where \\(X_i\\) is the random variable from the i-th Bernoulli trial, which takes values of 1 and 0.\n\n\nIn this definition I use the term independence without defining it properly, which will be done in chapter 10. Intuitively, independence between two Bernoulli trials (e.g. coin tosses) means that the outcome of one trial does not change the probability of the outcomes of any other trials. This amounts to the assumption that the probability of an outcome followed by another one is the product of the separate probabilities of the two outcomes. For example, if the two outcomes are wins and losses, then \\(P(\\{WL\\}) = P(W)P(L)\\). This will be used below in the calculation of the variance of the binomial random variable.\nTo find the probability distribution of the binomial random variable, we need to define the event of \\(k\\) wins out of \\(n\\) trials. Consider the case of 4 trials. It is easy to find the event of 4 wins, as it is comprised only of the outcome \\(\\{WWWW\\}\\). Then, \\(P(4) = p^4\\), based on the independence assumption. The event of winning 3 times consists of four strings: \\(\\{LWWW, WLWW, WWLW, WWWL\\}\\) so the probability of obtaining 3 wins is the sum of the four probabilities, each equal to $ p^3(1-p)$ from the independence assumption above, so \\(P(3) = 4p^3(1-p)\\). The event of winning 2 times is even more cumbersome, and consists of six strings: \\(\\{ LLWW, WLLW, WWLL, WLWL, LWLW, LWWL\\}\\), so \\(P(2) = 6p^2(1-p)^2\\) by the same reasoning.\nNow imagine doing this to calculate 50 wins out of 100 trials. The counting gets ugly very fast. We need a general formula to help us count the number of ways of winning \\(k\\) times out of \\(n\\) trials. We denote this number \\(\\binom{n}{k}\\), also known as “\\(n\\) choose \\(k\\)” because it corresponds to the number of ways of choosing \\(k\\) distinct objects out of \\(n\\) without regard to order. The connection is as follows: let us label each trial from 1 to \\(n\\). Then to construct a string with \\(k\\) wins, we need to specify which trials resulted in a win (the rest are of course losses). It does not matter in which order those wins are selected - it still results in the same string. Therefore the number of different strings of \\(n\\) binary trials with \\(k\\) successes is the same as the number of ways of selecting \\(k\\) different objects out of \\(n\\) different ones.\nThe number itself can be derived as follows: there are \\(n\\) possibilities for choosing the number of the first win, then \\(n-1\\) possibilities for choosing the number of the second win, etc, and finally when choosing the \\(k\\)-th win there are \\(n-k+1\\) possibilities (note that \\(k \\leq n\\), and if \\(n=k\\) there is only one option left for the last choice.) Thus, the total number of such selections is: \\(n(n-1)...(n-k+1) = n!/(n-k)!\\)\nBut note that we overcounted, because we considered different strings of wins depending on the order in which a win was selected, even if the resulting strings are the same (example: \\(n=4\\) and \\(k=4\\) gives us \\(4!\\) although there is only one string of 4 wins out of 4). In order to correct for the overcounting, we need to divide by the total number of ways of selecting the same string of \\(k\\) wins out of \\(n\\). This is number of ways of rearranging \\(k\\) wins, or \\(k!\\) Thus, the number we seek is:\n\\[\\binom{n}{k} = \\frac{n!}{k! (n-k)!}\\]\nWe can now calculate the general probability of winning \\(k\\) times out of \\(n\\) trials. First, each string of \\(k\\) wins and \\(n-k\\) losses has the probability \\(p^k (1-p)^{n-k}\\). Since we now know that the number of such strings is \\(C^n_k\\), the probability is:\n\\[\nP(\\mathrm{k \\; wins \\; in \\; n \\; trials}) =  P(B=k)= \\binom{n}{k}  p^k (1-p)^{n-k}\n\\]\nThis is the probability distribution of the binomial random variable \\(B\\).\nThe binomial random variable has much simpler formulas for the mean and the variance. First, we know that the mean of a sum of random variables is the sum of the means and the binomial random variable is a sum of \\(n\\) Bernoulli random variables \\(X\\). Let us say \\(X\\) takes only the values of 0 and 1 with probabilities \\(1-p\\) and \\(p\\), so we can use the additive property of expected value to calculate \\(E(B)\\):\n\\[\nE(B) = E\\left[\\sum_{i=1}^n X\\right] = \\sum_{i=1}^n E(X) = \\sum_{i=1}^n p = np\n\\] This means that the expected number of heads/successes is the product of the probability of 1 head/success and the number of trials, e.g. if the probability of success is 0.3, then the expected number of successes out of 100 is 30.\nNow let us calculate the variance, for which in general the same additive property is not true. But remember that in the section on variance above we showed that the variance of a sum of two random variables is the sum of their two separate variances as long as their covariance is zero. It turns out that for random variables that satisfy the product rule \\(P(x, y) = P(x)P(y)\\) their covariance is 0:\n\\[E((X-\\mu_X)(Y-\\mu_Y))  =  \\sum_i \\sum_j (x_i-\\mu_X) (y_j-\\mu_Y) P(x_i, y_j) =  \\] \\[ = \\sum_i(x_i-\\mu_X)P(x_i) \\sum_j (y_j-\\mu_Y) P(y_j) \\] We saw in section on variance above that the expected value of deviations from the mean is zero, which gives us:\n\\[E((X-\\mu_X)(Y-\\mu_Y))  = E(X-\\mu_X)E(Y-\\mu_Y) = 0\\]\nThe demonstrates that for independent variables the variance of their sum is the sum of the variances and we can use this to compute the variance of the binomial random variable:\n\\[\nVar(B) = Var\\left[\\sum_{i=1}^n X\\right]  = \\sum_{i=1}^n Var(X) =\\sum_{i=1}^n p(1-p) = np(1-p)\n\\]\nFor any given number of Bernoulli trials, the variance has a quadratic dependence on probability of success \\(p\\): if \\(p=1\\) or \\(p=0\\), corresponding to all successes, or all failures, respectively, then the variance is zero, since there is no spread in the outcome. For a fair coin \\(p=1/2\\) the variance is highest. This can be seen in the plots of binomial random variables for \\(n=2\\), \\(n=5\\), and \\(n=50\\), shown in figures below.\n\n\n\n\n\nThe binomial distribution for \\(n=2\\) and \\(p=0.2\\) and \\(p=0.5\\) (you should be able to tell which one is which!)\n\n\n\n\n\n\n\nThe binomial distribution for \\(n=2\\) and \\(p=0.2\\) and \\(p=0.5\\) (you should be able to tell which one is which!)\n\n\n\n\n\n\n\n\n\nThe binomial distribution for \\(n=5\\) and \\(p=0.2\\) and \\(p=0.5\\) (you should be able to tell which one is which!)\n\n\n\n\n\n\n\nThe binomial distribution for \\(n=5\\) and \\(p=0.2\\) and \\(p=0.5\\) (you should be able to tell which one is which!)\n\n\n\n\n\n\n\n\n\nThe binomial distribution for \\(n=50\\) and \\(p=0.2\\) and \\(p=0.5\\) (you should be able to tell which one is which!)\n\n\n\n\n\n\n\nThe binomial distribution for \\(n=50\\) and \\(p=0.2\\) and \\(p=0.5\\) (you should be able to tell which one is which!)\n\n\n\n\n\n\n4.2.3 Exercises\nCalculate the means and variances based on the plotted distributions using the definitions \\(\\ref{def:ch4_mean}\\) and \\(\\ref{def:ch4_var}\\) and compare your calculations against equations \\(\\ref{eq:ch4_mean_unif}\\) and \\(\\ref{eq:ch4_var_unif}\\) (for uniform random variables) and equations \\(\\ref{eq:ch4_binom_mean}\\) and \\(\\ref{eq:ch4_binom_var}\\) (for binomial random variables)\n\nCalculate the mean and the variance for the two uniform distributions plotted in figure @ref(fig:unif-dist).\nCalculate the mean and the variance for the two binomial distributions plotted in figure @ref(fig:bin-dist-1).\nCalculate the mean and the variance for the two binomial distributions plotted figure @ref(fig:bin-dist-2).\nEstimate (approximately) the mean and the variance for the two binomial distributions plotted in figure @ref(fig:bin-dist-3).\n\n\n\n4.2.4 testing for mutants\nSuppose that you’re screening people for a particular genetic abnormality. It is known from prior experience that about 5% of this population carry this mutation. You run your tests on a group of 20 people, and the results indicate that 3 of them are carriers. Clearly, this is higher than you expected - 3/20 is 15%, or 3 times higher than the estimate. One of your colleagues exclaims, What are the odds of this?\nTo answer this question, one must start by stating your assumptions. First, the people tested must be chosen from the same population, so we can assume a priori each had probability 5% of being a carrier. Second, the people must be selected without bias, that is, selection of one must be unlinked or independent of others. As a counter-example, if your selection included an entire biological family, that would be a biased selection - it may be that the whole family has the mutation, or maybe they don’t, but either way probability is no longer determined on a person-by-person basis. If these assumptions are made, then one can calculate the probability of making a selection of 20 people that includes 3 carriers of the mutation, using the binomial distribution.\nThe formula for the binomial distribution in equation \\(\\ref{eq:ch4_binom_dist}\\) provides the answer for any given number of mutants. For example, the probability of 3 people out of 20 being carriers for the mutation is: \\[P(\\mathrm{3 \\ out \\ of  \\ 20}; \\ p=0.05) = \\binom{20}{3} \\times 0.05^3  \\times 0.985^{17} =  \\] \\[ = 1140 \\times 0.05^3 \\times 0.985^{17} \\approx 0.0596\\]\nOne may want to ask a different question: what is the probability that there are at least 3 mutants in the sample of 20 people? To most efficient way to calculate this it is to answer the complementary question first: what is the probability that there are fewer than 3 mutants out of 20 people? This corresponds to three values of the random variable: 0, 1, or 2. We can calculate the total probability by adding up the three separate probabilities, since they represent non-overlapping events (one can’t have 1 and 2 mutants in a sample simultaneously): \\[ P(B < 3; \\  p=0.05) = P(B=0) + P(B=1) + P(B=2) = \\] \\[ = \\binom{20}{2} \\times 0.05^2  \\times 0.985^{18} +\\binom{20}{1} \\times 0.05^1  \\times 0.985^{19} +\\binom{20}{0} \\times 0.05^0  \\times 0.985^{20} \\approx \\] \\[ \\approx 0.925 \\] The answer to the original question is found by taking the complementary probability \\(1-0.925=0.075\\). Thus the probability of finding at least 3 mutants in a sample of 20 with individual probability 0.0015 is approximately 0.075. The answer is close to the probability of having exactly 3 mutants because the probability of finding more than 3 mutants is very low."
  },
  {
    "objectID": "probdist.html#random-number-generators-in-r",
    "href": "probdist.html#random-number-generators-in-r",
    "title": "4  Random variables and distributions",
    "section": "4.3 Random number generators in R",
    "text": "4.3 Random number generators in R\n\nSimulating randomness with a computer is not a simple task. Randomness is contrary to the nature of a computer, which is designed to perform operations exactly. However, there are algorithms that produce a string of numbers that are for all intents and purposes random: there is no obvious connection between one number and the next, and the values don’t form any pattern. Such algorithms are called random number generators, although to be more precise they produce pseudo-random numbers. The reason is that they actually produce a perfectly predictable string of numbers, which eventually repeats itself, but with a humongous period. One can even produce the same random number, or the same string of random numbers, by specifying the seed for the random number generator. This is very useful if one wants to reproduce the results of a code that uses random numbers.\nOf course, random variable are not all the same - they have different distributions. R has a number of functions for producing random numbers from different distributions. For example, to produce random numbers from a set of values with a uniform probability distribution, use the function sample(). For instance, the following command produces a random integer between 1 and 20. Repeating the same command produces a new random number, which (most likely) is not the same as the first. The first input argument (1:20) is the vector of values from which to draw the random number, and the second is the size of the sample:\n\nx <- sample(1:20,1)\ny <- sample(1:20,1)\nprint(x)\n\n[1] 9\n\nprint(y)\n\n[1] 3\n\n\nTo generate 10 randomly chosen integers between 1 and 20, see the following two commands, which differ in setting the value of the option replace. The first command doesn’t specify the value for replace, and by default it is set to FALSE, so the command draws numbers without replacing them (meaning that all the numbers in the sample are unique). In the second command replace is set to TRUE, so the numbers that were selected can be chosen again. In both cases, repeatedly running the command results in a different set of randomly chosen numbers, which you should investigate by copying the commands into R and running them yourself.\n\nx <- sample(1:20,10)\nprint(x)\n\n [1]  7  3  5 20 10 11  9  1 19 18\n\ny <- sample(1:20,10,replace=TRUE)\nprint(y)\n\n [1] 11  5  7 13  1 10 14 10  1  5\n\n\nIf you need to generate a random number from the binomial distribution, R has you covered. The command is rbinom(s, n, p) and it requires three input values: s is the number of observations (sample size), n is the number of binary trials in one observation, and p is the probability of success in one binary trial. The following two commands generate a single random number, the number of successes out of 20 trials with probability of success 0.2 and 0.6:\n\nx <- rbinom(1,20,0.2)\nprint(x)\n\n[1] 2\n\ny <- rbinom(1,20,0.6)\nprint(y)\n\n[1] 11\n\n\nTo generate an entire sample of random numbers, change the first input parameter to 10. As you’d expect, the samples of 10 observations are (most likely) noticeably different: when the probability p is 0.2, the number of successes tend to be less than 6, while for probability 0.6, the numbers are usually greater than 10.\n\nx <- rbinom(10,20,0.2)\nprint(x)\n\n [1] 2 6 5 3 4 1 3 3 4 3\n\ny <- rbinom(10,20,0.6)\nprint(y)\n\n [1] 14 14 13 11 14 12 13  8  9 14\n\n\nNotice that the range of possible values of this random variable is between 0 and 20, but unlike the uniform random numbers produced with the sample() function, the probability of obtaining different numbers are different, and depend on the parameter p. Calculation and plotting of the binomial distribution function can be accomplished with the command dbinom(x,n,p), where \\(x\\) is the value of the random variable (between 0 and n), \\(n\\) is the number of trials, and p is the probability of success. For instance, the following script calculate the probability of obtaining 1 success out of 20 with probability \\(p=0.2\\):\n\nn <- 20\np <- 0.2\nprint(dbinom(1,n,p))\n\n[1] 0.05764608\n\n\nThe script above calculates the probabilities of all of the possible values of the random variable by substituting the vector of these values (e.g. 0 to 20) instead of the number 1, generating the probability distribution vector. This vector is plotted vs. the values of the random variable using the barplot() function, producing an aesthetically pleasing plot of the binomial distribution. The script plots two binomial probability distributions, both with \\(n=20\\), the first with \\(p=0.2\\) and the second with \\(p=0.6\\). Notice also the use of the axis labels in barplot() using the same options xlab and ylab as in plot() and use the main option to produce a title above each plot.\n\nvalues.vec <- 0:n\nprob.dist <- dbinom(values.vec,n,p)\nbarplot(prob.dist,names.arg=values.vec,xlab='binomial RV',ylab='probability',\nmain='binom dist with n=20 and p=0.2')\np<-0.6\nprob.dist <- dbinom(values.vec,n,p)\nbarplot(prob.dist,names.arg=values.vec,xlab='binomial RV',ylab='probability',\nmain='binom dist with n=20 and p=0.6')\n\n\n\n\nThe binomial distribution for two different values of n and p produced using dbinom() function.\n\n\n\n\n\n\n\nThe binomial distribution for two different values of n and p produced using dbinom() function."
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "5  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever.\n\n1 + 1\n\n[1] 2"
  }
]